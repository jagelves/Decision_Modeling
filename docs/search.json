[
  {
    "objectID": "Decisions.html",
    "href": "Decisions.html",
    "title": "1  Decisions Under Uncertainty",
    "section": "",
    "text": "1.1 Shopaholic Retail Company\nImagine you are the CEO of the Shopaholic Retail Company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome. Determining how to adequately assess the risks involved in this decision and the potential outcomes is of importance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#shopaholic-retail-company",
    "href": "Decisions.html#shopaholic-retail-company",
    "title": "1  Decisions Under Uncertainty",
    "section": "",
    "text": "Success Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#sec-EMV",
    "href": "Decisions.html#sec-EMV",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.2 Expected Monetary Value",
    "text": "1.2 Expected Monetary Value\nAccording to your estimates, you believe that there is a \\(60\\)% chance of success and a \\(40\\)% chance of failure. Using this information, you can calculate the expected monetary value (EMV) of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\nThe expected monetary value estimates the average monetary outcome you can expect from a decision involving uncertainty. In this example, the value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EMV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EMV\\) is the expected monetary value, \\(p_{i}\\) is the probability of outcome \\(i\\), and \\(x_{i}\\) is the monetary value resulting from outcome \\(i\\). Notice that if we had other decisions involving uncertainty, we could also evaluate them using the EMV. Hence, the EMV allows us to rank and choose best decisions when uncertainty is present.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#decision-trees",
    "href": "Decisions.html#decision-trees",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.3 Decision Trees",
    "text": "1.3 Decision Trees\nDecision trees visually map the entire business decision process. Below you can see the decision tree for the decision to introduce a new production line.\n\n\n\n\n\ngraph LR\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( 0 )\n    C --&gt;|Success p=0.6| D( 5 )\n    C --&gt;|Failure p=0.4| E( -3 )\n\n\n\n\n\n\n Decision trees are read from left to right. Notice that there are two main branches stemming from the first decision node (i.e., the first square to the left). You can decide to Introduce or Not Introduce the new production line. If you Introduce, you reach a probability node (i.e., the circle that leads to the success and failure branches) where chance determines success or failure at the probabilities given. In the end, your choice of introducing the new product line yields an expected monetary value of 1.8 million, whereas not introducing the line yields no payoff.\nTo solve decision trees and find the optimal decision use backward induction. Starting from the right of the decision tree and working back to the left at each probability node calculate the EMV. At each decision node, take the maximum of EMV’s to identify the optimal decision. Applying this procedure to our simple example, we would start with the probability node that leads to the success and failure branches and calculate the EMV of 1.8 million (refer to Section 1.2). Moving back to our initial decision, we now have the choice to Introduce or Not Introduce. In this case we should Introduce, since the resulting EMV (1.8) is higher than Not Introduce (0).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#consulting-team",
    "href": "Decisions.html#consulting-team",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Consulting Team",
    "text": "1.4 Consulting Team\nConsider now the option of hiring a consulting team that promises to give you a bit more certainty. The team provides a recommendation based on market research, historical data, and their expertise for $500,000. However, it is known that the team has a false positive rate (i.e., recommending to go with a project when the project would fail) of 15% and a false negative rate of 5% (i.e., recommending not going with a project and when the project would succeed). Below you can see the updated decision tree with the results of the recommendation (i.e., Positive, and Negative).\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=?| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=?| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )\n\n\n\n\n\n\nNote that even though the consultants might recommend not introducing the production line, the CEO can still decide to go against their recommendation. Also, some probabilities are now unknown and must be calculated (they are highlighted in the decision tree with \\(?\\)). For example, given that the recommendation is positive, the probability that the introduction would succeed is unknown \\(p(Success|+)\\). We will use probability theory to uncover the missing probabilities in the upcoming sections. Finally, the final payoffs have all been adjusted to reflect the cost of hiring the team of consultants.\nUltimately, we would like gain more certainty on our decision to introduce the production line. Additionally, knowing if we should hire consultants to advise is equally essential.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#updating-probabilities",
    "href": "Decisions.html#updating-probabilities",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.5 Updating Probabilities",
    "text": "1.5 Updating Probabilities\nThe Law of total probability is useful in determining the probabilities that we get a positive recommendation \\(p(+)\\) or a negative recommendation \\(p(-)\\). In sum the law states:\n\n\\(p(A)=p(A|B)p(B)+p(A|B^c)p(B^c)\\)\n\nSubstituting the values provided in our problem we obtain:\n\n\\(p(+)=p(+|Failure)p(Failure)+p(+|Success)p(Success)\\) \n\\(p(+)=0.15(0.4)+0.95(0.6)\\) \n\\(p(+)=0.63\\)\n\nHence, the probability of obtaining a recommendation of introducing the new product line is \\(63\\)% and of not recommending the introduction is \\(p(-)=37\\)%. The updated decision tree is now:\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#bayes-theorem",
    "href": "Decisions.html#bayes-theorem",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.6 Bayes’ Theorem",
    "text": "1.6 Bayes’ Theorem\nNow we can use Bayes’ Theorem to update our probabilities given the recommendation from the consulting team. Recall that Bayes’ Theorem is a concept in probability that helps us update our beliefs when given new information. Mathematically, Bayes’ Theorem states:\n\n\\(p(A|B)=\\frac{p(B|A)p(A)}{p(B)}\\)\n\nSubstituting values we can find the missing probabilities at the edge of the tree. In particular:\n\n\\(p(Success|+)=\\frac{p(+|Success)p(Success)}{p(+)}\\) \n\\(p(Success|+)=\\frac{0.95(0.6)}{0.63}\\) \n\\(p(Success|+)=0.90\\)\n\nThis implies that \\(p(Failure|+)=0.10\\). Similarly, the probabilities \\(p(Success|-)\\) and \\(p(Failure|-)\\) can be found using Bayes’ theorem (the reader should try to obtain these probabilities). The updated decision tree is given below:\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=0.90| D( 4.5 )\n    C --&gt;|Failure p=0.10| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=0.08| J( 4.5 )\n    H --&gt;|Failure p=0.92| K( -3.5 )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#sec-Optimal",
    "href": "Decisions.html#sec-Optimal",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Optimal Decision",
    "text": "1.7 Optimal Decision\nNow that we have calculated the probabilities we can finally decide whether we should hire the consulting team, and most importantly whether we should introduce the new product line. Summarizing the decision tree by calculating the EMV’s for each probability node yields:\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( ))\n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt; D(EMV=3.70)\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt; J(EMV=-2.86)\n\n\n\n\n\n\nNote that even though the consultants might recommend not introducing the production line, the CEO can still decide to go against their recommendation. If the recommendation is positive, then the EMV resulting from introduction of the product line (\\(3.70\\)) is greater than the loss of not introducing the product line (\\(-0.5\\)). Similarly, if the recommendation is negative, the EMV from the introduction (\\(-2.86\\)) is less than the EMV of not introducing the product line (\\(-0.5\\)). As a consequence, the recommendation is aligned with the CEO’s best decision.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#the-value-of-information",
    "href": "Decisions.html#the-value-of-information",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.8 The Value of Information",
    "text": "1.8 The Value of Information\nWe are now faced with an important question. What is the value of the information provided by the consulting team? In other words, should the CEO hire the consultants? To answer this question, let’s start by calculating expected monetary value of the tree in Section 1.7:\n\n\\(EMV=0.63(3.70)+0.37(-0.5)\\) \n\\(EMV=2.146\\) \n\nThis EMV is higher than the EMV of making the decision without the consultants (1.8 million). This highlights that the information provided by the consulting team is valuable to the CEO. Moreover, the CEO should be willing to pay up to $846,000 (EMV with free information - EMV without information) for the consulting service.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#readings",
    "href": "Decisions.html#readings",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.9 Readings",
    "text": "1.9 Readings\nThe readings for this chapter are mainly from Winston and Albright (2019). Chapter 9 provides an excellent introduction to decision models with a couple of solved problems using Excel. I recommend using R (as a calculator) instead of Excel, as this book mainly uses R. To construct decision trees in R, you can use mermaid along with Quarto. Some probability concepts are essential to review before you start reading the chapter. In particular, discrete random variables, expected value, conditional probability, probability rules, and Bayes’ theorem.\n\nWinston and Albright (2019) Chapters 9.1 (Introduction), 9.2 (Elements of Decision Analysis), 9.3 (Single-Stage Decision Problems) and 9.5 (Multistage Decision Problems). It is recommended you follow along in R as opposed to Excel (or Precision Tree Add-In).\nJaggia and Kelly (2022) Chapter 4.1 (Fundamental Probability Concepts), 4.2 (Rules of Probability), 4.3 (Contingency Tables and Probabilities), and 4.4 (The Total Probability Rule and Bayes’ Theorem).\nQuarto: https://Quarto.org\nMermaid in Quarto Document: https://quarto.org/docs/authoring/diagrams.html\nMermaid in R Script: https://www.rdocumentation.org/packages/DiagrammeR/versions/1.0.10/topics/mermaid",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#lessons-learned-in-this-chapter",
    "href": "Decisions.html#lessons-learned-in-this-chapter",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.10 Lessons Learned In This Chapter",
    "text": "1.10 Lessons Learned In This Chapter\n\nUse the concept of Expected Monetary Value to rank decisions involving uncertainty.\nUse Decision Trees to map the business decision process.\nApply backward induction to solve decision problems.\nApply the Law of Total Probability and Bayes’ Theorem to update probabilities.\nDetermine the price of information.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#exercises",
    "href": "Decisions.html#exercises",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.11 Exercises",
    "text": "1.11 Exercises\n\nTesla, a U.S. EV car manufacturer, is considering expanding production to meet potential increases in the demand for one of its cars. Tesla’s alternatives are to construct a new plant, expand the existing plant, or do nothing in the short run. Analysts within Tesla expect the market for their EV’s may expand, remain stable, or contract, with probabilities of 0.35, 0.35, and 0.30, respectively. The table below contains the profit generated under each scenario. What is your recommendation to Tesla?\n\n\n\n\n\nExpand\nStable\nContract\n\n\n\n\nNew Plant\n400,000\n-100,000\n-200,000\n\n\nExpand Plant\n250,000\n-50,000\n-75,000\n\n\nDo Nothing\n50,000\n0\n-30,000\n\n\n\n\n\nSuggested Answer\n\nWe can use expected values to guide our recommendation to Tesla. The expected value of each option is as follows: constructing a new plant is 45,000, expanding plant is 47,500, and doing nothing is 8,500. Following these results we could recommend Tesla to expand its existing plant, as this option would give the company the highest expected profit.\n\n\nFrontier Airlines is considering purchasing a new airplane from an aircraft manufacturer. The manufacturer offers the airplane for 50 million. The airline estimates that, if the airplane is purchased, it could generate an additional revenue of 120 million over its operational lifetime. However, this is contingent on the condition that the airline industry remains stable and demand for flights remains high, which the airline estimates has a 70% probability of occurring.\nIf the industry faces a downturn (a 30% probability), the airline estimates it will only be able to generate $30 million in revenue from the airplane over its lifetime.\nAlternatively, the airline has the option to lease an older model airplane for 10 million. The older airplane would generate $40 million in revenue over its lifetime, regardless of industry conditions.\nShould the airline purchase the new airplane or lease the older model?\n\n\n\nSuggested Answer\n\nOnce again we can use the expected value in this problem. The option of buying the airplane yields an expected profit of 43 million dollars, whereas the option of leasing the airplane yields a profit of 30 million. Given this result, we should recommend Frontier airlines to buy the airplane.\n\n\nPharmaCo is a pharmaceutical company considering whether to launch a new drug called “CureFast”. The success of CureFast in the market depends on the drug’s effectiveness in treating a particular condition. PharmaCo estimates that there is a 40% chance that the drug will be highly effective and thus successful in the market. If CureFast is successful, the company will earn a profit of 50 million. However, if the drug is not effective, PharmaCo will incur a loss of 20 million due to production, marketing, and other sunk costs. The company can always decide to not launch the drug. This option yields a profit of 0.\nBefore deciding to launch the drug, PharmaCo can conduct a clinical trial at a cost of $5 million. The clinical trial is not perfect and has the following characteristics:\n\nIf the drug is truly effective, there is a 90% chance that the clinical trial will show positive results. If the drug is not effective, there is a 15% chance that the clinical trial will still show positive results.\n\nShould PharmaCo launch the drug? Should they conduct the clinical trial? What is the maximum amount they should pay for the trial?\n\n\n\nSuggested Answer\n\nThe expected value when not launching the clinical trial is 8 million. Since the expected value after conducting the trial is higher (11.2 million), PharmaCo should conduct the clinical trial and use the results to inform their decision on whether to launch CureFast.PharmaCo should pay at most 8.2 million for the clinical trial.\n\n\nJoão “Golden Toes” Silva is the next Brazilian league wonder kid, and Arsenal’s scouts are buzzing with excitement. The club’s analysts estimate that João has a 30% chance of being a success at Arsenal, performing at a top level and helping the club win titles.\nIf João is successful, Arsenal stands to make £18 million in profits through merchandise sales, increased ticket revenue, and potential transfer fees. However, if João fails to live up to expectations (which has a 70% chance), it could cost the club £8 million (wasted wages, and negative press).\nGiven the risk of signing the player, the club can conduct a thorough exam on João making sure that he will be a success. The test is perfect and has no type 1 or 2 errors. The cost of this physical test is £3 million.\nShould Arsenal sign “Golden Toes”? Should they conduct the physical test? What is the maximum Arsenal should pay for the test?\n\n\n\nSuggested Answer\n\nWithout the test, the expected value is -200,000 pounds. With the perfect test, the expected value is 2,400,000 pounds. We recommend Arsenal go with the test and sign the player. The club can pay up to 5,400,000 pounds for the test.\n\n\nThe Crypto Gains investment firm is planning to invest in Bitcoin to diversify its portfolio. The firm knows that it will need 50 Bitcoins six months from now to meet its investment strategy. The company can buy the Bitcoins at the market price six months from now or purchase a futures contract now. The contract guarantees delivery of the Bitcoins in six months, and the cost of purchasing it will be based on today’s market price. Assume that the possible Bitcoin futures contracts available for purchase are for 25 Bitcoins or 50 Bitcoins only. No futures contracts can be purchased or sold in the intervening months; thus, Crypto Gains’ possible decisions are to:\n\n\nPurchase a futures contract for 50 Bitcoins now.\nPurchase a futures contract for 25 Bitcoins now and purchase 25 Bitcoins in six months at market price.\nPurchase all 50 Bitcoins needed in six months at the market price.\nThe price of Bitcoin bought now for delivery in six months is 30,000 dollars per Bitcoin. The transaction cost for 25-Bitcoin and 50-Bitcoin future contracts are 1,500 and 2,800 dollars, respectively. Finally, the company has assessed the probability distribution for the possible Bitcoin prices six months from now in dollars per Bitcoin. The table below contains the possible prices and the corresponding probabilities. Which investment strategy should Crypto Gains pursue to minimize risk and potential costs while securing the necessary Bitcoins?\n\n\n\n\nPrice\nProbability\n\n\n\n\n$28,000\n0.10\n\n\n$29,500\n0.30\n\n\n$31,000\n0.35\n\n\n$32,500\n0.15\n\n\n$34,000\n0.10\n\n\n\n\n\nSuggested Answer\n\nThe expected values are a follows: buy 50 Bitcoins now via futures contract: 1,502,800, buy 25 Bitcoins now via futures contract, and 25 Bitcoins in six months at market price: 1,520,875, buy all 50 Bitcoins in six months at market price: 1,538,750. Crypto Gains should purchase a futures contract for 50 Bitcoins now to secure the necessary investment and minimize costs.\n\n\n\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. Business Statistics. McGraw Hill.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Tools.html",
    "href": "Tools.html",
    "title": "2  Tools for Working With Simulation",
    "section": "",
    "text": "2.1 Storing Our Data in R\nObjects, vectors, and data frames are all critical in the R programming language. They are helpful when storing and manipulating data in R. An object is a container for storing data and computations in R. It can store something as simple as a single integer or as informative as the output in regression analysis. The code below creates an object x that stores the number \\(5\\).\nx&lt;-5\nVectors are one-dimensional data arrays that can be stored in an object. They can contain elements of various data types, such as numerical values, character, or logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Below, the vector Books stores the titles of \\(5\\) monster classics a bookstore plans to release.\nbooks&lt;-c(\"Frankenstein\",\"Dracula\",\"Moby Dick\",\n         \"War Of The Worlds\",\"Beowulf\")\nLastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record. Think of a data frame as a collection of related vectors. We can easily construct a data frame by combining one or more vectors using the data.frame() function in R.\ndata.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45))\n\n              Books Price\n1      Frankenstein  9.50\n2           Dracula  5.78\n3         Moby Dick  6.34\n4 War Of The Worlds  5.67\n5           Beowulf  2.45",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#generating-random-numbers-in-r",
    "href": "Tools.html#generating-random-numbers-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.2 Generating Random Numbers in R",
    "text": "2.2 Generating Random Numbers in R\nSeveral functions are available in R that can be used to generate random numbers. These functions are based on a specific probability distribution. For instance, the rbinom() function generates random numbers based on the binomial distribution, while the runif() and rnorm() functions generate random numbers based on the uniform and normal distributions, respectively. The table below lists some functions that generate random numbers and their probability distribution.\n\n\n\nDistribution\nFamily\nPackage\nFunction\n\n\n\n\nUniform\nDiscrete\nextraDistr\nrdunif()\n\n\nBinomial\nDiscrete\nBase R\nrbinom()\n\n\nHypergeometric\nDiscrete\nBase R\nrhyper()\n\n\nPoisson\nDiscrete\nBase R\nrpois()\n\n\nUniform\nContinuous\nBase R\nrunif()\n\n\nNormal\nContinuous\nBase R\nrnorm()\n\n\nExponential\nContinuous\nBase R\nrexp()\n\n\nTriangle\nContinuous\nextraDistr\nrtriang()\n\n\n\nRecall that the binomial distribution illustrates the probability of x successes from an experiment with n trials. We can use the distribution to generate random numbers by providing the probability of success (p) and the number of trials (n) parameters. Similarly, the uniform distribution shows the probability of a random variable within a minimum and maximum limit. Hence, we can generate random numbers from the distribution by providing the minimum and the maximum. In general, we can generate random numbers that follow a variety of distributions by providing the required arguments to the particular random number generator needed.\nIn addition to the functions mentioned above, R provides the sample() function, a versatile tool for generating random values from a specified data set. The sample() function allows users to randomly select a specified number of elements from a vector, either with or without replacement. For example, sample(x, size, replace = FALSE) randomly selects elements from the vector x. If replace = TRUE, the function will sample with replacement, meaning the same element can be selected more than once. You can also pass a vector of probabilities using the prob argument, allowing you to generate various observed distributions.\nIn the code below we simulate the toss of a “biased coin” that turns head 60% of the times.\n\nsample(c(\"H\",\"T\"), 5, replace= TRUE, prob=c(0.6,0.4))\n\n[1] \"H\" \"T\" \"T\" \"T\" \"H\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#nightmare-reads-simulates-demand",
    "href": "Tools.html#nightmare-reads-simulates-demand",
    "title": "2  Tools for Working With Simulation",
    "section": "2.3 Nightmare Reads Simulates Demand",
    "text": "2.3 Nightmare Reads Simulates Demand\nNightmare Reads bookstore wishes to determine how many customers will buy their Monster Classic Series. They plan to send \\(100\\) catalogs by mail to potential customers. Before they send the catalogs, they decide to get an estimate on demand. Past data reveals that a customer will buy a book from the catalog with a probability of \\(0.70\\). Using R, they simulate the demand generated by their catalog.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=rbinom(5,100,0.7)))\n\n              Books Price Demand\n1      Frankenstein  9.50     68\n2           Dracula  5.78     66\n3         Moby Dick  6.34     61\n4 War Of The Worlds  5.67     70\n5           Beowulf  2.45     72\n\n\nThe demand for the book is generated using the rbinom() function. The first input of the rbinom() function specifies how many random numbers are needed (\\(5\\)), the second one specifies the number of trials in the experiment (\\(100\\)), and the last one specifies the probability of success (\\(0.7\\)). The bookstore can now assess the Monster Series’s revenues with these demands.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#using-loops-in-r",
    "href": "Tools.html#using-loops-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.4 Using Loops in R",
    "text": "2.4 Using Loops in R\nLoops and conditionals are extremely useful when creating our simulation models. Among the many benefits, they will allow us to quickly generate new variables for our model or test different variations of our parameters to see how the model behaves.\nA loop is a programming construct that allows you to repeat a block of code a specified number of times or until a specific condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a particular condition is true. Let us illustrate the syntax of the for loop by simulating demand for each book Monster Classics series and calculating the total revenue.\n\nRevenue&lt;-c()\n\nfor (i in MS$Price) {\n  Revenue&lt;-c(Revenue,i*rbinom(1,100,0.7))\n  print(Revenue)\n}\n\n[1] 627\n[1] 627.00 416.16\n[1] 627.00 416.16 469.16\n[1] 627.00 416.16 469.16 419.58\n[1] 627.00 416.16 469.16 419.58 173.95\n\n\nThe code above starts by creating an empty vector to store the revenue generated by each book. A for loop is then used to simulate the revenue for each book. The process starts by taking the first price in the MS$Price vector and multiplying it with a single random number drawn from the binomial distribution with \\(100\\) trials and probability \\(0.7\\) (our simulated demand). Note how the code combines the Revenue vector with the revenue generated by the simulation in the code c(Revenue, i*rbinom(1,100,0.7). This process is repeated for every number in the MS$Price vector, leading to a final vector with each book’s revenues.\nWe can avoid the use of the for loop by combining the Demand and Price vectors. Generally speaking, vectorization allows us to perform an operation to the entire vector at once, rather than having to iterate through each element one by one. When possible try to vectorize the problem as this operation is more efficient than using for loops.\n\nRevenue&lt;-MS$Demand*MS$Price\nRevenue\n\n[1] 646.00 381.48 386.74 396.90 176.40",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#using-conditionals-in-r",
    "href": "Tools.html#using-conditionals-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.5 Using Conditionals in R",
    "text": "2.5 Using Conditionals in R\nConditionals allow you to execute different code blocks based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false.\nLet us go back to the Monster Classic example and assume that the bookstore has gained additional insight into the demand for their collection. In particular, assume that if the book is either Frankenstein or Dracula, the probability of a customer buying it is \\(0.9\\) (the probability of the other books remains at \\(0.7\\)). We can now modify our demand simulation using a loop and a conditional.\n\ndemand&lt;-c()\n\nfor (i in MS$Books){\n  if (i==\"Frankenstein\"| i==\"Dracula\"){\n    p=0.9\n  }\n  else {p=0.7}\n  demand&lt;-c(demand,rbinom(1,100,p))\n}\n\nprint(demand)\n\n[1] 85 87 73 69 70\n\n\nIn the code above, the inner conditional checks whether the titles are either Frankenstein or Dracula. If so, the random binomial number is drawn with the probability \\(0.9\\). If not, it is drawn with probability \\(0.7\\). The for loop, goes through all the books in the series and adds a simulated demand. Below is our data frame with the new simulated values.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=demand))\n\n              Books Price Demand\n1      Frankenstein  9.50     85\n2           Dracula  5.78     87\n3         Moby Dick  6.34     73\n4 War Of The Worlds  5.67     69\n5           Beowulf  2.45     70\n\n\nWe can simplify this calculation by using a function to modify the vector of books (i.e. vectorization). In particular, the ifelse() function allows us to take the Books variable and transform it to a vector of demands. The first argument of the ifelse() function is the logical condition (i.e. Books==“Frankenstein”| Books==“Dracula”), the second argument is the action to be taken when the condition holds true (i.e. rbinom(length(MS$Books),100,0.9)) and the last argument is the action to be taken when the condition does not hold (i.e. rbinom(length(MS$Books),100,0.7)). Below is simpler code to generate the demand of books:\n\ndemand&lt;-ifelse(MS$Books==\"Frankenstein\"| MS$Books==\"Dracula\",\n               rbinom(length(MS$Books),100,0.9),\n               rbinom(length(MS$Books),100,0.7))\ndemand\n\n[1] 90 89 69 73 68",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#the-va-department-of-transportation-wants-your-services",
    "href": "Tools.html#the-va-department-of-transportation-wants-your-services",
    "title": "2  Tools for Working With Simulation",
    "section": "2.6 The VA Department of Transportation Wants Your Services",
    "text": "2.6 The VA Department of Transportation Wants Your Services\nThe VA ferry crossing the James River was first established in \\(1925\\). The ferry transports vehicles back and forth from Jamestown to Scotland in a \\(15\\)-minute ride. The VA Department of Transportation wants you to simulate the daily demand for the ferry so that they can schedule staff and the number of ferries to run.\nAssume that the VA Department of transportation shares four weeks of data with hopes of us being able to simulate the fifth week. Can we generate a convincing simulation of the data? The table below records the number of vehicles that used the ferry service:\n\n\n\nDay\nWeek 1\nWeek 2\nWeek 3\nWeek 4\n\n\n\n\nMon\n1105\n1020\n1163\n1070\n\n\nTue\n1128\n1048\n1066\n1145\n\n\nWed\n1189\n1102\n1183\n1083\n\n\nThu\n1175\n1094\n1003\n1045\n\n\nFri\n1101\n1142\n1095\n1018\n\n\nSat\n1459\n1464\n1408\n1443\n\n\nSun\n1580\n1534\n1512\n1599\n\n\n\nOne thing that becomes apparent is that weekdays have less demand for the ferry than weekends. In particular, weekdays ferry rides seem to vary between \\(1000\\) to \\(1200\\) while weekend rides vary between \\(1400\\) to \\(1600\\). We can represent this visually with a histogram. Let’s first create vectors that capture the data.\n\nweekdays&lt;-c(1105,1020,1163,1070,\n            1128,1048,1066,1145,\n            1189,1102,1183,1083,\n            1175,1094,1003,1045,\n            1101,1162,1095,1018)\nweekends&lt;- c(1459,1464,1408,1443,\n            1580,1534,1512,1599)\n\nNow we can visualize the data for weekdays by creating a histogram.\n\nhist(weekdays, main=\"Weekday Ferry Rides\", xlab=\"\",\n     ylab=\"Cars\")\n\n\n\n\n\n\n\n\nThis visual shows that ferry rides are uniformly distributed during the weekdays. Furthermore, if you were to graph the histogram for weekends, you will once again notice that the rides follow a uniform distribution. Using this information, we can now simulate data for week five using a for loop and a conditional.\n\nlibrary(extraDistr)\nset.seed(14)\n\nsim&lt;-c()\n\nfor (i in 1:7){\n  if (i&lt;6){\n    rides&lt;-rdunif(1,1000,1200)\n    sim&lt;-c(sim,rides)\n  } else {\n    rides&lt;-rdunif(1,1400,1600)\n    sim&lt;-c(sim,rides)\n  }\n}\n\nsim\n\n[1] 1051 1128 1192 1111 1197 1502 1587\n\n\nGiven that cars are discrete, we use the discrete uniform distribution random number generator from the extraDistr package. The code sets a for loop that generates seven numbers (one for each day of the week). The conditional ensures that the first five numbers generated (weekdays) draw numbers between \\(1000\\) and \\(1200\\) and that the last two numbers (weekends) are drawn within the (\\(1400\\),\\(1600\\)) interval. The set.seed() function is used so that the reader can replicate the values generated if needed.\nBelow, the table is reproduced once more, but with the simulated values. Can you identify the simulated values from the ones provided by the VA Department of Transportation? Can you generate a similar simulation by just using vector operations and the ifelse()function?\n\n\n\nDay\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\n\n\n\n\nMon\n1105\n1020\n1163\n1070\n1051\n\n\nTue\n1128\n1048\n1066\n1145\n1128\n\n\nWed\n1189\n1102\n1183\n1083\n1192\n\n\nThu\n1175\n1094\n1003\n1045\n1111\n\n\nFri\n1101\n1142\n1095\n1018\n1197\n\n\nSat\n1459\n1464\n1408\n1443\n1502\n\n\nSun\n1580\n1534\n1512\n1599\n1587",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#chi-square-test",
    "href": "Tools.html#chi-square-test",
    "title": "2  Tools for Working With Simulation",
    "section": "2.7 Chi Square Test",
    "text": "2.7 Chi Square Test\nAlthough we have visual confirmation that ferry rides during weekdays are uniformly distributed, we would like to perform a statistical test to prove that this is in fact the case. The Chi-Square Test is generally used to compare observed frequencies with expected frequencies across categorical data. Using this comparison, we can determine if the observed data does in fact follow the hypothesis distribution. Below is code to perform the test:\n\n# Observed frequencies\nobserved &lt;- c(5, 5, 5, 5, 5)\n\n# Expected frequencies under uniform distribution\n# Since we have 5 categories and assuming uniform distribution,\n# each category is expected to have an equal frequency.\ntotal_observations &lt;- sum(observed)\nnum_categories &lt;- length(observed)\nexpected &lt;- rep(total_observations / num_categories, num_categories)\n\n# Perform the Chi-Square test\nchi_square_test &lt;- chisq.test(observed, p = rep(1/num_categories, num_categories))\n\n# Display the test results\nprint(chi_square_test)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0, df = 4, p-value = 1\n\n\nChi-Square Statistic (X-squared) measures the divergence between observed and expected frequencies. Since the observed distribution is the same as the expected frequency, the divergence is 0. The p-value indicates the probability of observing a Chi-Square statistic as extreme as, or more extreme than, the one computed, assuming the null hypothesis (that the distribution is uniform) is true. A small p-value (typically &lt; 0.05) indicates that the observed distribution is significantly different from a uniform distribution. Since the the reported p-value is 1, we can’t reject the null hypothesis and we vote in favor of ferry rides being uniformly distributed.\nNote that the Chi-Square test can be used for any discrete distribution or alternately as an approximation when the distributions are continuous. For testing normality (continuous distribution), other tests like the Shapiro-Wilk (shapiro.test() in R) or Kolmogorov-Smirnov (ks.test()) are more commonly used and better suited than the Chi-Square test.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#readings",
    "href": "Tools.html#readings",
    "title": "2  Tools for Working With Simulation",
    "section": "2.8 Readings",
    "text": "2.8 Readings\nThese reading will help you review the concepts and theory necessary for completing this chapter. Grolemund (2014) reviews the R basics needed to perform computer simulation, Jaggia and Kelly (2022) introduces probability distributions, Gelves (2022) has several applied probability problems (with solutions), while Winston and Albright (2019) provides an application of the distributions to business simulation.\n\nGrolemund (2014) Chapter 1 (The Very Basics), Chapter 2 (Packages and Help Pages), Chapter 3 (R Objects), Chapter 7.2, 7.3 (Conditional Statements), Chapter 9.3, 9.4, 9.5 (Loops).\nJaggia and Kelly (2022) Chapter 5 (Discrete Probability Distributions) and Chapter 6 (Continuous Probability Distributions).\nGelves (2022) Chapter 10 (Discrete Random Variables), Chapter 11 (Continuous Random Variables). This is mainly review from your probability course. It is recommended you attempt the exercises in both chapters (solutions are provided at the end).\nWinston and Albright (2019) Chapter 10.1 (Introduction) and 10.2 (Probability Distributions for Input Variables). Pay special attention to the probability distributions and try to replicate the examples in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#lessons-learned-in-this-chapter",
    "href": "Tools.html#lessons-learned-in-this-chapter",
    "title": "2  Tools for Working With Simulation",
    "section": "2.9 Lessons Learned In This Chapter",
    "text": "2.9 Lessons Learned In This Chapter\n\nGenerate random numbers using R functions.\nUse Loops and Conditionals to simulate variables.\nApply objects, vectors, and data frames to store and manipulate data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Tools.html#exercises",
    "href": "Tools.html#exercises",
    "title": "2  Tools for Working With Simulation",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\n\nSpaceX needs a countdown script for their next spaceship launch. Your task is to create a countdown that starts from 10 and goes down to 1, followed by a message saying “Liftoff!”. Create a for loop that starts at 10 and counts down to 1. After the loop ends, print “Liftoff!” to simulate the spaceship launch.\n\n\n\nSuggested Answer\n\n\nfor (i in 10:1) {\n  print(i)\n}\n\n[1] 10\n[1] 9\n[1] 8\n[1] 7\n[1] 6\n[1] 5\n[1] 4\n[1] 3\n[1] 2\n[1] 1\n\nprint(\"Liftoff!\")\n\n[1] \"Liftoff!\"\n\n\n\n\nThe Triangle Tavern, a popular nightlife spot, needs your help to enforce its strict age policy. The owner has observed that the youngest patrons trying to enter are around 16 years old, and the oldest are typically no more than 25. The bouncers report that 21-year-olds are the most common age group among patrons. Your task is to estimate the proportion of underage patrons that attempt to enter the bar using the triangle distribution. Create two vectors. One named ages that simulates the ages of 1000 customers, and a vector entry_status that uses a conditional statement (using the ifelse() function) to populate the vector with “Access Granted” for ages 21 and older, and “Access Denied” for ages under 21.\n\n\n\nSuggested Answer\n\nUse the rtriang() function from the extraDist package to generate 1,000 ages.\n\nlibrary(extraDistr)\nages&lt;-rtriang(1000,16,25,21)\n\nWe can now use a conditional statement using the ifelse() function to generate a variable to return when the customer has access or not.\n\nentry_status&lt;-ifelse(ages&gt;=21,\"Access Granted\",\"Access Denied\")\n\nLastly, we can report the percentage of underage people that are attempting to enter the bar.\n\nround(sum(entry_status==\"Access Denied\")/length(entry_status),2)\n\n[1] 0.58\n\n\n\n\nThe Bellagio Casino in Las Vegas wants you to create a simulation of their roullete wheel, where players bet on whether the outcome will be red or black. In a standard roulette wheel, there are 18 red slots, 18 black slots, and 2 green slots (0 and 00) out of a total of 38 slots. Your task is to simulate a player betting on red for 100 spins of the roulette wheel by using a for loop and a conditional statement. If the ball lands on a red slot, the player wins and earns 1 dollar. If the ball lands on a black or green slot, the player loses 1 dollar. What is the player’s total winnings or losses after 100 spins? How does the “house edge” (the presence of the green slots) affect the player’s outcome over many spins?\n\n\n\nSuggested Answer\n\nBelow is code that implements both a for loop and a conditional to simulate the roullete.\n\n# Set up the roulette wheel\nwheel &lt;- c(rep(\"red\", 18), rep(\"black\", 18), rep(\"green\", 2))\n\n# Initialize the player's total winnings\ntotal_winnings &lt;- 0\n\n# Simulate the bets using the sample function\nset.seed(10)\nfor (i in 1:100) {\n  # Spin the roulette wheel\n  spin_result &lt;- sample(wheel, 1)\n  \n  # Use Conditional to determine if the player wins or loses\n  if (spin_result == \"red\") {\n    total_winnings &lt;- total_winnings + 1  # Player wins $1\n  } else {\n    total_winnings &lt;- total_winnings - 1  # Player loses $1\n  }\n}\n\nFor this particular simulation (seed set at 10), the player wins/losses a total of -14 dollars. The “house edge” biases the odds in favor of the casino.\n\n\nFranklin and Templeton wants you to generate 4 simulated returns for the SPY (an ETF that tracks the S&P 500). They provide you with the data and histogram shown below. What distribution would you use to simulate the data? What parameters would you use?\n\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/returns.csv\")\ndata %&gt;% filter(Stock==\"TSLA\") %&gt;% ggplot() +\n  geom_histogram(aes(x=Return), bg=\"grey\",col=\"black\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nSuggested Answer\n\nThe distribution of returns looks normal. We can use the mean and the standard deviation of the sample provided as inputs in the rnorm() function.\n\ndata %&gt;% filter(Stock==\"SPY\") %&gt;% ggplot() +\n  geom_histogram(aes(x=Return), bg=\"grey\",col=\"black\") +\n  geom_histogram(aes(x=rnorm(length(Return),mean(Return),\n                             sd(Return))), bg=\"blue\",\n                 alpha=0.5) +\n  theme_classic()\n\n\n\n\n\n\n\n\nAs you can see the simulated demand follows the pattern of the sample returns (i.e. the normal distribution). To retrieve the mean and standard deviation of each investment we can use the code below:\n\ndata %&gt;% group_by(Stock) %&gt;% \n  summarise(Average=mean(Return),\n            SD=sd(Return))\n\n# A tibble: 3 × 3\n  Stock  Average     SD\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 AAPL  0.00173  0.0257\n2 SPY   0.000828 0.0177\n3 TSLA  0.00511  0.0502\n\n\nLastly, we can now use the mean and the standard deviation of SPY to simulate four returns:\n\nrnorm(4,0.00173,0.025667)\n\n[1]  0.025137694 -0.053666308  0.034830434 -0.009258298\n\n\n\n\nXYZ Call Center, a popular customer service provider, wants you to simulate the daily number of calls their agents receive. The company has provided you with each employees call volume for a day and the histogram for the data. Analyze the data to determine the appropriate distribution that can be used to model the number of daily calls. You can determine the parameters for the distribution based on the historical data. Generate four simulated values representing the possible daily call volumes using the chosen distribution and parameters. What distribution would you use to simulate the daily call volumes for XYZ Call Center? What are the parameters of the chosen distribution? What are the four simulated daily call volumes?\n\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/CallCenter.csv\")\ndata %&gt;% ggplot() +\n  geom_histogram(aes(x=Calls), bg=\"grey\",col=\"black\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nSuggested Answer\n\nGiven the context of the problem and data, we can think about using the Poisson distribution. The data shows us number of calls per day for each employee (i.e. number of successes in a given time interval). The mean number of call is 34.079 per day. We can use this parameter to generate simulated data as shown below.\n\ndata %&gt;% ggplot() +\n  geom_histogram(aes(x=Calls), bg=\"grey\",col=\"black\") +\n  geom_histogram(aes(x=rpois(length(Calls),mean(Calls,na.rm=TRUE))),\n                 bg=\"blue\",\n                 alpha=0.5) +\n  theme_classic()\n\n\n\n\n\n\n\n\nThe simulated data seems to resemble the call center data. We can perform a Chi-Squared Test:\n\nobserved &lt;- table(data$Calls)\n\n# Calculate the mean (lambda) for the Poisson distribution\nlambda &lt;- mean(data$Calls, na.rm=TRUE)\n\n# Calculate the expected frequencies for each observed count\nexpected &lt;- dpois(as.numeric(names(observed)), lambda) * sum(observed)\n\n# Perform the Chi-Square test\nchi_square_test &lt;- chisq.test(x = observed, p = expected / sum(expected))\n\n# Display the test results\nprint(chi_square_test)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 35.136, df = 34, p-value = 0.4141\n\n\nGiven the large p-value, we can’t reject the null and vote in favor a Poisson distribution. Now we can generate four simulated number of calls in a day by using the rpois() function and the estimated mean number of calls in a day.\n\nrpois(4,mean(data$Calls,na.rm=TRUE))\n\n[1] 35 31 30 36\n\n\n\n\n\n\n\nGelves, J. Alejandro. 2022. Business Statistics. Bookdown. https://jagelves.github.io/BusinessStats/.\n\n\nGrolemund, Garret. 2014. Hands-on Programming with r. O’Reilly. https://jjallaire.github.io/hopr/#license.\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. Business Statistics. McGraw Hill.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Tools for Working With Simulation</span>"
    ]
  },
  {
    "objectID": "Simulation.html",
    "href": "Simulation.html",
    "title": "3  Simulation in R",
    "section": "",
    "text": "3.1 Roll With It Sushi Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. Every morning, the Sous-Chef purchases fish at the market and any unsold fish at the end of the day gets discarded.\nThe restaurant has contacted you because they are still determining how many people will attend the event and worry about how this will impact their business financially. To ensure they have enough sushi and budget appropriately, they want you to recommend the amount of fish needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people, and without your guidance they feel like this is the best guide in determining the fish needed.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (divisible). Each guest at the festival is entitled to 5 ounces of fish. If the sushi runs out, some customers will not be happy. However, the restaurant has promised to refund their entry fee. Additionally, they have promised to serve their famous Miso soup to every attendee. The cost of making a batch for up to 50 guests is 300 dollars.\nGiven that the festival charges 20 dollars per entry, how much yellow-tail would you recommend the restaurant to purchase to maximize expected profits?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#model-framework",
    "href": "Simulation.html#model-framework",
    "title": "3  Simulation in R",
    "section": "3.2 Model Framework",
    "text": "3.2 Model Framework\nThe restaurant provides you with a lot of information which might be overwhelming at first glance. To make the task less daunting, we should organize/classify the information so that we can create a model. For many business problems, the data can be classified into the following parts:\n\nThe inputs have given fixed values and provide the model’s basic structure. These are values that are most likely to be given and determined.\nThe decision variables are values the decision maker controls. We are usually interested in finding the optimal level of this variable.\nThe calculated values transform inputs and decision variables to other values that help describe our model. These make the model more informative and often are required to derive outputs.\nThe random variables are the primary source of uncertainty in the model. They are often modeled by sampling probability distributions.\nThe outputs are the ultimate values of interest; the inputs, decision variables, random variables, or calculated values determine them.\n\nBelow, you can see how we can classify and input the information in R.\n\nlibrary(extraDistr)\nOrder_Oz&lt;-160 # Decision Variable\n\nPrice_Fish_Oz&lt;-9/16 # Input\nPrice_Miso&lt;-300 # Input\nEntry_Fee&lt;-20 # Input\nFish_Entitled_Oz&lt;-5 #Input\n\nset.seed(20)\nAttendance&lt;-round(rtriang(1,20,50,30),0) # Random Variable\n\nDemand&lt;-Attendance*Fish_Entitled_Oz # Calculated \nAvailable&lt;-Order_Oz # Calculated\nConsumption&lt;-min(Demand,Available) #Calculated\nRevenue&lt;-Consumption/Fish_Entitled_Oz*Entry_Fee# Calculated\nCost&lt;-Order_Oz*Price_Fish_Oz+Price_Miso #Calculated\nProfit&lt;-Revenue-Cost # Outcome\n\nIn the model above, you can see how the inputs have fixed values. These values were given to us by the restaurant and the restaurant has little or no control over them (the market price of the fish, the cost of making Miso soup, etc.). The random variable, captures the source of uncertainty (i.e., how many people attend the event). As you can see we have used here the rtriang() function from the extraDistr package to generate the attendance. We have chosen the triangle distribution since the restaurant has provided us with a lower limit (20), an upper limit (50), and a most likely case for attendance (30). Note also the use of the set.seed() function. This allows you to replicate the random numbers generated.\nThe calculated variables combine inputs, the decision variable, and the random variable to provide insight on how much is fish is expected to be both consumed and available. They also help us determine the profit (output), which is our main guide in knowing whether the decision of ordering \\(x\\) ounces of fish is the “best”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#the-news-vendor-problem",
    "href": "Simulation.html#the-news-vendor-problem",
    "title": "3  Simulation in R",
    "section": "3.3 The News Vendor Problem",
    "text": "3.3 The News Vendor Problem\nNote how the decision variable (Order_Oz) affects directly our outcome (Profit). We can see that it decreases the restaurant’s profit through costs, but also affects revenue through the amount of fish available. This is the heart of the problem. We don’t know how many people will attend, so if the restaurant orders too much fish their profits will go down because their costs are large. However, if they order too little then they will have to issue refunds, which decrease their revenue.\nAs you observe the Profit formula in the code above, you’ll notice the use of the min() function. This function returns the minimum of the Attendance and Consumption. The intuition here is that the restaurant can only collect entry fees for people who consumed sushi when the attendance is greater than the amount of sushi available. Likewise, their revenue will be capped at the total amount of people who attended, even if they ordered plenty of fish.\nThe problem illustrated above is called the news vendor problem. The news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order (and sometimes at what price to sell it). The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#law-of-large-numbers",
    "href": "Simulation.html#law-of-large-numbers",
    "title": "3  Simulation in R",
    "section": "3.4 Law of Large Numbers",
    "text": "3.4 Law of Large Numbers\nBefore we answer the question of how much fish to order, we must realize a couple of flaws of the model we have created. Mainly, we have run the simulation once and it is unlikely (although possible) that the attendance will be exactly the single value simulated by the rtriang() function (\\(44\\)). Instead, we want to provide the restaurant with a set of eventualities. Worst case scenarios (low attendance), best case scenarios (high attendance), and most likely outcome for their decision. This is only possible if we generate several attendance numbers, and see how the profit (output) behaves.\nAn additional problem is that if we tell the restaurant that the expected profit of ordering \\(x\\) amount of fish is \\(y\\), we want to make sure that the average is not biased. Recall that as the sample size of a study increases, the average of the sample will converge towards the true population mean. In other words, as the number of simulations in our model increases, the estimate of the expected profit becomes more accurate. This is known as the Law of Large Numbers.\nThe code below repeats the simulation several times, allowing for different attendance scenarios to arise. Although, there is not a set number of times one should run a simulation to get a good estimate of the mean (or distribution), computers are powerful enough to run thousands if not millions of simulations. Below we run the simulation 10,000 times for illustration purposes.\n\nn&lt;-10000\nV_Order_Oz&lt;-rep(Order_Oz,n)\nV_Price_Fish_Oz&lt;-rep(Price_Fish_Oz,n)\nV_Price_Miso&lt;-rep(Price_Miso,n)\nV_Entry_Fee&lt;-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz&lt;-rep(Fish_Entitled_Oz,n)\n\nset.seed(12)\nV_Attendance&lt;-round(rtriang(n,20,50,30),0)\nV_Demand&lt;-V_Attendance*V_Fish_Entitled_Oz\nAvailable&lt;-V_Order_Oz\n\nV_Consumption&lt;-pmin(V_Demand,Available) \nV_Revenue&lt;-V_Consumption/Fish_Entitled_Oz*Entry_Fee\nV_Cost&lt;-Order_Oz*Price_Fish_Oz+Price_Miso\nV_Profit&lt;-V_Revenue-V_Cost\n\nIn the code above, you will notice that operations have been vectorized to avoid the use of for loops. For example, the order (Order_Oz) has been repeated 10000 times. By doing this, the inputs can now be combined with the randomly generated attendance (V_Attendance). Finally, note as well the use of the pmin() function instead of the min() function. This allows us to apply the min() function to each element (rows) of the vectors.\nFrom the simulation it seems like that the restaurant would make on average a profit of about \\(211.5\\) dollars if they order \\(160\\) ounces of fish. There are however, a couple of questions left unanswered. First, what are the other possible profits when ordering \\(160\\) ounces? Second, is there another amount of fish that would give them a higher expected profit?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#flaw-of-averages",
    "href": "Simulation.html#flaw-of-averages",
    "title": "3  Simulation in R",
    "section": "3.5 Flaw Of Averages",
    "text": "3.5 Flaw Of Averages\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering \\(160\\) ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\n\n\n\n\nAs you can see most of the outcomes are close to \\(250\\) dollars. So a better recommendation to the restaurant would be to inform them that when ordering \\(160\\) ounces, they will most likely get a profit of \\(250\\) dollars. There is a small risk of them making less that \\(100\\) dollars in profit, but that they should not expect more than \\(250\\) dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#optimal-order-amount",
    "href": "Simulation.html#optimal-order-amount",
    "title": "3  Simulation in R",
    "section": "3.6 Optimal Order Amount",
    "text": "3.6 Optimal Order Amount\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a for loop. Below is the code:\n\nOrder_Oz=seq(160,240,10)\nPrice_Fish_Oz&lt;-9/16 \nPrice_Miso&lt;-300 \nEntry_Fee&lt;-20 \nFish_Entitled_Oz&lt;-5 \n\nProfits&lt;-c()\n\nfor (i in Order_Oz){\nn&lt;-100000\nV_Order_Oz&lt;-rep(i,n)\nV_Price_Fish_Oz&lt;-rep(Price_Fish_Oz,n)\nV_Price_Miso&lt;-rep(Price_Miso,n)\nV_Entry_Fee&lt;-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz&lt;-rep(Fish_Entitled_Oz,n)\n\nset.seed(12)\nV_Attendance&lt;-round(rtriang(n,20,50,30),0) \nV_Demand&lt;-V_Attendance*V_Fish_Entitled_Oz \nAvailable&lt;-V_Order_Oz\n\nV_Profit&lt;-pmin(V_Demand,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits&lt;-c(Profits,mean(V_Profit))\n}\n\n(results&lt;-data.frame(Order=Order_Oz,Profits=Profits))\n\n  Order  Profits\n1   160 212.0358\n2   170 225.7342\n3   180 235.1670\n4   190 240.7776\n5   200 243.1558\n6   210 242.8690\n7   220 240.4326\n8   230 236.4392\n9   240 231.4190\n\n\nThis table suggests that ordering \\(160\\) ounces is not optimal. Once again highlighting that the average attendance is not a good estimate of how much fish we should order. Instead, we can see that \\(200\\) ounces of fish should be ordered (this feeds \\(40\\) people) to maximize the expected profits (\\(243\\) dollars).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#sensitivity-analysis",
    "href": "Simulation.html#sensitivity-analysis",
    "title": "3  Simulation in R",
    "section": "3.7 Sensitivity Analysis",
    "text": "3.7 Sensitivity Analysis\nWhat if the price of yellow-tail increases unexpectedly? What if the restaurant wishes to provide each guest with six ounces of fish instead of five? The model we have created above can accommodate these changes easily by just changing the input values. We can additionally, inform the restaurant how sensitive the results are to changes in key inputs or assumptions. This is the idea behind sensitivity analysis.\nSensitivity analysis assesses the robustness of a model or decision by evaluating the impact of changes in certain key input variables on the output of the model or decision. Sensitivity analysis helps to identify which variables are most important and how sensitive the output is to changes in those variables.\nLet’s consider providing each guest with six or seven ounces of fish. How does this change our recommendation? When running our model with different amounts of fish provided, you’ll notice that when each guest gets six ounces of fish, the restaurant should order \\(240\\) ounces yielding an expected profit of \\(220\\) dollars. When each guest gets seven ounces of fish, the restaurant should order \\(270\\) ounces which generates an expected profit of \\(198\\) dollars. Perhaps providing each guest with more fish is a bad idea as profits are highly sensitive to every ounce increase.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#readings",
    "href": "Simulation.html#readings",
    "title": "3  Simulation in R",
    "section": "3.8 Readings",
    "text": "3.8 Readings\nThe readings for this chapter will introduce the application of simulation models to business. Winston and Albright (2019) highlight the technical application of the models using Excel. It is recommended that you follow in R instead. Kwak and Ingall (2007) motivates the use of Monte Carlo simulation for managing project risks and uncertainties.\n\nWinston and Albright (2019) Chapter 10.3 (Simulation and the Flaw of Averages), and 10.4 (Simulation with Built-In Excel Tools). Example 10.4 (Additional Uncertainty at Walton Bookstore) It is recommended you follow along using R instead of Excel.\nKwak and Ingall (2007)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#lessons-learned-in-this-chapter",
    "href": "Simulation.html#lessons-learned-in-this-chapter",
    "title": "3  Simulation in R",
    "section": "3.9 Lessons Learned In This Chapter",
    "text": "3.9 Lessons Learned In This Chapter\n\nIdentify the parts of a simulation model.\nCreate a simulation model in R.\nIdentify the Flaw of Averages.\nFind optimal values in simulation models",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Simulation.html#exercises",
    "href": "Simulation.html#exercises",
    "title": "3  Simulation in R",
    "section": "3.10 Exercises",
    "text": "3.10 Exercises\n\nApple is preparing to launch a special edition smartphone accessory designed exclusively for its latest iPhone. This accessory is only compatible with this specific model, and the company anticipates that demand will peak during the first 12 months following the smartphone’s release. After this period, a new model of the smartphone is expected to be released, rendering the accessory obsolete. The company estimates that demand for the accessory during the twelve-month period is governed by the probability distribution in the table below.\nThe production involves a fixed cost of 20,000 dollars, with a variable cost of 15 dollars per unit produced. Each accessory is sold for 100 dollars. However, any unsold units after the twelve-month period will have no value, as they are not compatible with the newest iPhone.\nThe company is considering producing 10,000 units of the accessory. Using a simulation with 100,000 replications, calculate the expected profit and standard deviation. Additionally, determine the range within which the company can be 90% certain that the actual profit from selling the accessory during the twelve-month period will fall.\n\n\n\n\nDemand (units)\nProbability (%)\n\n\n\n\n6,000\n10%\n\n\n7,000\n20%\n\n\n8,000\n30%\n\n\n9,000\n25%\n\n\n10,000\n15%\n\n\n\n\n\nSuggested Answer\n\nExpected Profit: 645,000\nStandard Deviation: 119,326\n90% Confidence Interval: [430,000, 830,000]\n\n\nAlaska Airlines is preparing for the peak holiday travel season and needs to decide how many seats to overbook on the SEA-SFO route. Overbooking is expected in the airline industry, as some passengers typically do not show up for their flights. The airline has data indicating that the number of no-show passengers on this route follows a normal distribution with a mean of 20 and a standard deviation of 5.\nIf the airline does not overbook enough seats, empty seats result in lost revenue, as the cost of an unused seat is estimated to be $300. However, if the airline overbooks too many seats, it may need to compensate bumped passengers with vouchers worth $600 each. Use 10,000 simulations to determine the optimal number of seats to overbook to minimize the airline’s expected cost. Try possible values from 10 to 30 in increments of 5.\n\n\n\nSuggested Answer\n\nOptimal Overbooking Strategy: 18 seats\nMinimum Expected Cost: $1,633\n\n\nThe Bronco Wine Company is planning to launch a special edition of its finest wine for the upcoming holiday season. The winery faces uncertainty in both customer demand and grape yield, which affects production.\nDemand is modeled by a triangular distribution with a minimum of 500, most likely 1,500, and maximum of 2,500 bottles. Production Costs are 10,000 dollar fixed plus 15 dollars per bottle. Selling Price is 100 dollars per bottle. Salvage Value is 30 per unsold bottle (up to 1,000 bottles). The maximum number of bottles that can be produced is also uncertain and follows a Beta distribution with parameters α = 2 and β = 5, scaled to a maximum capacity of 3,000 bottles. The winery is considering production levels ranging from 1,000 to 3,000 bottles in increments of 500.\nUse 10,000 simulations to determine the optimal number of bottles to produce to maximize the winery’s expected profit. Create a five number summary to describe the distribution of profits at the optimal number of bottles.\n\n\n\nSuggested Answer\n\nOptimal Production Level: 1,500\nExpected Profit: $47,663\nMin: -32500, Q1: 15900, Median:46100, Q3:78020, Max:117500\n\n\nYou are considering an investment in the SPY (S&P 500 ETF) over a 20-year period. Your investment strategy involves making an initial investment, followed by yearly contributions.\nThe key parameters for your investment are as follows:\n\nStock Ticker: SPY (S&P 500 ETF)\nInitial Investment: $1,000\nAverage Yearly Return: 10% (0.1)\nStandard Deviation of Yearly Returns: 18% (0.18)\nYearly Investment: $6,000\nInvestment Period: 20 years\n\nRun 10,000 simulations of your investment over 20 years. Each year, returns are normally distributed with the given average and standard deviation. Track the value of your investment at the end of each year. Calculate the following statistics for the investment value at the end of 20 years:\n\nAverage (mean) end value\nWorst Case Scenario: Minimum\nBear Case: 25th percentile (Q1)\nBase Case: Median (50th percentile)\nBull Case: 75th percentile (Q3)\nBest Case Scenario: Maximum\n\nBased on your simulation results, how much can you expect to have in your account after 20 years on average? What are the Bear, Base and Bull case predictions?\n\n\n\nSuggested Answer\n\nExpected End Value after 20 years: $380,000\nBear Case: $233,000\nBase Case: $332,000\nBull Case: $476,000\n\n\nYou are tasked with simulating the price of BTC (Bitcoin) over one year using Brownian motion and plotting the result. Brownian motion is a mathematical model used to describe random motion, often applied in finance to simulate stock price movements. In particular, the returns of a stock are described by the following equation: \\(\\text{returns} = (\\mu - 0.5 \\times \\sigma^2) \\times dt + \\sigma \\times \\sqrt{dt} \\times \\epsilon\\); where \\(\\epsilon\\) is a random shock drawn from a standard normal distribution, \\(\\mu\\) is the expected annual return, \\(\\sigma^2\\) is the variance of returns, and \\(dt = 1/365\\). Assume that the starting Bitcoin price is 50,000 dollars, with an average return of 40% and standard deviation of 100%. Let the path of Bitcoin be given by: \\(Price=S0*cumprod(1+returns)\\); where \\(S0\\) is the initial price. Generate 1000 price paths using Brownian motion (set seed to 150). Report the mean, bear (Q1), base (Q2) and bull cases (Q3) of the final prices after 1 year.\n\n\n\nSuggested Answer\n\nExpected End Value 1 year (365 days): $48,293\nBear Case (Q1): $13,642\nBase Case (Q2): $27,306\nBull Case (Q3): $58,680\n\n\n\n\n\nKwak, Young, and Lisa Ingall. 2007. “Exploring Monte Carlo Simulation Applications for Project Management.”\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simulation in R</span>"
    ]
  },
  {
    "objectID": "Tools2.html",
    "href": "Tools2.html",
    "title": "4  Time Series Tools",
    "section": "",
    "text": "4.1 Chilango’s Kitchen Wants You to Forecast Avocado Prices\nChilango’s Kitchen specializes in tacos and burritos made to order in front of the customer. Guacamole is the perfect pairing to their delicious food and one of the restaurant’s best sellers. Their guac uses just six ingredients: avocados, lime juice, cilantro, red onion, jalapeño, and kosher salt. Because of its popularity, each restaurant goes through approximately five cases of avocados daily, amounting to more than \\(44,000\\) pounds annually. Chilango’s Kitchen wants you to provide insights on the price of avocados in California.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#sec-Avocado",
    "href": "Tools2.html#sec-Avocado",
    "title": "4  Time Series Tools",
    "section": "4.2 The Avocado Data Set",
    "text": "4.2 The Avocado Data Set\nThe Hass Avocado Board provides the data set and contains weekly retail scan data of U.S. retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The data reflects an expanded, multi-outlet reporting aggregating the following channels: grocery, mass, club, drug, dollar, and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. Other avocados (e.g. greenskins) are not included in this data. You can find the data here.\nNote: In general the data is recorded weekly. However, there is an entry on 01/01/2018, that is right after 12/31/2017. This is a single observation that is not weekly. There are also missing dates from 12/02/2018-12/31/2018.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "href": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "title": "4  Time Series Tools",
    "section": "4.3 Loading tidyverse and Inspecting the Data.",
    "text": "4.3 Loading tidyverse and Inspecting the Data.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado&lt;-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe code above imports the data as a tibble (a data structure similar to a data frame) and saves it in an object named avocado. The output informs us that three variables are classified as a character, while the rest are double. You can preview the data with either the spec() or glimpse() commands.\n\nspec(avocado)\n\ncols(\n  date = col_character(),\n  average_price = col_double(),\n  total_volume = col_double(),\n  `4046` = col_double(),\n  `4225` = col_double(),\n  `4770` = col_double(),\n  total_bags = col_double(),\n  small_bags = col_double(),\n  large_bags = col_double(),\n  xlarge_bags = col_double(),\n  type = col_character(),\n  year = col_double(),\n  geography = col_character()\n)\n\n\nYou will notice that the date variable is of type character. We can use the lubridate package to coerce this variable to a date. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function. You can learn more about this package in Wickham (2017) Chapter 18.\n\nlibrary(lubridate)\navocado$date&lt;-mdy(avocado$date)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#piping-and-dplyr",
    "href": "Tools2.html#piping-and-dplyr",
    "title": "4  Time Series Tools",
    "section": "4.4 Piping and dplyr",
    "text": "4.4 Piping and dplyr\ndplyr is commonly used with “piping”. Generally speaking, “piping” allows us to chain functions. “Piping” (%&gt;% or |&gt;) passes the object on the left of the pipe as the first argument in the function to the right of the pipe. We can illustrate this using the select() and arrange() functions.\n\navocado %&gt;% select(c(average_price,geography)) %&gt;%\n  arrange(desc(average_price)) %&gt;% head(5)\n\n# A tibble: 5 × 2\n  average_price geography           \n          &lt;dbl&gt; &lt;chr&gt;               \n1          3.25 San Francisco       \n2          3.17 Tampa               \n3          3.12 San Francisco       \n4          3.05 Miami/Ft. Lauderdale\n5          3.04 Raleigh/Greensboro  \n\n\nThere is a lot to explain in this line of code. Let’s start with the functions used. The select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data in ascending order, and the desc() function is nested inside to sort in descending order.\nLet’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%&gt;%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. The data frame is sorted in descending order so that the highest average avocado price is displayed first. Finally, the head() function is used to retrieve the top five entries.\nAs noted in Section 4.2, there is an additional date in the data set that is between weeks (2018-01-01). We can remove this observation by using the filter() function.\n\navocado %&gt;% filter(date!=ymd(\"2018-01-01\")) -&gt; avocado\n\nYou should notice that whereas the select() function chooses particular variables (i.e., columns), the filter() function chooses rows of the tibble that meet the conditions listed. Note also that the filtered data set is assigned (-&gt;) to avocado overwriting the older object.\nThe examples above highlight the use of dplyr functions to transform your data. There are plenty of other functions, but learning these are outside the scope of this book. To find out more, I recommend reading Wickham (2017) Chapter 4. Let’s use the filter() and select() functions once more to retrieve California’s average price of organic avocados for 2015-2018.\n\navocado %&gt;% \n  filter(geography==\"California\", type==\"organic\", year&lt;=2018) %&gt;% \n  select(date, average_price) -&gt; cali\n\nWhen exploring time series data, it is important to standardize the time interval between observations. The code below calculates the number of days between observations.\n\ncali %&gt;% pull(date) %&gt;% diff()\n\nTime differences in days\n  [1] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n [38] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n [75] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n[112] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n[149] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n[186] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n\n\nThe pull() function vectorizes the date variable, making it possible to apply the diff() function. The result confirms that all observations are seven days apart and that no missing observations or duplicates exist.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#sec-Visual",
    "href": "Tools2.html#sec-Visual",
    "title": "4  Time Series Tools",
    "section": "4.5 Visualizing The Data",
    "text": "4.5 Visualizing The Data\nTo visualize the data, we will be using ggplot2. One of the main functions in ggplot2 is the aes() function. This function sets the plotting canvas and determines the mapping of variables. The geom_line() function specifies the type of plot and is added to the canvas with the plus sign +. In time series, we will use the line plot regularly. Labels for the graph are easily set with the labs() function and there are plenty of themes available to customize your visualization. Below, the theme_classic() is displayed. To learn more about the ggplot package, you can refer to Wickham (2017) Chapter 2. Below is the code to create a line plot of California’s average avocado price.\n\nggplot(data=cali) + \n  geom_line(mapping=aes(x=date,y=average_price),\n            color=\"black\") +\n  theme_classic() + \n  labs(x=\"\",\n       y=\"Average Price\", \n       title=\"Organic Avocado Price in California\",\n       subtitle=\"2015-2018\",\n       caption=\"https://hassavocadoboard.com\")  \n\n\n\n\n\n\n\n\nThe average price of avocados in California increased during the period considered. It reached a maximum of about \\(2.60\\) in \\(2017\\) and a minimum of \\(1.10\\) in \\(2015\\). There is also a seasonal pattern with low prices at the beginning of the year and peaks mid-year. In upcoming chapters, we will extrapolate these patterns and use them to forecast time series.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#tsibbles",
    "href": "Tools2.html#tsibbles",
    "title": "4  Time Series Tools",
    "section": "4.6 tsibbles",
    "text": "4.6 tsibbles\nWhen analyzing time series, time plays a central role. Consequently, we will use a data structure to handle a time series called a tsibble (time series tibble). tsibbles are defined by a time index (i.e., the date) that has a standard interval (i.e., days, weeks, months, and years) and keys (i.e., dimensions that do not vary with time). In the avocado data set, we are mainly interested in the average price of the avocados. You will note that the prices are classified by location (geography) and type (organic and conventional). You can learn more about tsibbles here.\ntsibbles, as well a variety of packages that help us analyze time series, are part of the fpp3 package. Below we load the package, and coerce our avocado tibble to a tsibble.\n\nlibrary(fpp3)\navocado %&gt;%\n  as_tsibble(key=c(type, geography),\n           index=date) %&gt;%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-&gt; avocadots\n\nIn the code above, the as_tsibble() function was called with the parameter regular set at true, indicating that the date has no gaps and occurs every week (the greatest common divisor of the index column). Additionally, the type of avocado and the geography do not vary with time and are classified as a key. The function filter_index() is called as it allows us to determine the window for analysis. As noted in Section 4.2, there are some missing dates in December 2018. We limit the analysis for now to 2015-2018.\nRecall, that we are interested in the average price of avocados for California. We can specify the tsibble for analysis, by using dplyr.\n\navocadots %&gt;% filter(geography==\"California\", type==\"organic\") %&gt;%\n  select(date,average_price) -&gt; calits",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#time-series-decomposition",
    "href": "Tools2.html#time-series-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.7 Time Series Decomposition",
    "text": "4.7 Time Series Decomposition\nAs highlighted in Section 4.5, the average price of avocados in California has a trend and a seasonal pattern. There are methods available to tease these components and make them more apparent. STL (Season Trend decomposition using LOESS) decomposes the series into a trend, seasonality, and an error (unexplained) component. It is easy to run this method in R using the fable package.\nIn practice, the decomposition is constructed in several steps. First, a moving average of the series is calculated to track the trend. The trend is then subtracted from the series to obtain a de-trended series. The seasonal component is calculated by averaging the values based on the window provided (52 weeks or yearly) for the de-trended series. The error is the remaining series fluctuation that is not explained by the trend or the seasonal component (\\(Series-Trend-Seasonal=Error\\)). In the end, each component can be graphed and displayed, as illustrated below.\n\ncalits %&gt;%\n  model(STL=STL(average_price~trend(window=199)+\n              season(window=51), robust=TRUE)) %&gt;%\n  components() %&gt;% autoplot()+ theme_classic()\n\n\n\n\n\n\n\n\nfable allows us to construct this model easily by using the model() function. This function will allow us to estimate a variety of time series models, so we will be using it regularly. The particular model we are running is STL, hence the use of the STL() function within the model() function. We define the model by specifying the dependent variable (i.e., average_price) followed by a tilde (~) and the components. As you can see, the window argument in the trend() function is set to a relatively large value. By doing this the moving average reflects the general direction of the series and avoids the small fluctuations of the data. The season() function specifies \\(51\\) weeks to capture the yearly seasonality. Finally, the robust parameter is set to true to make the fitting process less sensitive to outliers or influential data points. Experiment by changing the window parameter to see how the decomposition changes. You can learn more about decomposition and these functions in Chapter 3 of Hyndman (2021).\nAs shown above, the trend is increasing, and the seasonal component confirms low levels at the beginning of the year and high levels in the summer. These are two general patterns that determine the price of avocados in California and provide valuable insight to share with Chilango’s Kitchen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#readings",
    "href": "Tools2.html#readings",
    "title": "4  Time Series Tools",
    "section": "4.8 Readings",
    "text": "4.8 Readings\nReadings get you familiar with the techniques used in time series analysis. Hyndman (2021) is the main reading here. It provides a good introduction to forecasting, plotting time series and decomposition. The book makes use of fable to conduct STL decomposition and tsibbles to capture the data. Most of time series analysis is made easier in R by using tidyverse. Wickham (2017) provides an excellent introduction to some of the packages included in tidyverse such as dplyr, ggplot, and lubridate.\n\nHyndman (2021) Chapter 1 (Getting Started), and Chapter 2 (Time Series Graphics), and Chapter 3 (Time Series Decomposition).\nWickham (2017) Chapter 2 (Data Visualization), Chapter 4 (Data Transformation), and Chapter 18 (Dates and Times).\ntsibble: https://tsibble.tidyverts.org\nfable: https://fable.tidyverts.org\ntidyverse: https://tidyverse.tidyverse.org",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#lessons-learned-in-this-chapter",
    "href": "Tools2.html#lessons-learned-in-this-chapter",
    "title": "4  Time Series Tools",
    "section": "4.9 Lessons Learned in This Chapter",
    "text": "4.9 Lessons Learned in This Chapter\nIn this module you have been introduced to data wrangling, plotting, tsibbles and time series decomposition. You have learned how to:\n\nManipulate dates with lubridate.\nSelect and filter variables using dplyr.\nPlot time series using ggplot.\nApply tsibbles in time series analysis.\nDecompose a series using the model() and STL() functions in fable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Tools2.html#exercises",
    "href": "Tools2.html#exercises",
    "title": "4  Time Series Tools",
    "section": "4.10 Exercises",
    "text": "4.10 Exercises\n\nImport the following data as a tsibble with yearly data: https://jagelves.github.io/Data/StrawberryPrice.csv. Graph the time series using ggplot2 and conduct time series decomposition using the fpp3 package.\n\n\n\nSuggested Answer\n\nBelow is code to load the libraries, and import the data:\n\nlibrary(tidyverse)\nlibrary(fpp3)\n\nSB&lt;-read_csv(\"https://jagelves.github.io/Data/StrawberryPrice.csv\")\n\nNow we can create a tsibble with the following command:\n\nSB %&gt;% mutate(Year=year(mdy(DATE))) %&gt;%\n  mutate(Price=as.double(Price)) %&gt;% \n  group_by(Year) %&gt;% \n  summarise(APrice=mean(Price, na.rm=T)) %&gt;% \n  tsibble(index=Year)-&gt; SB_ts\n\nThe first mutate coerces the DATE variable to a date and extracts the year. The next mutate coerces the Price variable to a double. Lastly, the data is grouped by years and collapsed into an average. To graph we use the autoplot() command:\n\nlibrary(ggthemes)\nSB_ts %&gt;% autoplot(.vars=APrice) +\n  theme_clean() +\n  labs(title=\"Yearily Average Price of Strawberries\",\n       y=\"\",x=\"\")\n\n\n\n\n\n\n\n\nThe decomposition is achieve by using the model() function:\n\nSB_ts %&gt;% model(STL=STL(APrice ~ trend() + season(), \n                        robust=T)) %&gt;% \n  components() %&gt;% autoplot() + theme_classic()\n\n\n\n\n\n\n\n\n\n\nUse the dataset found below: https://jagelves.github.io/Data/BTC_USD.csv This dataset has daily Bitcoin prices for the past few years but you should focus only on 2024. Your task is to calculate two types of moving averages using the Adj Close variable, the 50-day and 200-day moving averages. After calculating these averages, you will graph the data and identify the number of times a Golden Cross and a Death Cross have occurred. A Golden Cross is a bullish signal, indicating that a stock’s price might continue to rise. It occurs when a shorter-term moving average (like the 50-day moving average) crosses above a longer-term moving average (like the 200-day moving average). A Death Cross is the opposite, representing a bearish signal and continuation to the downside. It happens when a shorter-term moving average (50-day) crosses below a longer-term moving average (200-day). How many times did these indicators guide investors correctly?\n\n\n\nSuggested Answer\n\nStart by importing the data and defining a tsibble:\n\nlibrary(tidyverse)\nlibrary(fpp3)\n\nBTC&lt;-read_csv(\"https://jagelves.github.io/Data/BTC_USD.csv\")\n\nBTC_ts &lt;- BTC %&gt;%\n  mutate(Date = mdy(Date)) %&gt;% \n  as_tsibble(index=Date) -&gt; BTC_ts\n\nBTC_ts %&gt;% filter_index(\"2024-01-01\"~.) -&gt; BTC_ts\n\nWe can now create the moving averages by using the slide_dbl() function.\n\nBTC_ts %&gt;% \n  mutate(MA50=slider::slide_dbl(`Adj Close`,mean,\n                              .before=20, .after=30, \n                              .complete=F)) %&gt;% \n  mutate(MA200=slider::slide_dbl(`Adj Close`,mean,\n                              .before=100, .after=100, \n                              .complete=F))-&gt; BTC_ts\n\nNow we can plot using the ggplot2 package:\n\nlibrary(ggthemes)\nBTC_ts %&gt;% autoplot(.vars=`Adj Close`,col=\"blue\",alpha=0.5) +\n  autolayer(object = BTC_ts,MA50) +\n  autolayer(object = BTC_ts,MA200,col=\"red\") +\n  theme_clean() +\n  labs(title= \"BTC Price 2024\",\n       x=\"\",y=\"\")\n\n\n\n\n\n\n\n\nThe graph shows a golden cross at the beginning of March and a death cross mid June. The golden cross was followed by an increase in the price of BTC, and the death cross was followed by a decline in the price of BTC.\n\n\nThe data file: https://jagelves.github.io/Data/ElectricityBill.csv contains energy consumption from a typical U.S. household. Import the data, graph the Bill Amount variable and perform STL decomposition. Is there a clear seasonal pattern?\n\n\n\nSuggested Answer\n\nTo import the data and create a tsibble use the following code:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nrm(list=ls())\nElec&lt;-read_csv(\"http://jagelves.github.io/Data/ElectricityBill.csv\")\n\nElec %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  select(Date,`Bill Amount`) %&gt;% \n  as_tsibble(index=Date)-&gt; Elec_ts\n\nThe graph can be created using the autoplot function.\n\nlibrary(ggthemes)\nElec_ts %&gt;% autoplot(.vars=`Bill Amount`) + theme_clean() +\n  labs(title=\"Energy Bill In Dollars\",\n       y=\"\",x=\"\")\n\n\n\n\n\n\n\n\nThe decomposition is performed using the model() and STL() functions:\n\nElec_ts %&gt;% model(STL=STL(`Bill Amount`~trend()+season())) %&gt;% \n  components() %&gt;% autoplot() + theme_clean()\n\n\n\n\n\n\n\n\nThe graph above shows a clear seasonal pattern. The energy bill is at its highest in the month of January, and lowest in the month of May. In general, winter and summer have higher bills than fall and spring.\n\n\n\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nWickham, Hadley. 2017. R for Data Science. O’Reilly. https://r4ds.hadley.nz.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series Tools</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html",
    "href": "Benchmarks.html",
    "title": "5  Model Benchmarks",
    "section": "",
    "text": "5.1 Benchmarks\nOne of the most intuitive (but naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A naive prediction sets the prediction of a future period to the value of the preceding period. For example, if you consider the task of predicting your weight, a simple heuristic would be to think that your weight tomorrow be the same as the weight observed today. Mathematically we would write:\nwhere \\(\\hat y_{T+h}\\) is the predicted value for \\(h\\) periods ahead, and \\(y_T\\) is the value observed at the current time period \\(T\\). We can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). Thinking about weight once again, we note that as kids grow, we expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. We would “drift” the naive prediction upward. Mathematically we would write:\nwhere \\(h(\\frac{y_t-y_1}{T-1})\\) can be thought as the average increase of \\(y\\) from period \\(1\\) to the current period \\(T\\). One could also predict weight by observing weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean could be a good predictor of your future weight. Formally:\nLastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of our diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#sec-Bench",
    "href": "Benchmarks.html#sec-Bench",
    "title": "5  Model Benchmarks",
    "section": "",
    "text": "\\(\\hat y_{T+h}=y_T\\)\n\n\n\n\\(\\hat y_{T+h}=y_T+h(\\frac{y_t-y_1}{T-1})\\)\n\n\n\n\\(\\hat y_{T+h}=\\frac{(y_1+y_2+...+y_T)}{T}\\)\n\n\n\n\\(\\hat y_{T+h}=b_0+b_1(T+h)\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#modeling-the-the-average-price-of-avocados",
    "href": "Benchmarks.html#modeling-the-the-average-price-of-avocados",
    "title": "5  Model Benchmarks",
    "section": "5.2 Modeling the the Average Price of Avocados",
    "text": "5.2 Modeling the the Average Price of Avocados\nLet’s apply these four models to forecast the average price of avocados in California. We’ll start by loading the tidyverse and fpp3 packages and importing the data.\n\nlibrary(tidyverse)\nlibrary(fpp3)\ncali&lt;-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\nRecall that we can create a tsibble from the csv file using the as_tsibble() function. The code below selects the variable of interest and the filter_index() function is used to focus our analysis for 2015-01-04~2018-06-02 with 2018-06-02 not being included.\n\ncali %&gt;%\n  select(date,average_price) %&gt;%\n  as_tsibble(index=date) %&gt;%\n  filter_index(\"2015-01-04\"~\"2018-06-02\") -&gt; calits_train\n\nNow we can use the model() function to run the benchmarks discussed in Section 5.1. We have saved the models to an object called fit.\n\nfit &lt;- model(calits_train,mean=MEAN(average_price),\n              Naive=NAIVE(average_price),\n              Drift=RW(average_price~drift()),\n              LS=TSLM(average_price~trend()))\n\nThe fit object is saved as a mable (model table). The model()function specifies the four models to be estimated using their respective functions (i.e., MEAN(), NAIVE(), RW(), and TSLM()). To explore the coefficients of the models estimated, we use the coef() function with fit as its single argument. The output table has been enhanced visually by using the gt package.\n\nlibrary(gt)\ncoef(fit) %&gt;% \n  gt() %&gt;% \n  cols_align(\"center\") %&gt;% \n  tab_header(title = md(\"**Model Coefficients For The Avocado Data**\")) %&gt;% \n  tab_style(\n    locations =cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(statistic,estimate,std.error,p.value),\n             decimals = 4)\n\n\n\n\n\n\n\nModel Coefficients For The Avocado Data\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nmean\nmean\n1.6845\n0.0212\n79.3980\n0.0000\n\n\nDrift\nb\n0.0021\n0.0104\n0.2010\n0.8410\n\n\nLS\n(Intercept)\n1.4856\n0.0391\n38.0158\n0.0000\n\n\nLS\ntrend()\n0.0022\n0.0004\n5.8702\n0.0000\n\n\n\n\n\n\n\nThe table records the estimates and p-values for all the benchmarks discussed in Section 5.1. The Naive model has no entry, as the forecast is created by using the previous period’s observed value. Note as well that the Drift and LS models select a positive slope to account for the trend. Below we illustrate the fit of the Mean model by a dashed blue line, the Least Squares model by the red line and the Naive model by the orange line.\n\ncalits_train %&gt;% autoplot(average_price) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %&gt;% filter(`.model`==\"LS\")) +\ngeom_line(aes(y = .fitted), col=\"orange\",\n            data = augment(fit) %&gt;% filter(`.model`==\"Naive\")) +\n  geom_line(aes(y = .fitted), col=\"blue\", linetype=\"dashed\",\n            data = augment(fit) %&gt;% filter(`.model`==\"mean\")) +\n  labs(y=\"\", title= \"California's Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - May 27, 2018\",\n       x=\"\")\n\n\n\n\n\n\n\n\nThe graph illustrates how closely the Naive model follows the data. This might seem like a good model, but consider how the heuristic makes a mistake every period. Since average prices are constantly changing every week, predicting the previous value always results in an error. Critically, the Naive prediction does not explain the series governing process. The LS model, on the other hand, provides some insight into a force that is influencing the data—a rising trend. We can use characteristics such as a trend or seasonality to forecast a series effectively.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#model-fit",
    "href": "Benchmarks.html#model-fit",
    "title": "5  Model Benchmarks",
    "section": "5.3 Model Fit",
    "text": "5.3 Model Fit\nThe model fit will be assessed by comparing the fitted values against observed values. In general, a good fit is determined by how far the fitted values are from the observed ones. If we square all of the distances between actual points and predicted values (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE).\n\n\\(MSE = \\frac{ \\sum (\\hat{y}_t-y_t)^2}{T}\\)\n\n How we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE’s but instead find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since all the accuracy metrics are the smallest. We highlighted these results and made the table more appealing using the gt library.\n\naccuracy(fit) %&gt;% \n  gt() %&gt;%\n  cols_align(\"center\") %&gt;% \n  tab_header(title = md(\"**Model Fit**\")) %&gt;% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %&gt;% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"Naive\",\n    targets =\"row\")\n\n\n\n\n\n\n\nModel Fit\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\nACF1\n\n\n\n\nmean\nTraining\n0.00\n0.28\n0.23\n−2.71\n13.59\n2.26\n2.05\n0.87\n\n\nNaive\nTraining\n0.00\n0.14\n0.10\n−0.19\n6.04\n1.00\n1.00\n−0.21\n\n\nDrift\nTraining\n0.00\n0.14\n0.10\n−0.31\n6.05\n1.00\n1.00\n−0.21\n\n\nLS\nTraining\n0.00\n0.26\n0.21\n−2.21\n12.23\n2.07\n1.87\n0.85",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#forecast",
    "href": "Benchmarks.html#forecast",
    "title": "5  Model Benchmarks",
    "section": "5.4 Forecast",
    "text": "5.4 Forecast\nThe forecast of the series is obtained by using the forecast() function and specifying the number of periods (\\(h\\)) ahead to forecast. Below we forecast \\(27\\) weeks and save the result in an object called calits_fc.\n\ncalits_fc &lt;- fit %&gt;% forecast(h=27)\n\nThe autoplot() and autolayer() functions are used below to create a graph with the forecasts and the training set. The argument level is set to NULL to omit the prediction intervals.\n\ncalits_fc %&gt;% autoplot(level=NULL) + theme_classic() + \n  autolayer(calits_train, average_price) +\n  labs(y=\"\", title= \"California's Forecasted Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - Dec 2, 2018\",\n       x=\"\")\n\n\n\n\n\n\n\n\nNote how the Mean and Naive models predict that the series will continue without a trend. The LS and Drift models predicts that the series will continue its trend but, like all other methods, do not consider the seasonal pattern that is evident in the average price of avocados. In future chapters, we will look at models that account for both trend and seasonality.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#over-fitting",
    "href": "Benchmarks.html#over-fitting",
    "title": "5  Model Benchmarks",
    "section": "5.5 Over-Fitting",
    "text": "5.5 Over-Fitting\nOver-fitting can happen when a model is overly flexible. This can make the model fit to the random fluctuations or noise in the data, rather than the underlying pattern. This is a major failing in modeling as it ignores the systematic pattern that governs the time series.\nTo overcome this problem, we usually have a training set or subset of the data that we use to estimate the model’s parameters. Once the model is estimated, we assess its performance on a new subset of the data that was not used in estimating the model. This second subset of data is called the test set. A model that over-fits to the training data, will often perform poorly when forecasting the test set.\nRecall that benchmarks were estimated for the period between 2015-01-04~2018-06-02. We will call this our training set. For our test set, we’ll use the 2018-06-02~2018-12-02 period. The code below creates a test set and a set that includes both the test set and training set using the filter_index() function.\n\ncali %&gt;%\n  as_tsibble(index=date) %&gt;%\n  filter_index(\"2018-06-02\"~\"2018-12-02\") -&gt; calits_test\n\ncali %&gt;%\n  as_tsibble(index=date) %&gt;%\n  filter_index(.~\"2018-12-02\") -&gt; calits\n\nNow we can plot the training set, the forecast, and the test set by using the code below.\n\ncalits_fc %&gt;% autoplot(level=NULL) + \n  theme_classic() + \n  autolayer(calits_train, average_price) + \n  autolayer(calits_test, average_price)\n\n\n\n\n\n\n\n\nThe graph shows how the LS method does well with the test data and a long forecast period. This can be confirmed by obtaining the accuracy measures against the test set. The code below uses the accuracy() function to generate the main table.\n\naccuracy(calits_fc, calits_test) %&gt;% select(-ACF1, RMSSE, MASE)\n\n\n\n\n\n\n\n\n\nModel Fit\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\n\n\n\n\nDrift\nTest\n0.17\n0.21\n0.17\n9.24\n9.24\n\n\nLS\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\n\n\nNaive\nTest\n0.20\n0.24\n0.20\n10.85\n10.85\n\n\nmean\nTest\n0.13\n0.18\n0.13\n6.72\n6.85\n\n\n\n\n\n\n\nInterestingly, the Naive method is no longer the best model since it will always predict the series’ previous value regardless of how many periods we forecast. On the other hand, the LS model correctly uses the deterministic trend to forecast the future. Trends are useful in predicting time series.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#cross-validation",
    "href": "Benchmarks.html#cross-validation",
    "title": "5  Model Benchmarks",
    "section": "5.6 Cross Validation",
    "text": "5.6 Cross Validation\nInstead of selecting a single training set and test set, we can create several. Specifically, we could take the first three observations of our time series and define them as the training set. We can then estimate a model and forecast the fourth (or nth) observation. The forecast error is recorded and the training set is changed so that now the first four observations are used to estimate the model and forecast the fifth (or nth) observation. This procedure is repeated as many times as the data allows. Below we create a table that enables us to follow the cross-validation of our benchmarks.\n\navocado_cv &lt;- calits_train %&gt;%\n  stretch_tsibble(.init = 3, .step = 1)\n\nstretch_tsibble() is a handy function that creates a variable called id that is initialized with the .init argument. In this case, the first three observations are given \\(id=1\\). The id then changes with a step of \\(.step=1\\). That is, \\(id=2\\) for the first four observations, then \\(id=3\\) for the first five observations, and so on. Below is a sample of the tsibble.\n\n\n\n\n\n\n\n\nCV tsibble\n\n\ndate\naverage_price\n.id\n\n\n\n\n2015-01-04\n1.24\n1\n\n\n2015-01-11\n1.10\n1\n\n\n2015-01-18\n1.24\n1\n\n\n2015-01-04\n1.24\n2\n\n\n2015-01-11\n1.10\n2\n\n\n2015-01-18\n1.24\n2\n\n\n2015-01-25\n1.30\n2\n\n\n2015-01-04\n1.24\n3\n\n\n\n\n\n\n\nUsing this new tsibble, the benchmarks are estimated for each id and forecasts are generated for one period ahead (\\(h=1\\)). The accuracy is measured and averaged across all iterations for each model. Results are shown in the table below.\n\navocado_cv %&gt;%\n  model(Mean=MEAN(average_price),\n        Naive=RW(average_price),\n        Drift=RW(average_price ~ drift()),\n        LS=TSLM(average_price~date)) %&gt;%\n  forecast(h = 1) %&gt;% accuracy(calits) %&gt;% \n  gt() %&gt;%\n  cols_align(\"center\") %&gt;% \n  tab_header(title = md(\"**Model Fit Cross Validation**\")) %&gt;% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %&gt;% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"Naive\",\n    targets =\"row\")\n\n\n\n\n\n\n\nModel Fit Cross Validation\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\nACF1\n\n\n\n\nDrift\nTest\n0.00\n0.14\n0.10\n−0.58\n6.09\n1.01\n1.01\n−0.21\n\n\nLS\nTest\n−0.05\n0.27\n0.21\n−4.64\n12.35\n2.05\n1.94\n0.85\n\n\nMean\nTest\n0.12\n0.29\n0.21\n5.36\n11.73\n2.12\n2.07\n0.85\n\n\nNaive\nTest\n0.00\n0.14\n0.10\n−0.15\n5.96\n0.99\n1.00\n−0.21\n\n\n\n\n\n\n\nThe Naive method performs the best when forecasting one period ahead. However, we note once again that the Naive method will provide the same forecast for one, two, three or more periods ahead. You can confirm that this model would lose its appeal when predicting several periods ahead. Most importantly, there is no formal model telling us how data is generated.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#other-accuracy-measures",
    "href": "Benchmarks.html#other-accuracy-measures",
    "title": "5  Model Benchmarks",
    "section": "5.7 Other Accuracy Measures",
    "text": "5.7 Other Accuracy Measures\nAIC (Akaike Information Criterion), AICc (corrected AIC), and BIC (Bayesian Information Criterion) are commonly used measures of model accuracy or goodness of fit in statistical modeling. They are used to compare different models and select the one that best balances model complexity (number of parameters estimated) and fit.\n\nAIC is a measure that penalizes model complexity. It balances the trade-off between model fit and the number of parameters in the model. The AIC value is calculated using the formula:\n\n\n\\(AIC = T*ln(SSE/T) + 2(k+2)\\)\n\n where \\(T\\) is the number of observations, \\(SSE\\) the sum squared errors, and \\(k\\) is the number of predictors (i.e., complexity of the model). The lower the AIC value, the better the model.\n\nAICc is an adjustment to the AIC measure, particularly for smaller sample sizes. AIC tends to overestimate the complexity penalty when the number of data points is relatively small. AICc adds a correction factor to account for this and is calculated using the formula:\n\n\n\\(AICc = AIC + \\frac{2(k+2)(k+3)}{T-k-3}\\)\n\n AICc provides a more accurate measure of model fit in situations where the sample size is small. Note that as \\(T\\) gets large, the correction factor approximates zero. As with the AIC, a lower AICc value indicates better fit.\n\nBIC, also known as Schwarz Information Criterion (SIC), is another measure that penalizes model complexity. BIC is based on Bayesian principles and provides a stronger penalty for model complexity compared to AIC. The BIC value is calculated using the formula:\n\n\n\\(BIC = T*ln(SSE/T) + (k+2)*ln(T)\\)\n\n BIC puts a greater emphasis on simplicity compared to AIC or AICc.\nThese measures can be easily calculated in R using the glance() function. The code below estimates the ETS and ARIMA models (which we will learn in the upcoming modules) for illustration purposes since the Naive and Mean models are non-parametric and do not provide us with an AIC, AICc, or BIC.\n\ncalits_train %&gt;%\n  model(LS=TSLM(average_price~trend()),\n        ETS=ETS(average_price),\n        ARIMA=ARIMA(average_price))%&gt;% \n  glance() %&gt;%\n  select('.model',\"AIC\",\"AICc\",\"BIC\")\n\n\n\n\n\n\n\n\n\nModel Fit Information Criterion\n\n\n.model\nAIC\nAICc\nBIC\n\n\n\n\nLS\n−476.14\n−476.01\n−466.60\n\n\nETS\n203.26\n203.39\n212.80\n\n\nARIMA\n−205.83\n−205.76\n−199.48\n\n\n\n\n\n\n\nThe model with the lowest AIC (AICc or BIC) is the simple Least Squares model that only has two parameters to estimate (slope and intercept). These results indicate that LS provides a good fit relative to it’s complexity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#readings",
    "href": "Benchmarks.html#readings",
    "title": "5  Model Benchmarks",
    "section": "5.8 Readings",
    "text": "5.8 Readings\nThe primary reading for this chapter comes from Hyndman (2021). Topics include the forecasting process, accuracy measures, and time series analysis using simple regression. For a basic introduction to regression and time series with Excel, the Winston and Albright (2019) reading is recommended.\n\nHyndman (2021) Chapter 5 (The Forecaster’s Toolbox), Chapter 7 (Time Series Regression Models).\nWinston and Albright (2019) Chapter 13.1 (Introduction), 13.2 (Overview of Regression Models), 13.3 (Simple Regression Models).\ngt package: https://gt.rstudio.com",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#leasons-learned",
    "href": "Benchmarks.html#leasons-learned",
    "title": "5  Model Benchmarks",
    "section": "5.9 Leasons Learned",
    "text": "5.9 Leasons Learned\nIn this module you have been introduced to the general procedure in forecasting time series. Particularly you have learned to:\n\nCreate forecasts with simple heuristics.\nAssess the fit of the model with accuracy measures.\nCreate a test set and train set to avoid over-fitting.\nPerform cross validation.\nSelect models with the AIC, AICc or BIC.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "Benchmarks.html#exercises",
    "href": "Benchmarks.html#exercises",
    "title": "5  Model Benchmarks",
    "section": "5.10 Exercises",
    "text": "5.10 Exercises\n\nYou’ve been tasked by the Japanese government with an important mission: forecast the country’s population by 2030. Your projections will help them effectively allocate resources and plan for future demands. To accomplish this, you’ll rely on population data available at the following link: https://jagelves.github.io/Data/UNPOP.csv Build two models using the TSLM approach. One with a simple linear trend and another with a quadratic trend (use I(trend()^2) in R to incorporate a quadratic term). Compare the models by calculating their BIC (Bayesian Information Criterion), which will help you identify which model is more accurate. Provide your population estimate for 2030 using the model with the lower BIC. Based on your findings, determine whether Japan’s population is expected to increase or decrease by 2030.\n\n\n\nSuggested Answer\n\nHere is code to obtain the AIC, AICc and BIC:\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(fpp3)\n\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/UNPOP.csv\")\n\nRows: 21983 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): Variant, Area, Notes, ISO3, ISO2, Type, Population_Annual_Doubling...\ndbl (58): Index, Location_code, SDMX_code, Parent code, Year, Total_Populati...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata %&gt;% filter(ISO3==\"JPN\") %&gt;% \n  select(Year, Total_Population,ISO3) %&gt;% \n  as_tsibble(index=Year,key=ISO3) -&gt; JAP_ts\n\nJAP_ts %&gt;% model(TSLML=TSLM(Total_Population~trend()),\n                 TSLMQ=TSLM(Total_Population~trend()+I(trend()^2))) -&gt; fit\n\nfit %&gt;% glance() %&gt;%\n  select('.model',\"AIC\",\"AICc\",\"BIC\")\n\n# A tibble: 2 × 4\n  .model   AIC  AICc   BIC\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 TSLML  1270. 1271. 1277.\n2 TSLMQ  1048. 1048. 1057.\n\n\nFor the forecast we can use the forecast() function:\n\nfit %&gt;% forecast(h=7) %&gt;% filter(.model==\"TSLMQ\")\n\n# A fable: 7 x 5 [1Y]\n# Key:     ISO3, .model [1]\n  ISO3  .model  Year   Total_Population   .mean\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;             &lt;dist&gt;   &lt;dbl&gt;\n1 JPN   TSLMQ   2024 N(125404, 1484405) 125404.\n2 JPN   TSLMQ   2025 N(125043, 1503406) 125043.\n3 JPN   TSLMQ   2026 N(124657, 1524189) 124657.\n4 JPN   TSLMQ   2027 N(124247, 1546855) 124247.\n5 JPN   TSLMQ   2028 N(123812, 1571507) 123812.\n6 JPN   TSLMQ   2029 N(123353, 1598249) 123353.\n7 JPN   TSLMQ   2030 N(122869, 1627190) 122869.\n\n\nThe population is expected to go down to 122,868 people. This represents a decrease of 1.21%.\n\n\nFertility rates are crucial because they directly impact population growth and demographic structure. Low fertility rates can lead to an aging population, shrinking workforce, and challenges in sustaining economic growth and social welfare programs. To maintain a stable population in the U.S., the total fertility rate (TFR) needs to be around 2.1 children per woman (i.e. the replacement rate) If the TFR falls below this level for an extended period, the population may decline without immigration. Use the data found here: https://jagelves.github.io/Data/UNPOP.csv to plot the fertility rate for the US from 1960-2023 and the current replacement rate. What is the two year projected fertility rate when using a simple linear trend? How does this change when running the same model with a log transformation on the fertility variable?\n\n\n\nSuggested Answer\n\nTo graph the fertility rate we can use the following code:\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\n\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/UNPOP.csv\")\n\ndata %&gt;% filter(ISO3==\"USA\") %&gt;% \n  select(Year, Total_Fertility_Rate,ISO3) %&gt;% \n  as_tsibble(index=Year,key=ISO3) -&gt; USFert\n\nUSFert %&gt;% \n  autoplot(.vars=Total_Fertility_Rate) + theme_clean() +\n  geom_hline(yintercept = 2.1, lty=2) +\n  labs(title=\"Total Fertility Rate in the US.\",\n       x=\"\", y=\"\")\n\n\n\n\n\n\n\n\nThe two year projections are retrieved with the following code:\n\nUSFert %&gt;% model(TSLML=TSLM(Total_Fertility_Rate~trend()),\n                 TSLMLog=TSLM(log(Total_Fertility_Rate)~trend())) -&gt; fit\n\nfit %&gt;% forecast(h=2) \n\n# A fable: 4 x 5 [1Y]\n# Key:     ISO3, .model [2]\n  ISO3  .model   Year Total_Fertility_Rate .mean\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;               &lt;dist&gt; &lt;dbl&gt;\n1 USA   TSLML    2024         N(1.4, 0.18)  1.38\n2 USA   TSLML    2025         N(1.4, 0.18)  1.35\n3 USA   TSLMLog  2024    t(N(0.43, 0.025))  1.56\n4 USA   TSLMLog  2025    t(N(0.42, 0.025))  1.54\n\n\nIf the linear trend is assumed to continue, in two years the projected fertility rate will go down to 1.35. However, if we perform the log transformation, the total fertility rate is expected to fall at a slower rate and reach 1.54 in two years. Below is a graph of the models and projections:\n\nfit %&gt;% forecast(h=2) %&gt;% autoplot(level=NULL) +\n  autolayer(USFert,Total_Fertility_Rate) + theme_clean() +\n  autolayer(fit %&gt;% augment() %&gt;% filter(.model==\"TSLML\"),.fitted, col=\"red\") +\n  autolayer(fit %&gt;% augment() %&gt;% filter(.model==\"TSLMLog\"),.fitted, col=\"#008080\") +\n  labs(title=\"Total Fertility Rate in the US.\",\n       x=\"\", y=\"\")\n\n\n\n\n\n\n\n\n\n\nRefer back to Problem 2, where your task is to forecast the fertility rate. For both the linear and logarithmic models you’ve developed, calculate the RMSE (Root Mean Squared Error) using the models’ fitted values. Use the augment() function to extract the residuals for each model, then square these residuals, average them and finally take the square root. Based on the RMSE, determine which model provides a better fit for the data.\n\n\n\nSuggested Answer\n\nTo obtain the residuals and perform the calculation you can use the code below. First obtain the RMSE for the Linear model:\n\nfit %&gt;% augment() %&gt;% filter(.model==\"TSLML\") %&gt;%\n  pull(.resid) %&gt;% `^`(2) %&gt;% mean() %&gt;% sqrt()\n\n[1] 0.4055513\n\n\nThen we can apply the same code for the Logarithmic model:\n\nfit %&gt;% augment() %&gt;% filter(.model==\"TSLMLog\") %&gt;%\n  pull(.resid) %&gt;% `^`(2) %&gt;% mean() %&gt;% sqrt()\n\n[1] 0.3850176\n\n\nIf we follow the RMSE, the logarithmic model fits the data better.\n\n\nRefer back to Problem 2, where your task is to forecast the fertility rate. For both the linear and logarithmic models you’ve developed, perform leave-one-out cross-validation. Initialize the cross-validation with eight observations and use a step of one. Which model performs better according to the RMSE?\n\n\n\nSuggested Answer\n\nTo perform leave-one-out cross validation, we initialize the process with eight observations and set the step size to one. The cross validation forecasts one period ahead. The code below performs the cross validation:\n\nUSFert %&gt;%\n  stretch_tsibble(.init = 8, .step = 1) %&gt;% \n  model(TSLML=TSLM(Total_Fertility_Rate~trend()),\n        TSLMLog=TSLM(log(Total_Fertility_Rate)~trend())) %&gt;% forecast(h=1) %&gt;% accuracy(USFert) %&gt;% \n  select(.model,RMSE)\n\n# A tibble: 2 × 2\n  .model   RMSE\n  &lt;chr&gt;   &lt;dbl&gt;\n1 TSLML   0.457\n2 TSLMLog 0.359\n\n\nFollowing the RMSE, the TSLMLog model performs the best.\n\n\nGrocers Inc. purchases avocados at Richmond/Norfolk’s market price and sells them in their stores for $1.5 each. Using the avocado price data found here: https://jagelves.github.io/Data/avocado2020-2023.csv, forecast margin for the company over the next 4 weeks using a simple mean, the NAIVE and simple trend. Use only data from W2 2023 onward.\n\n\n\nSuggested Answer\n\nWe can start by doing some data wrangling. Load the data, select variables, get the appropriate dates and create the tsibble:\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(fpp3)\n\navo&lt;-read_csv(\"https://jagelves.github.io/Data/avocado2020-2023.csv\")\n\navo %&gt;% select(Geography,Date, Type, AveragePrice) %&gt;% \n  filter(Geography==\"Richmond/Norfolk\") %&gt;% \n  filter(Type==\"Conventional\") %&gt;% \n  mutate(Date=mdy(Date)) %&gt;% \n  mutate(Date=yearweek(Date)) %&gt;% \n  select(Date,AveragePrice) %&gt;% \n  distinct(Date, .keep_all = TRUE) %&gt;% \n  arrange(Date) %&gt;% \n  as_tsibble(index=Date) %&gt;% \n  filter_index(\"2023 W2\"~.) -&gt; avo_ts\n\nWe can now build our models and forecast the price. Below we also report the margin calculation:\n\navo_ts %&gt;% model(MEAN=MEAN(AveragePrice),\n                 NAIVE=NAIVE(AveragePrice),\n                 TSLM=TSLM(AveragePrice~trend())) -&gt; fit\n\nfit %&gt;% forecast(h=4) %&gt;% \n  as_tibble() %&gt;% \n  select(Date,.model,.mean) %&gt;% \n  mutate(Margin=(1.5-.mean)/1.5)\n\n# A tibble: 12 × 4\n       Date .model .mean Margin\n     &lt;week&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 2023 W25 MEAN    1.03  0.312\n 2 2023 W26 MEAN    1.03  0.312\n 3 2023 W27 MEAN    1.03  0.312\n 4 2023 W28 MEAN    1.03  0.312\n 5 2023 W25 NAIVE   1.13  0.247\n 6 2023 W26 NAIVE   1.13  0.247\n 7 2023 W27 NAIVE   1.13  0.247\n 8 2023 W28 NAIVE   1.13  0.247\n 9 2023 W25 TSLM    1.09  0.272\n10 2023 W26 TSLM    1.10  0.268\n11 2023 W27 TSLM    1.10  0.265\n12 2023 W28 TSLM    1.11  0.262\n\n\n\n\n\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Benchmarks</span>"
    ]
  },
  {
    "objectID": "ETS.html",
    "href": "ETS.html",
    "title": "6  ETS",
    "section": "",
    "text": "6.1 ETS Components\nETS models build on simple exponential smoothing (SES). The basic idea behind SES is to assign more weight to recent observations and gradually decrease the weights as the observations become older. The model emphasizes the most recent data points and gives less importance to older observations.\nMathematically, the simple exponential smoothing model can be defined as:\nwhere \\(\\hat{y}_{t+h}\\) is the forecast of period \\(t+h\\), \\(l_{t}\\) is smoothed value of the series at time \\(t\\), \\(y_t\\) is the value observed at the current time period \\(t\\) and \\(\\alpha\\) is the smoothing parameter. Note that when \\(\\alpha\\) is equal to one, the forecast equation is equivalent to the Naive model, and when \\(\\alpha\\) is equal to zero, the smoothing equation is always \\(l_{t-1}\\).\nThe SES model is useful when forecasting series that have no trend or seasonality. The SES model can easily be modified to account for trend and seasonality by adding additional components. For example, the Holt’s linear trend method adds a component to account for a linear trend, the damped trend methods flatten the trend some time into the future, and the Holt-Winters model accounts for seasonality. The collection of models generated by adding different components are summarized as Error, Trend, and Seasonality (ETS) models. We apply the ETS model to the deliveries of the electric car company Tesla in the sections below.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#ets-components",
    "href": "ETS.html#ets-components",
    "title": "6  ETS",
    "section": "",
    "text": "Forecast Equation: \\(\\hat{y}_{t+h}=l_t\\)\n\n\nSmoothing Equation: \\(l_{t}=\\alpha y_t + (1-\\alpha)l_{t-1}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#teslas-deliveries",
    "href": "ETS.html#teslas-deliveries",
    "title": "6  ETS",
    "section": "6.2 Tesla’s Deliveries",
    "text": "6.2 Tesla’s Deliveries\nDeliveries are a carefully watched number by Tesla shareholders and are the closest approximation of sales disclosed by the company. Additionally, Tesla’s deliveries are closely followed due to their impact on financial markets, the EV industry, innovation and disruption, production efficiency, and the growth of the EV market. The numbers serve as a key performance indicator for Tesla’s success and provide insights into the broader trends in the electric vehicle industry. Can we use the ETS model to forecast Tesla’s deliveries?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#sec-data",
    "href": "ETS.html#sec-data",
    "title": "6  ETS",
    "section": "6.3 The Data",
    "text": "6.3 The Data\nThe data can be found here Tesla. Below is code that inputs the data as a tsibble in R.\n\nlibrary(fpp3)\n\n# Create tsibble\ntesla&lt;-tsibble(\n  period=yearquarter(c(\"2016:Q1\",\"2016:Q2\",\"2016:Q3\",\"2016:Q4\",\n                       \"2017:Q1\",\"2017:Q2\",\"2017:Q3\",\"2017:Q4\",\n                       \"2018:Q1\",\"2018:Q2\",\"2018:Q3\",\"2018:Q4\",\n                       \"2019:Q1\",\"2019:Q2\",\"2019:Q3\",\"2019:Q4\",\n                       \"2020:Q1\",\"2020:Q2\",\"2020:Q3\",\"2020:Q4\",\n                       \"2021:Q1\",\"2021:Q2\",\"2021:Q3\",\"2021:Q4\",\n                       \"2022:Q1\",\"2022:Q2\",\"2022:Q3\",\"2022:Q4\",\n                       \"2023:Q1\",\"2023:Q2\")),\n  deliveries=c(14.8,14.4,24.5,22.2,\n               25,22,26.2,29.9,\n               30,40.7,83.5,90.7,\n               63,95.2,97,112,\n               88.4,90.7,139.3,180.6,\n               184.82,201.25,241.3,308.6,\n               310.5,254.7,343.8,405.3,\n               422.9,466.1),\n  index=period     # This is the time variable\n)\n\nAs you can see the tsibble is created with the tsibble() function included in the fpp3 package. The yearquarter() function from the lubridate package is used to coerce the period data to a date. The time variable is then specified via the index parameter. The code below creates the plot of Tesla’s deliveries using the autoplot() function.\n\ntesla %&gt;% autoplot(.vars=deliveries) + theme_classic() +\n  labs(title= \"Tesla Car Deliveries\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")\n\n\n\n\n\n\n\n\nThe most striking aspect of Tesla’s deliveries is the exponential trend. There also seems to be a seasonal component, with relatively higher production Q4 versus the other quarters. These characteristics will be adopted by the ETS model to forecast the series. Below we can see the STL decomposition that confirm these characteristics.\n\ntesla %&gt;%\n  model(STL(deliveries~trend() + season())) %&gt;%\n  components() %&gt;% autoplot() + theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#models",
    "href": "ETS.html#models",
    "title": "6  ETS",
    "section": "6.4 Models",
    "text": "6.4 Models\nTo model the data and create the appropriate forecasts, we start by generating test and training sets from the available data.\n\ntrain_tesla&lt;-filter_index(.data=tesla,\"2016 Q1\"~\"2021 Q4\")\ntest_tesla&lt;-filter_index(.data=tesla,\"2022 Q1\"~\"2023 Q2\")\n\nThere is no fixed rule for determining the length of the train and test sets. In this example, it is important to allocate a sufficiently large portion of the data to the training set to capture the underlying seasonality and trend of Tesla’s deliveries. The sets are easily created using the filter_index() function.\nFive models will be estimated based on ETS. The first one is the Simple Exponential Smoothing model with additive errors (SES), the Holt model that includes an additive trend (HOLT), a dampened trend model (DAMPED), a damped model with seasonality (DAMPEDS), and finally an algorithmic function that attempts to select the best ETS model (see Hyndman (2021), Chapter 8). Along with these five models two more models are set forth. The first one is a simple least squares model (LS) and the second one is a quadratic model with seasonality dummies (LSS).\nModel selection will be done via cross validation. Recall, that the the stretch_tsibble() function reshapes the tsibble to accommodate for cross validation. The .init parameter sets the first eight observations to estimate our initial model and the .step argument increases the training set by four. Cross validation is done four periods ahead (a year) and accuracy measures are created by comparing forecasts to the test set.\nEach component of the ETS model can be included as either multiplicative (\\(M\\)) or additive (\\(A\\)). The trend component can be assigned to be damped (\\(Ad\\) or \\(Md\\)). If the component is to be omitted from the model, None (\\(N\\)) is specified. Below is the code to estimate the models and the results of the cross validation.\n\nlibrary(gt)\ntrain_tesla %&gt;% stretch_tsibble(.init = 8, .step=4) %&gt;%\n  model(\n    SES=ETS(deliveries ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    HOLT=ETS(deliveries ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    DAMPED=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"N\")),\n    DAMPEDS=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\nALGO=ETS(deliveries),\nLS = TSLM(deliveries ~ trend()+I(trend()^2)),\nLSS = TSLM(deliveries ~ trend()+I(trend()^2)+season()))%&gt;%\n  forecast(h = 4) %&gt;%\n  accuracy(tesla) %&gt;% select(-\"ACF1\") \n\n\n\n\n\n\n\n\n\nCross Validation Models\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\n\n\n\n\nALGO\nTest\n40.231\n57.608\n45.297\n23.474\n28.654\n1.042\n0.990\n\n\nDAMPED\nTest\n18.701\n55.497\n44.900\n8.033\n32.803\n1.033\n0.954\n\n\nDAMPEDS\nTest\n21.011\n47.198\n38.590\n9.399\n23.120\n0.888\n0.811\n\n\nHOLT\nTest\n15.122\n57.200\n46.728\n4.127\n34.811\n1.075\n0.983\n\n\nLS\nTest\n11.657\n43.208\n37.200\n2.527\n28.375\n0.856\n0.742\n\n\nLSS\nTest\n9.739\n41.890\n36.739\n1.641\n28.748\n0.845\n0.720\n\n\nSES\nTest\n40.991\n58.160\n45.750\n23.988\n28.824\n1.053\n0.999\n\n\n\n\n\n\n\nThe accuracy measures reveal that the DAMPEDS and LSS models perform consistently well. Below, we will continue with the DAMPEDS and LSS models as the trend seems to be exponential and there seems to be evidence of seasonality. These model are estimated and saved into an object called fit below.\n\nfit &lt;- tesla %&gt;%\n  model(\n    DAMPEDS = ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\n    LSS = TSLM(deliveries ~ trend()+I(trend()^2)+season())\n  )\n\nIf one is interested in retrieving the model coefficients, one can use the tidy() (or coef()) function. Below the function is used along with the fit object to retrieve the coefficients of the Least Squares model with seasonality:\n\ntidy(fit) %&gt;% filter(.model==\"LSS\") %&gt;%\n  select(-\".model\")\n\n\n\n\n\n\n\n\n\nLSS Model Coefficients\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.43\n13.97\n2.61\n0.02\n\n\ntrend()\n−7.33\n1.92\n−3.82\n0.00\n\n\nI(trend()^2)\n0.70\n0.06\n11.62\n0.00\n\n\nseason()year2\n−8.63\n10.91\n−0.79\n0.44\n\n\nseason()year3\n8.07\n11.35\n0.71\n0.48\n\n\nseason()year4\n21.40\n11.35\n1.88\n0.07\n\n\n\n\n\n\n\nThe output above, reveals that the seasonal dummy for Q4 is statistically significant at the \\(10\\)% confirming the seasonal pattern found in the decomposition (Section 6.3). The plot below shows the fit of the models with the blue line representing the LSS model and the red line the DAMPEDS model.\n\ntesla %&gt;% autoplot(deliveries, lwd=1.2, alpha=0.5) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"blue\",\n            data = augment(fit)  %&gt;% filter(`.model`==\"LSS\")) +\n              geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %&gt;% filter(`.model`==\"DAMPEDS\")) + \n  labs(title= \"Tesla Car Deliveries Fitted Values\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#information-criterion",
    "href": "ETS.html#information-criterion",
    "title": "6  ETS",
    "section": "6.5 Information Criterion",
    "text": "6.5 Information Criterion\nWe can also attempt to select our models via the AIC, AICc, or BIC. The code below summarizes these measure for the models considered.\n\ntrain_tesla %&gt;%\n  model(\n    SES=ETS(deliveries ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    HOLT=ETS(deliveries ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    DAMPED=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"N\")),\n    DAMPEDS=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\nALGO=ETS(deliveries),\nLS = TSLM(deliveries ~ trend()+I(trend()^2)),\nLSS = TSLM(deliveries ~ trend()+I(trend()^2)+season())) %&gt;% \n  report()  %&gt;%\n  select('.model',\"AIC\",\"AICc\",\"BIC\")\n\n\n\n\n\n\n\n\n\nModel Fit Information Criterion\n\n\n.model\nAIC\nAICc\nBIC\n\n\n\n\nSES\n237.16\n238.36\n240.69\n\n\nHOLT\n233.99\n237.32\n239.88\n\n\nDAMPED\n236.37\n241.31\n243.44\n\n\nDAMPEDS\n233.92\n250.84\n245.70\n\n\nALGO\n223.19\n224.39\n226.73\n\n\nLS\n150.12\n152.22\n154.83\n\n\nLSS\n147.17\n154.17\n155.41\n\n\n\n\n\n\n\nHere, once again the LSS model seems to perform the best as it provides the lowest values. Among the ETS models, the ALGO model now stands out. This should be of no surprise, as the ALGO model is designed to choose ETS components that minimize the AIC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#forecasts",
    "href": "ETS.html#forecasts",
    "title": "6  ETS",
    "section": "6.6 Forecasts",
    "text": "6.6 Forecasts\nForecasts are created by using the fit object. We will forecast four quarters ahead using the forecast() function. The code below generates a table with the forecasts.\n\nlibrary(gt)\nfit %&gt;%\n  forecast(h = 4) %&gt;% select(-\".model\") -&gt; deliveries_fc\ndeliveries_fc\n\n\n\n\n\n\n\n\n\nForecasts\n\n\nperiod\ndeliveries\n.mean\n.model\n\n\n\n\n2023 Q3\nN(516, 854)\n515.90\nDAMPEDS\n\n\n2023 Q4\nN(563, 1681)\n562.94\nDAMPEDS\n\n\n2024 Q1\nN(579, 2895)\n578.73\nDAMPEDS\n\n\n2024 Q2\nN(602, 4563)\n601.50\nDAMPEDS\n\n\n2023 Q3\nN(489, 709)\n488.95\nLSS\n\n\n2023 Q4\nN(539, 754)\n538.99\nLSS\n\n\n2024 Q1\nN(556, 782)\n555.69\nLSS\n\n\n2024 Q2\nN(587, 844)\n586.56\nLSS\n\n\n\n\n\n\n\nForecasts for the four quarters are shown above, with the corresponding mean. In general, both models predict Tesla will continue its trend and increase its deliveries every quarter. According to the DAMPEDS model, Tesla is expected to deliver about \\(516,000\\) cars on average. The plot below illustrates the forecasts for both models along with the \\(95\\)% prediction intervals. The increasing standard deviation for future periods reminds us that longer-period forecasts have even more uncertainty.\n\nfit %&gt;%\n  forecast(h = 4) %&gt;%\n  autoplot(tesla, level=95) +\n  labs(x=\"Quarter\", y=\"\",\n       title = \"Tesla Car Deliveries Forecasts\",\n       subtitle = \"Q1 2017 to Q2 2023\") + theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#scenarios",
    "href": "ETS.html#scenarios",
    "title": "6  ETS",
    "section": "6.7 Scenarios",
    "text": "6.7 Scenarios\nOn October 9, 2023, Tesla announced 435,059 deliveries for Q3, signaling strong performance. However, during the previous earnings call, Tesla had already cautioned investors about potential delivery impacts due to planned factory shutdowns and upgrades. These shutdowns are necessary to retool production lines for their upcoming vehicles, the Highland Model 3 and Cybertruck. Interestingly, a similar factory shutdown occurred in Q2 2022, giving us valuable historical data to make more precise predictions.\nTo factor in the effect of these shutdowns, we can introduce a dummy variable, where we assign a value of 1 during shutdown quarters and 0 otherwise. Here’s the code:\n\ntesla&lt;-mutate(tesla,\n                Down=case_when(\n                  period==yearquarter(\"2022 Q2\") ~ 1,\n                                       TRUE ~ 0))\n\nThe Down variable, captures the impact of factory shutdowns on deliveries. Now, we can incorporate this information into our Time Series Linear Model (TSLM) to estimate Tesla’s deliveries more accurately:\n\nfit &lt;- tesla %&gt;%\n  model(\n    LSS = TSLM(deliveries~trend()+\n                 I(trend()^2)+season()+Down)\n  )\n\nNext, let’s explore two possible scenarios: one where Tesla undergoes a factory shutdown and another where production continues uninterrupted. Using the scenarios() function, we can create a dataset for forecasting over the next four periods, simulating both situations:\n\nDown_Scenario &lt;- scenarios(\n  Factory_Down = new_data(tesla, 4) |&gt;\n    mutate(Down=rep(1,4)),\n  Factory_Up = new_data(tesla, 4) |&gt;\n    mutate(Down=rep(0,4)),\n  names_to = \"Scenario\")\n\nFinally, we can visualize how each scenario affects Tesla’s future delivery forecasts. Whether Tesla experiences another factory shutdown or keeps production running smoothly, this analysis will help us better understand the potential outcomes.\n\ntesla %&gt;%\n  autoplot(deliveries) + \n  autolayer(forecast(fit,new_data=Down_Scenario),\n            level=NULL)+ theme_classic() +\n  labs(x=\"Quarter\", y=\"\",\n       title = \"Tesla Car Deliveries Forecasts\",\n       subtitle = \"Scenario Forecast\") + theme_classic()\n\n\n\n\n\n\n\n\nThe new forecast of 429,404 is now closer to the reported delivery number of 435,059 for Q3 2023 when compared to the analyst consensus estimate of 454,809. For the 4th quarter of 2023 the analyst consensus was 480,500, which is inline with the forecast of the LSS model (about 480,000). The 95% confidence interval of [422,553, 537,182] is retrieved using the hilo() function.\n\nfit %&gt;% forecast(new_data=Down_Scenario) %&gt;%\n  hilo(95) %&gt;% unpack_hilo(\"95%\") %&gt;%\n  as_tibble() %&gt;%\n  select(Down,period,.mean,`95%_lower`,`95%_upper`) %&gt;%\n  gt() %&gt;% fmt_number(columns=c(.mean,\n                       `95%_lower`,`95%_upper`),\n             decimals = 2)\n\n\n\n\n\n\n\nDown\nperiod\n.mean\n95%_lower\n95%_upper\n\n\n\n\n1\n2023 Q3\n429.40\n373.00\n485.81\n\n\n1\n2023 Q4\n479.87\n422.55\n537.18\n\n\n1\n2024 Q1\n497.35\n439.66\n555.04\n\n\n1\n2024 Q2\n537.02\n480.78\n593.27\n\n\n0\n2023 Q3\n496.03\n451.89\n540.16\n\n\n0\n2023 Q4\n546.49\n500.98\n592.00\n\n\n0\n2024 Q1\n563.97\n517.60\n610.35\n\n\n0\n2024 Q2\n603.65\n554.66\n652.64",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#readings",
    "href": "ETS.html#readings",
    "title": "6  ETS",
    "section": "6.8 Readings",
    "text": "6.8 Readings\nThe main reading for ETS models comes from Chapter 8 of Hyndman (2021). These readings provide a bit more detail on the mathematical background behind each model and a few more applications. For an overview of the moving averages, the Holt, and Winters’ models review Winston and Albright (2019).\n\nHyndman (2021) Chapter 8 (Exponential Smoothing).\nWinston and Albright (2019) Chapter 13.5 (Overview of Time Series Models), 13.6 (Moving Average Models), 13.7 (Exponential Smoothing Models).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#leasons-learned",
    "href": "ETS.html#leasons-learned",
    "title": "6  ETS",
    "section": "6.9 Leasons Learned",
    "text": "6.9 Leasons Learned\nIn this module you have been introduced to ETS model. Particularly you have learned to:\n\nUse the model() and ETS() functions to estimate the model.\nIdentify when ETS model is superior to other model by using the cross validation and information criterion.\nForecast time series with the ETS model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ARIMA.html",
    "href": "ARIMA.html",
    "title": "7  ARIMA",
    "section": "",
    "text": "7.1 Preliminaries",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#preliminaries",
    "href": "ARIMA.html#preliminaries",
    "title": "7  ARIMA",
    "section": "",
    "text": "White Noise\nIn time series, white noise refers to a sequence of random data points that have no correlation to each other. This process is used as a benchmark for other types of time series data that exhibit patterns or trends. By comparing a series with the white noise process, we can verify if the series has systematic components that can be modeled (i.e., if there is any signal).\nWe can generate a white noise process by using the normal distribution with a mean of zero and a constant variance. Below we create a tsibble with the simulated data.\n\nlibrary(fpp3)\nset.seed(10)\nwn&lt;-tsibble(x=rnorm(100),period=seq(1:100),index=period)\n\nWe can now use the autoplot() function to observe the white noise process.\n\nwn %&gt;% autoplot(x) + theme_classic() + \n  labs(title=\"White Noise Process\",\n       subtitle=\"Mean=0 and Standard Deviation=1\",\nx=\"\",y=\"\") + \n  geom_hline(yintercept = 0, col=\"blue\", lwd=1, linetype=\"dashed\",\n             alpha=0.4)\n\n\n\n\n\n\n\n\nA very unpredictable and ragged pattern is shown in the graph above. The series behaves erratically but fluctuates around a mean of zero and keeps a standard deviation of one. Given the unpredictability of the white noise process, the best one can do is to describe it by its mean and standard deviation.\n\n\nStationarity\nA time series is said to be stationary if its statistical properties do not change over time. In other words, a stationary time series has a constant mean, variance, and auto-covariance, regardless of the time at which the series is observed. An example of a stationary process is the white noise process introduced above.\nMany time series models, including the ARIMA model, require a stationary time series. These models make predictions about future values based on past values, and the statistical properties of the past values are used to inform these predictions. If the time series’ statistical properties change over time, then the models may not work well, as the assumptions underlying them would not be met.\nIn general, before modeling and forecasting, we will check whether the series is stationary (i.e., has no trend and is homoskedastic). To eliminate the trend in the series we will use the first difference of the series. We can do this in R by using the difference() function. For example consider Tesla’s quarterly vehicle deliveries.\n\n\n\n\n\n\n\n\n\nDeliveries have been on an upward trend as the company is currently scaling its production, and demand is strong. This series is not stationary since it crosses the mean (blue line) once and never revisits it. That is, the mean is not constant and changes with time. It is possible to make the series stationary by finding differences (i.e. the change in deliveries from quarter to quarter). Below is the graph of the first difference.\n\ntesla %&gt;%\n  autoplot(difference(deliveries)) + theme_classic() +\n  labs(title=\"Quarterly Change In Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\") + \n  geom_hline(yintercept = mean(difference(tesla$deliveries), na.rm = TRUE), col=\"blue\", linetype=\"dashed\", lwd=1, alpha=0.4)\n\n\n\n\n\n\n\n\nThe series now fluctuates closer to the mean, but unlike the white noise process behaves less erratic. You will notice that in some periods the change in deliveries from quarter to quarter is high. For example, following the lows at the beginning of the year, deliveries seem to increase sharply. There seems to be correlations between the quarters (time dependencies).\nAnother pattern shown in the graph above is heteroskedasticity (increasing variance), The variance of the series seems to be low in the period of 2016-2018 while significantly higher for the period after. To normalize the variance of the series we can conduct a Box-Cox transformation.\n\nlambda &lt;- tesla %&gt;% \n  features(deliveries, features = guerrero) %&gt;% pull(lambda_guerrero) \n\ntesla %&gt;%\n  autoplot(box_cox(difference(deliveries), lambda)) +\n  labs(y = \"\")+ theme_classic() +\n  labs(title=\"Box-Cox Transformation of Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\")\n\n\n\n\n\n\n\n\nThe transformation has made the series a bit more homoskedastic (variance is more uniform) than the series without the transformation. It is important to note that both transformations (differencing and box-cox) can be undone by using an inverse function. Hence, we can always return the series to its original form. Luckily, fable does this automatically for us when we forecast a model.\nA couple of statistical features used to determine the stationarity of a series are the unitroot_kpss and unitroot_ndiffs. In general, a low p-value allows us to reject the null of hypothesis of stationarity. Below, we test the features with Tesla’s deliveries.\n\ntesla %&gt;%\n  features(deliveries, features = c(unitroot_kpss, unitroot_ndiffs)) \n\n\n\n\n\n\n\n\n\nStationarity Tests\n\n\nkpss_stat\nkpss_pvalue\nndiffs\n\n\n\n\n0.876\n0.010\n1\n\n\n\n\n\n\n\nUnfortunately, the test does not report the calculated p-value. Instead, it reports a p-value of 0.01 when the p-value is below 0.01 and 0.1 when it is above 0.1. Given that the p-value reported is 0.01, we verify that Tesla deliveries are non-stationary and that two differences are required to make the data stationary (ndiffs=2).\n\n\nThe autocorrelation function\nAutocorrelations are essential in time series analysis since they indicate the degree of similarity between a time series and a lagged version of itself (a previous period). They help identify patterns and trends in the data allowing us to predict future values of the series. For example, suppose a time series exhibits a strong positive autocorrelation at a lag of \\(k\\) periods. In such a case, the value at time \\(t+k\\) will likely be similar to that at time \\(t\\). Formally we can write the autocorrelation as:\n\n\\(\\rho_{y_t,y_{t-k}}=\\frac{cov(y_t,y_{y-k})}{sd(y_t)sd(y_{t-k})}\\)\n\n Let’s illustrate the use of autocorrelations with an example. This time let’s inspect personal income growth in the state of California. Below we load the data and create the train and test sets.\n\nlibrary(fpp3)\nlibrary(tidyverse)\nPI&lt;-read_csv(\"https://jagelves.github.io/Data/PersonalIncome.csv\")\nPI %&gt;% as_tsibble(index=Date) %&gt;% \n  filter_index(1970~2005) -&gt; PI_train\nPI %&gt;% as_tsibble(index=Date) %&gt;% \n  filter_index(2006~2021) -&gt; PI_test\n\nThe autocorrelation function can now be constructed by using the ACF() function and plotting it with autoplot() as shown below.\n\nPI_train %&gt;%\n  ACF(lag_max = 12,PI_Growth) %&gt;% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"\",\n                                 title=\"ACF Personal Income Growth in California\")\n\n\n\n\n\n\n\n\nThe plot shows that the correlation of the series with its first lag is strongest. That is, positive income growth in the previous period correlates with positive income growth in the current period. The plot also shows continuous decay in the strength of the correlation as the lags get larger. A positive income growth two or three periods ago still positively influences the current period’s income growth, but less than the immediate previous period. The blue lines determine which autocorrelations are statistically different from zero (significant) at the 5% level. As you can see, lags 1-4 are positively correlated with the series and are statistically significant.\nOn the other hand, a white noise process is expected to show no correlation with its lags since the series is constructed from independent draws from a normal distribution with constant variance. Previous periods do not affect the current or future periods. Below you can see the autocorrelation function of the white noise process.\n\nwn %&gt;% ACF(x) %&gt;% autoplot() + theme_bw() + labs(x=\"\", y=\"ACF\") +\nlabs(x=\"\", y=\"\", title=\"ACF White Noise Process\")\n\n\n\n\n\n\n\n\nInterestingly, lag 14 shows a positive correlation with the series. It is important to note that correlations can happen by chance even if we construct the series from a random process.\n\n\nThe partial autocorrelation function\nAs with the ACF, the partial autocorrelation function (PACF) summarizes the relationships between a series and its lags. However, the relationships of intervening lags are removed. The sample partial autocorrelation at lag \\(k\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nFormally speaking, when we calculate the autocorrelation between \\(y_t\\) and \\(y_{t+k}\\), information flows from \\(t\\) to \\(t+k\\), so that indirectly \\(\\rho_k\\) (the correlation) accounts for the contribution of lags between \\(t\\) and \\(t+k\\). A series of regressions would allow us to calculate the PACF. Luckily, R calculates these easily for us as shown below:\n\nPI_train %&gt;%\n  PACF(lag_max = 12,PI_Growth) %&gt;% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF Personal Income Growth In California\")\n\n\n\n\n\n\n\n\nThe graph shows that the series has a strong correlation only with its first lag. Specifically, lag 2, 3, and 4 seemed to have been correlated with the series (see ACF), but this was mainly because of the influence of lag 1.\nLet’s inspect the white noise process once more to confirm that there are no patterns.\n\nwn %&gt;% PACF(x) %&gt;% autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF White Noise Process\")\n\n\n\n\n\n\n\n\nIn sum, white noise processes are unpredictable and we can only describe them by their mean and standard deviation. Series that have patterns in their ACF or PACF can be modeled using ARIMA. Below we illustrate how to model Personal Income Growth in California with an AR(1) model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#the-ar1-model",
    "href": "ARIMA.html#the-ar1-model",
    "title": "7  ARIMA",
    "section": "7.2 The AR(1) model",
    "text": "7.2 The AR(1) model\nIn the previous section, we established that the growth of personal income in California has a decaying ACF and a single significant spike (at lag 1) in the PACF. These patterns can be generated with an AR(1) model. Specifically, the AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\n\nwhere \\(c\\) is a constant, \\(\\phi\\) is the lag coefficient, \\(y_{t-1}\\) is the first lag of \\(y\\), and \\(\\epsilon\\) is random error. Since this model uses only the first lag of the series as the independent variable, it is known as AR(1). The AR model can be extended to include more lags of \\(y\\), and in general, it would be called an \\(AR(p)\\) model, where \\(p\\) is the largest lag included. Below we simulate some data based on the AR(1) model.\n\ny&lt;-c(0)\nphi&lt;-0.7\nconst&lt;-1\nnrep&lt;-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data generated to see what the ACF looks like for a AR(1) process. This will allow us to visually identify the process.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %&gt;% ACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\n\n\n\n\nNote the resemblance of the ACF of the simulated data to that of the personal income growth. There is a decaying ACF described by a strong correlation of lag one and subsequent lower correlations as the lags get larger. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %&gt;% PACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+labs(x=\"\",y=\"PACF\",                         title=\"PACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\n\n\n\n\nOnce again, we can see the resemblance if we compare the PACF to the one in personal income growth. Specifically, there is one significant spike at lag one, and all other partial autocorrelations are not statistically different from zero. The patterns in the ACF and PACF confirm that we can model personal income growth with an AR(1) process.\n\nModeling and Residuals\nLet’s model personal income growth using the AR(1) model. We’ll also estimate a Least Squares model to compare. Recall, that we can estimate these models by using the model() function and retrieve the coefficients with the coef() function. We will use the AR() function along with the order() function set to one, to model the AR(1) process.\n\nPI_fit&lt;-PI_train %&gt;% \n  model(AR1 = AR(PI_Growth ~ order(1)),\n        LS = TSLM(PI_Growth ~ trend()))\ncoef(PI_fit)\n\n\n\n\n\n\n\n\n\nModel Coefficients For PI Growth\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nAR1\nconstant\n1.80\n0.82\n2.19\n0.04\n\n\nAR1\nar1\n0.70\n0.12\n5.80\n0.00\n\n\nLS\n(Intercept)\n9.19\n0.80\n11.50\n0.00\n\n\nLS\ntrend()\n−0.17\n0.04\n−4.43\n0.00\n\n\n\n\n\n\n\nThe estimated coefficient (\\(\\phi=0.7\\)) for the AR(1) process is equal the one used in our simulation. However, the estimated constant is 1.8. If we changed the constant to 1.8, the simulation would resemble the personal income growth data better. That is, the AR model selects the constant and lag coefficient (\\(\\phi\\)) such that it fits the data.\nIf the AR(1) process correctly describes the series, the errors should behave like white noise. If the model picks up all of the systemic variation, only random error should be left. To inspect the errors we can use the augment() function along with the object that contains the model (PI_fit). The ACF of the error is displayed below.\n\nerrors_PI&lt;-augment(PI_fit)\n\nerrors_PI %&gt;% select(.resid) %&gt;% ACF(.resid) %&gt;% \n  autoplot() + theme_bw() +\n  labs(title=\"ACF for Model Errors of AR(1) and LS\",\n       x=\"\")\n\n\n\n\n\n\n\n\nThe ACF of the errors from the AR(1) model resemble white noise. This suggests that we have correctly identified the systematic component of the series. In other words, there is nothing left to model since the errors are entirely random. This is not the case for the LS model since we still observe some significant spikes (lag 1 and lag 13) in the ACF function.\n\nerrors_PI %&gt;% select(.resid) %&gt;% PACF(.resid) %&gt;% \n  autoplot() + theme_bw()\n\n\n\n\n\n\n\n\nThe PACF once again shows no pattern for the residuals of the AR(1) model and some significant lags for the LS model. This further confirms that the AR(1) model correctly identifies the data generating process in the time series.\n\n\nModel Selection\nWe can choose between the LS and AR(1) models by looking at the AIC, AICc, or BIC.\n\nglance(PI_fit) %&gt;% arrange(AICc) %&gt;% select(.model:BIC)\n\n\n\n\n\n\n\n\n\nModel Fit Measures\n\n\n.model\nsigma2\nAIC\nAICc\nBIC\n\n\n\n\nAR1\n4.30\n−20.25\n−19.89\n−17.09\n\n\nLS\n5.51\n65.35\n66.10\n70.10\n\n\n\n\n\n\n\nHere we note that the AR(1) model performs better in all of the metrics as they are significantly lower than those for the LS. The accuracy on the test set shown below, once more confirms that the AR(1) model performs better than the LS model.\n\nPI_fc&lt;-PI_fit %&gt;% forecast(new_data = PI_test)\nPI_fc  %&gt;% accuracy(PI_test) \n\n\n\n\n\n\n\n\n\nAccuracy Measures\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\nACF1\n\n\n\n\nAR1\nTest\n−1.51\n3.47\n2.45\n−115.69\n156.61\nNaN\nNaN\n0.33\n\n\nLS\nTest\n2.60\n4.43\n3.87\n22.50\n106.69\nNaN\nNaN\n0.48\n\n\n\n\n\n\n\nThe graph below shows the test set along with the forecast of the AR(1) model. Prediction confidence intervals are shown to highlight the uncertainty of the prediction. The blue line indicates the mean of the predictions which are assumed to follow a normal distribution.\n\nPI_fc %&gt;% filter(.model==\"AR1\") %&gt;% autoplot(level=95) + theme_classic() +\n  autolayer(PI_train, PI_Growth) +\n  autolayer(PI_test, PI_Growth) + \n  labs(title=\"Personal Income Growth AR(1) Forecast Accuracy\",\nsubtitle=\"1970-2021\", y=\"\",x=\"\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#the-ma1-model",
    "href": "ARIMA.html#the-ma1-model",
    "title": "7  ARIMA",
    "section": "7.3 The MA(1) model",
    "text": "7.3 The MA(1) model\nThe moving average (MA) process models time series by using past errors (random shocks). Formally,\n\n\\(y_t=\\mu + \\theta \\epsilon_{t-1} +\\epsilon_{t}\\)\n\n\nwhere \\(\\mu\\) is the mean of the time series, and \\(\\epsilon\\) is the error term. As with the AR model, further error term lags could be included in the MA. An MA process with highest lag of \\(q\\) is called an MA(q) process. The MA process assumes that the current observation of a time series is a linear combination of the weighted sum of past error terms. The error terms capture the unpredictable and random fluctuations in the time series that are not accounted for by the AR model. Below we simulate an MA(1) process and generate the ACF and PACF plots.\nset.seed(13)\ne_t&lt;-rnorm(300,0,0.5)\ny_t=2+0.95*dplyr::lag(e_t)+e_t\n\ntsibble(y=y_t,period=seq(1,length(y_t)),index=period) %&gt;% ACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Simulated MA(1)\")\n\ntsibble(y=y_t,period=seq(1,length(y_t)),index=period) %&gt;% PACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"PACF For Simulated MA(1)\")\n\n\n\n\n\n\n\n\n\n\nThe pattern shown is one significant spike in the first lag of the ACF, and a decaying PACF as lags get larger. This is the opposite of the AR(1) process where we had a decaying ACF and one spike in the PACF.\n\nU.S. Treasury Example\nConsider the percent monthly change the yield of U.S. treasury securities (TCM5Y). Below is the plot of the series.\n\ntreasury&lt;-read_csv(\"https://jagelves.github.io/Data/treasury.csv\")\n\ntreasury %&gt;% mutate(period=1:length(OBS)) -&gt; treasury\nas_tsibble(treasury, index = period) %&gt;%\n  autoplot() + theme_classic() + \n  labs(title=\"Percent Monthly Change the Yield of U.S. Treasury Securities\",\n       subtitle=\"Monthly (1953-2008)\",\n       x=\"\", y=\"\")\n\n\n\n\n\n\n\n\nInterestingly, the graph above looks like white noise. A very ragged and unpredictable pattern that fluctuates around a mean of approximately zero. Let’s take a look at the ACF and PACF.\nas_tsibble(treasury, index = period) %&gt;% \n  ACF(lag_max = 12, changetreasury) %&gt;% \n  autoplot()+ theme_bw() +\nlabs(x=\"\", y=\"\", title=\"ACF for TCM5Y\")\n\nas_tsibble(treasury, index = period) %&gt;% \n  PACF(lag_max = 12, changetreasury) %&gt;% \n  autoplot()+ theme_bw() +\nlabs(x=\"\", y=\"\", title=\"PACF for TCM5Y\")\n\n\n\n\n\n\n\n\n\n\nWhen comparing with the MA(1) simulated values, the ACF of TCM5Y reflects the same pattern. One spike, and all other lags not significantly different from zero in the ACF. The PACF doesn’t exactly replicate the one from the simulation, but the pattern created is the same. Mainly, there is a decaying PACF, a large lag 1 partial autocorrelation, followed by smaller ones until they all are insignificant. These correlations additionally alternate in sign. As with the AR(1) process, we can estimate the model, check that the errors are white noise, and then forecast.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#arima",
    "href": "ARIMA.html#arima",
    "title": "7  ARIMA",
    "section": "7.4 ARIMA",
    "text": "7.4 ARIMA\nThe ARIMA model is based on stationary, AR, and MA processes. ARIMA stands for AutoRegressive Integrated Moving Average.\n\nAutoRegressive (AR): The “AR” part means that the future values of a time series depend on its own past values. If today’s value is high, tomorrow’s value is likely to be high too.\nIntegrated (I): The “I” part is about making the time series stationary. It means we take the differences between consecutive observations to remove trends or seasonality.\nMoving Average (MA): The “MA” part means that the future values also depend on the past prediction errors (residuals). If the model predicted a value that was too high or too low in the past, it will try to correct that in the future predictions.\n\nARIMA combines these three components to forecast future values in a time series by considering its own past, removing trends, and accounting for past errors in predictions. In terms of notation, an ARIMA(1,1,2), will perform the first difference of the series, and estimate the model with an AR(1) and MA(2) component. Below we estimate the ARIMA model.\n\nRevisiting Chilango’s Restaurant\nLet’s apply the ARIMA model to the avocado data. The code below loads the data and creates the training and test set.\n\ncali&lt;-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\ncali %&gt;%\n  as_tsibble(key=geography,index=date,regular=T) %&gt;%\n  filter_index(\"2015-01-04\"~\"2018-12-02\") -&gt; cali\n\ncali %&gt;%\n  as_tsibble(key=geography,index=date,regular=T) %&gt;%\n  filter_index(.~\"2018-06-02\") -&gt; calits_train\n\ncali %&gt;%\n  as_tsibble(key=geography,\n             index=date, regular=T) %&gt;%\n  filter_index(\"2018-06-02\"~\"2018-12-02\") -&gt; calits_test\n\nNext, we will estimate the LS, ETS, AR1, and ARIMA models. We will allow an algorithm to specify the parameters of both the ETS and ARIMA models by using the ETS() and ARIMA() functions and omitting arguments. Within the ARIMA() function we will set the approximation argument to FALSE so that the search for the best ARIMA model is exhaustive. Note also how an AR1 model is calculated. The ARIMA() function is called along with the pdq() function. The first argument of the pdq() function is the order of the AR process (p), the next argument is the number of differences needed to make the series stationary (d), and the last argument is the order of the MA process (q). Hence, for the AR1 model we specify pdq(1,0,0).\n\nfit &lt;- model(calits_train,ETS=ETS(average_price),\n              ARIMA=ARIMA(average_price,approximation = F),\n              AR1=ARIMA(average_price~pdq(1,0,0)),\n              LS=TSLM(average_price~trend()))\n\nThe coefficients for the models can be retrieved by using the tidy() or coef() functions.\n\nfit %&gt;% coef()\n\n\n\n\n\n\n\n\n\nModel Coefficients For The Avocado Data\n\n\ngeography\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nCalifornia\nETS\nalpha\n0.6461\nNA\nNA\nNA\n\n\nCalifornia\nETS\nl[0]\n1.2082\nNA\nNA\nNA\n\n\nCalifornia\nARIMA\nar1\n0.9220\n0.1097\n8.4033\n0.0000\n\n\nCalifornia\nARIMA\nar2\n−0.7364\n0.1135\n−6.4873\n0.0000\n\n\nCalifornia\nARIMA\nma1\n−1.1478\n0.0807\n−14.2160\n0.0000\n\n\nCalifornia\nARIMA\nma2\n0.8490\n0.0966\n8.7888\n0.0000\n\n\nCalifornia\nAR1\nar1\n0.8816\n0.0350\n25.2051\n0.0000\n\n\nCalifornia\nAR1\nconstant\n0.1971\n0.0097\n20.3814\n0.0000\n\n\nCalifornia\nLS\n(Intercept)\n1.4856\n0.0391\n38.0158\n0.0000\n\n\nCalifornia\nLS\ntrend()\n0.0022\n0.0004\n5.8702\n0.0000\n\n\n\n\n\n\n\nThe models suggested by the algorithms are fairly simple. The ETS model suggested is a SES with a smoothing parameter of about \\(0.65\\). As for the ARIMA model, an ARIMA(2,0,2) process is suggested. Below we plot the ACF and the PACF of the average price of avocados.\ncalits_train %&gt;% ACF(lag_max=12, average_price) %&gt;%\nautoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Avocado Average Price\")\n\ncalits_train %&gt;% PACF(lag_max=12, average_price) %&gt;%\nautoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"PACF For Avocado Average Price\")\n\n\n\n\n\n\n\n\n\n\nThe patterns shown in the plots above are textbook AR(1). Hence, the inclusion of the model in the set of candidates. The code below forecasts the series 27 periods ahead and plots.\n\nfit %&gt;% forecast(h=27) %&gt;% autoplot(level=NULL) + \n  theme_classic() + \n  autolayer(cali, average_price) + \n  labs(y=\"\", title= \"California's Forecasted Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - Dec 2, 2018\",\n       x=\"\")\n\n\n\n\n\n\n\n\nInterestingly, the AR(1) and the LS model perform the best. The LS model overestimates the test data, while the AR(1) underestimates it. One could imagine combining the forecasts of these models to obtain a better one.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#combination-of-models-ensembles",
    "href": "ARIMA.html#combination-of-models-ensembles",
    "title": "7  ARIMA",
    "section": "7.5 Combination of Models (Ensembles)",
    "text": "7.5 Combination of Models (Ensembles)\nTo create a combination of models we can use the fit object containing the mable. Specifically, any model in the fit object can be combined using a linear combination. The code below combines the AR(1) model and the LS model by finding a simple average between both models.\n\nfit %&gt;%  mutate(Ensemble=0.5*LS+0.5*AR1) -&gt; fit2\n\nNow we can plot the forecasts for the ensemble, the AR1 and the LS model.\n\nfit2 %&gt;% select(AR1,LS,Ensemble) %&gt;%\n  forecast(h=27) %&gt;% autoplot(level=NULL) +\n  autolayer(cali,average_price) + theme_classic() +\n  labs(y=\"\", title= \"California's Forecasted Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - Dec 2, 2018\",\n       x=\"\")\n\n\n\n\n\n\n\n\nThe plot illustrates how the ensemble fits the test data better than each model individually. Let’s confirm the accuracy of the ensemble to the test set by calculating some measures.\n\nfit2 %&gt;% forecast(h=27) %&gt;% accuracy(cali)\n\n\n\n\n\n\n\n\n\nModel Fit\n\n\n.model\ngeography\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\nACF1\n\n\n\n\nAR1\nCalifornia\nTest\n0.16\n0.21\n0.16\n8.65\n8.65\n1.63\n1.49\n0.50\n\n\nARIMA\nCalifornia\nTest\n0.16\n0.20\n0.16\n8.20\n8.20\n1.55\n1.46\n0.51\n\n\nETS\nCalifornia\nTest\n0.20\n0.23\n0.20\n10.39\n10.39\n1.95\n1.69\n0.50\n\n\nEnsemble\nCalifornia\nTest\n0.03\n0.13\n0.10\n1.38\n5.26\n0.98\n0.92\n0.51\n\n\nLS\nCalifornia\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\n1.37\n1.14\n0.52",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#readings",
    "href": "ARIMA.html#readings",
    "title": "7  ARIMA",
    "section": "7.6 Readings",
    "text": "7.6 Readings\nChapter 9 of Hyndman (2021) deals with ARIMA models. The readings are not very technical and omit explaining the model selection process. For readers wanting even more mathematical details as well as a deep dive into the model selection process I recommend reading Gonzalez (2013). For those looking for an alternative to Hyndman (2021) you can try Svetunkov (2023) or Hank (2023).\n\nHyndman (2021) Chapter 9 (ARIMA Models).\nGonzalez (2013) Chapter 6 (MA models), Chapter 7 (AR models), Chapter 8 (Forecasting Practice I)\nSvetunkov (2023)\nHank (2023)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#leasons-learned",
    "href": "ARIMA.html#leasons-learned",
    "title": "7  ARIMA",
    "section": "7.7 Leasons Learned",
    "text": "7.7 Leasons Learned\nIn this module you have been introduced to ARIMA model. Particularly you have learned to:\n\nApply autocorrelations, partial autocorrelations, stationarity and white noise to model time series.\nIdentify the AR(1) and MA(1) processes using the ACF and PACF.\nModel the ARIMA process using the ARIMA() function.\nProvide forecasts of the ARIMA process in R.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gelves, J. Alejandro. 2022. Business Statistics. Bookdown. https://jagelves.github.io/BusinessStats/.\n\n\nGonzalez, Gloria. 2013. Forecasting for Economics and Business.\nPearson.\n\n\nGrolemund, Garret. 2014. Hands-on Programming with r. O’Reilly.\nhttps://jjallaire.github.io/hopr/#license.\n\n\nHank, Christoph. 2023. Introduction to Econometrics with r.\nUniversity of Duisburg-Essen. https://www.econometrics-with-r.org/index.html.\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice.\nOtexts. https://otexts.com/fpp3/.\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. Business Statistics.\nMcGraw Hill.\n\n\nKwak, Young, and Lisa Ingall. 2007. “Exploring Monte Carlo\nSimulation Applications for Project Management.”\n\n\nSvetunkov, Ivan. 2023. Forecasting and Analytics with the Augmented\nDynamic Adaptive Model (ADAM). Chapman; Hall/CRC. https://doi.org/10.1201/9781003452652.\n\n\nWickham, Hadley. 2017. R for Data Science. O’Reilly. https://r4ds.hadley.nz.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management\nScience. Cengage.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Modeling",
    "section": "",
    "text": "Preface\nMaking informed decisions is crucial in a world of uncertainty and ever-changing dynamics. This book is a guide that explores decision analysis, simulation, and time series techniques using R. It is recommended for anyone seeking to learn data-driven approaches to predict outcomes, simulate scenarios, and make informed choices.\nIn the book’s first part, we delve into decision analysis. Here, we explore various frameworks, such as decision trees and risk analysis, enabling us to assess our choices’ potential outcomes and consequences. We uncover strategies to quantify uncertainties, analyze trade-offs, and optimize decision paths. Real-world examples illustrate how these techniques are applied in business scenarios.\nThe book’s second part deals with simulation. We create dynamic, virtual environments to mimic complex systems in business and evaluate multiple scenarios. Through Monte Carlo simulation, we unlock the power to predict outcomes and quantify the impacts of our decisions. Practical exercises and step-by-step guidance equip readers with the skills to build and analyze simulations, enabling them to gain deeper insights into the consequences of their choices.\nIn the final part of this book, we introduce time series analysis. As time is fundamental in many domains, we explore techniques to extract meaningful patterns and forecast future values. The classical methods of ARIMA and ETS are introduced by illustrating their real-world applications in business. Readers will learn to navigate the intricacies of forecasting and leverage time series insights to enhance decision-making.\nComments are welcomed at jagelves@wm.edu",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ETS.html#exercises",
    "href": "ETS.html#exercises",
    "title": "6  ETS",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\n\nTesla is rapidly expanding its footprint in renewable energy and energy storage solutions, with innovations like the Powerwall, Megapack, and Solar Roof driving the company forward. Tesla’s energy storage systems are gaining traction in both residential and large-scale utility markets, while its vision of Virtual Power Plants (VPPs) is revolutionizing how power is managed and distributed.\nTesla is forecasting strong growth in energy deployment as they continue to scale their products and enter new markets. You have been asked to help forecast Tesla Energy’s deployment for Q4 2024, using historical quarterly data on energy storage and solar deployments from previous years.\nYou can find data on Tesla’s quarterly energy deployment (in megawatt hours) here: https://jagelves.github.io/Data/teslaE.csv\nForecast Tesla Energy’s deployment for the next four quarters using an ETS model with all additive terms, a TSLM model with linear trend and seasonality, and a TSLM model with quadratic trend and seasonality. Evaluate model performance using the information criterion. Finally, provide your forecast for the upcoming quarters with a graph. What is your expectation for next quarter? Does energy deployment go up or down relative to last quarter? What about when you compare to the same quarter last year?\n\n\n\nSuggested Answer\n\nWe can use the code below to develop the models:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nstorage&lt;-read_csv(\"http://jagelves.github.io/Data/teslaE.csv\")\n\nstorage %&gt;%\n  mutate(Date=yearquarter(Date)) %&gt;%\n  as_tsibble(index=Date) -&gt; storage_ts\n\nstorage_ts %&gt;%\n  model(LM=TSLM(EnergyStorage~trend()+season()),\n        LM2=TSLM(EnergyStorage~trend()+I(trend()^2)+season()),\n        ETS=ETS(EnergyStorage~error(\"A\") + trend(\"A\") + season(\"A\"))) -&gt;fit\n\nTo obtain the information criterion we can use the glance() function:\n\nfit %&gt;% glance() %&gt;% select(.model,AIC:BIC)\n\n# A tibble: 3 × 4\n  .model   AIC  AICc   BIC\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 LM      14.4  26.4  18.2\n2 LM2     11.2  29.9  15.7\n3 ETS     59.1 104.   64.9\n\n\nForecasts are obtained with the forecast() function setting h=4.\n\nfit %&gt;% forecast(h=4) %&gt;%\n  hilo(95)\n\n# A tsibble: 12 x 5 [1Q]\n# Key:       .model [3]\n   .model    Date EnergyStorage .mean                   `95%`\n   &lt;chr&gt;    &lt;qtr&gt;        &lt;dist&gt; &lt;dbl&gt;                  &lt;hilo&gt;\n 1 LM     2024 Q3     N(6.1, 3)  6.10 [2.716774,  9.486083]95\n 2 LM     2024 Q4     N(5.9, 3)  5.86 [2.480108,  9.249416]95\n 3 LM     2025 Q1   N(6.9, 3.1)  6.87 [3.407192, 10.336379]95\n 4 LM     2025 Q2   N(8.4, 3.1)  8.42 [4.952192, 11.881379]95\n 5 LM2    2024 Q3   N(8.1, 3.4)  8.11 [4.465782, 11.745374]95\n 6 LM2    2024 Q4   N(8.6, 4.4)  8.58 [4.472006, 12.680511]95\n 7 LM2    2025 Q1    N(10, 5.1)  9.97 [5.531357, 14.401500]95\n 8 LM2    2025 Q2    N(12, 6.9) 12.4  [7.256176, 17.535049]95\n 9 ETS    2024 Q3   N(6.6, 3.1)  6.59 [3.114998, 10.060041]95\n10 ETS    2024 Q4     N(6, 3.1)  5.98 [2.509588,  9.454631]95\n11 ETS    2025 Q1   N(6.7, 3.1)  6.65 [3.180104, 10.125147]95\n12 ETS    2025 Q2   N(8.1, 3.1)  8.06 [4.584554, 11.529598]95\n\n\nFinally, the graph is obtained using the autoplot() and autolayer() functions:\n\nfit %&gt;% forecast(h=4) %&gt;% autoplot(level=NULL)+\n  autolayer(storage_ts,EnergyStorage) + theme_clean()\n\n\n\n\n\n\n\n\nForecast suggest that energy deployment will go down relative to last quarter. However, if we compare the same quarter a year ago, energy deployment is expected to increase.\n\n\nIce Cream Heaven is a small ice cream shop that has recently purchased a new ice cream-making machine to expand its production and meet increased demand during the summer months. The machine was installed in July 2023, and while it has helped boost production, it has also increased the shop’s energy consumption.\nThe owners of Ice Cream Heaven want to estimate how much more they are paying in energy bills due to this new machine. They have historical monthly energy consumption data before and after the machine was installed and want to forecast what their energy consumption would have been without the machine to compare it with the actual values. Use the data found here: http://jagelves.github.io/Data/ElectricityBill.csv and an ETS model with all additive components to provide your estimate.\n\n\n\nSuggested Answer\n\nThe code below provides an estimate by forecasting the months of August 2023 onward with an ETS model with additive terms.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nrm(list=ls())\nElec&lt;-read_csv(\"http://jagelves.github.io/Data/ElectricityBill.csv\")\n\nElec %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  select(Date,`Bill Amount`) %&gt;% \n  as_tsibble(index=Date)-&gt; Elec_ts\n\npre&lt;-Elec_ts %&gt;% filter_index(.~\"2023 Jul\")\n\npre %&gt;% model(ETS=ETS(`Bill Amount`~error(\"A\") + trend(\"A\") + season(\"A\"))) -&gt; fitp\n\nfitp %&gt;% forecast(h=12) %&gt;% \n  as_tibble() %&gt;% select(Date,.mean) -&gt; NoMach\n\nElec_ts %&gt;% filter_index(\"2023 Aug\"~.) %&gt;% \n  as_tibble()  -&gt; Mach\n  \nsum(Mach$`Bill Amount`- NoMach$.mean)\n\n[1] 648.7878\n\n\nFollowing this analysis, the ice cream shop has paid an additional 649 dollars. Below is a graph that visualizes the difference:\n\nlibrary(ggthemes)\nElec_ts %&gt;% autoplot(`Bill Amount`) + theme_clean() +\n  geom_vline(xintercept=as.Date(\"2023-06-01\"), lty=2, col=\"blue\") +\n  labs(x=\"\", title=\"Electric Bill in Dollars\",\n       subtitle=\"Forecast in blue\") +\n  autolayer(forecast(fitp,h=12), level=NULL)\n\n\n\n\n\n\n\n\n\n\nRefer back to question 2, where your task is to estimate the extra cost in energy of a machine. Use the entire data set and forecast as scenario where the machine is kept and one where the machine is sold. Change the model to a TSLM with linear trend and seasonality. How much extra would the ice cream shop pay if they kept the machine?\n\n\n\nSuggested Answer\n\nCreate a dummy variable that tags the dates when the new machine was in operation and fit the TSLM model:\n\nElec_ts&lt;-mutate(Elec_ts,\n                Down=case_when(\n                  Date&gt;=yearmonth(\"2023 7\") ~ 1,\n                  TRUE ~ 0))\n\nElec_ts %&gt;%\n  model(\n    LM2 = TSLM(`Bill Amount`~trend()+season()+Down)) -&gt; fit2\n\nNow we can create a new tsibble with data that assumes the new machine and no new machine with the scenarios() function.\n\nMach &lt;- scenarios(\n  New_Mach = new_data(Elec_ts, 12) %&gt;% \n    mutate(Down=rep(1,12)),\n  Old_Mach = new_data(Elec_ts, 12) %&gt;% \n    mutate(Down=rep(0,12)),\n  names_to = \"Scenario\")\n\nLastly, we can use this new data to forecast with our model:\n\nforecast(fit2,new_data=Mach) -&gt;est\nas_tibble(est) %&gt;% group_by(Down) %&gt;% \n  summarise(Sum=sum(.mean)) %&gt;% pull(Sum) %&gt;% diff()\n\n[1] 535.3909\n\n\nIt will cost the ice cream shop an additional 535 dollars in energy consumption to keep the machine. Below is a graph of the two scenarios:\n\nElec_ts %&gt;%\n  autoplot(`Bill Amount`) + \n  autolayer(forecast(fit2,new_data=Mach),\n            level=NULL)+ theme_classic() +\n  labs(x=\"Months\", y=\"\",\n       title = \"Energy Consumption\",\n       subtitle = \"Scenario Forecast\") + theme_clean()\n\n\n\n\n\n\n\n\n\n\nDelta Airlines relies heavily on maintaining high load factors (the percentage of seats filled on flights) to maximize profitability and optimize operational efficiency. Accurately forecasting load factors helps Delta plan routes, set prices, and allocate resources effectively.\nDelta has ask for you to forecast the load factor the next four periods using historical data and two different time series models: the ETS model (algorithmic) and the TSLM model (with seasonality and trend). By doing so, you will help Delta better anticipate future demand and optimize its flight operations.\nYou can find the load factor data here: https://jagelves.github.io/Data/AirlinesLoad.csv Evaluate both models based on their RMSE on the test set (January 2022 to December 2023). Choose the model with the lowest RMSE as the best method for forecasting. Using the chosen model, forecast Delta’s load factor for the next four months (January 2024 to April 2024).\n\n\n\nSuggested Answer\n\nHere is the R code to complete the task. Start by loading the data and creating the tsibble.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nLoad&lt;-read_csv(\"http://jagelves.github.io/Data/AirlinesLoad.csv\")\n\nLoad %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  filter(Airline==\"Delta\") %&gt;% \n  select(Date,LF_total) %&gt;% as_tsibble(index=Date) -&gt; Load_ts\n\nNow we can create the train and test sets with the dates provided:\n\nLoad_ts %&gt;% filter_index(\"2022 Jan\"~\"2023 Dec\") -&gt; train\nLoad_ts %&gt;% filter_index(\"Jan 2024\"~.) -&gt; test\n\nNext, estimate both models and compare the accuracy of their predictions on the test set:\n\ntrain %&gt;% \n  model(TSLM=TSLM(LF_total~trend()+season()),\n        ETS=ETS(LF_total)) -&gt; fit\n\nfit %&gt;% forecast(test) %&gt;% \n  accuracy(test) %&gt;% select(.model,RMSE)\n\n# A tibble: 2 × 2\n  .model  RMSE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 ETS     2.45\n2 TSLM    3.38\n\n\nIt seems like the algorithmic ETS performs better. Finally, let’s forecast the series using the algorithmic ETS model:\n\nLoad_ts %&gt;% \n  model(ETS=ETS(LF_total)) %&gt;% \n  forecast(h=4)\n\n# A fable: 4 x 4 [1M]\n# Key:     .model [1]\n  .model     Date  LF_total .mean\n  &lt;chr&gt;     &lt;mth&gt;    &lt;dist&gt; &lt;dbl&gt;\n1 ETS    2024 May N(88, 16)  88.0\n2 ETS    2024 Jun N(92, 31)  92.5\n3 ETS    2024 Jul N(93, 47)  92.9\n4 ETS    2024 Aug N(91, 63)  91.0\n\n\nHere is the graph:\n\nLoad_ts %&gt;% \n  model(ETS=ETS(LF_total)) %&gt;% \n  forecast(h=4) %&gt;% autoplot(level=95) +\n  autolayer(train,LF_total) + autolayer(test,LF_total,lty=2) +\n   theme_clean() + \n  labs(title=\"Delta Airlines Load Factors\",\n       x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#exercises",
    "href": "ARIMA.html#exercises",
    "title": "7  ARIMA",
    "section": "7.8 Exercises",
    "text": "7.8 Exercises\n\nRecall problem 2 in chapter 6 (ExercisesETS), where you are tasked to forecast the energy consumption of a new ice cream machine. Use the entire data found here: http://jagelves.github.io/Data/ElectricityBill.csv to estimate an AR1 model. Graph the ACF and PACF and confirm that an AR1 is appropriate. Check residuals from the model and confirm that they resemble white noise. Create a graph of the series and the forecast for four periods ahead.\n\n\n\nSuggested Answer\n\nLet’s start by loading the data and creating a tsibble:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nrm(list=ls())\nElec&lt;-read_csv(\"http://jagelves.github.io/Data/ElectricityBill.csv\")\n\nElec %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  select(Date,`Bill Amount`) %&gt;% \n  as_tsibble(index=Date)-&gt; Elec_ts\n\nWe can now graph the ACF and the PACF of the Bill Amount variable:\nElec_ts %&gt;% ACF(`Bill Amount`,lag_max = 24) %&gt;% autoplot()\nElec_ts %&gt;% PACF(`Bill Amount`,lag_max = 24) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nThere is a one spike at lag number one in the ACF and one spike in lag number one in the PACF. We could use both an AR1 or MA1 process to model the behavior. Let’s estimate the AR1.\n\nElec_ts %&gt;% model(AR1=ARIMA(`Bill Amount`~pdq(1,0,0))) -&gt; fit\n\nNow we can check the residuals and see if they resemble white noise:\nfit %&gt;% augment() %&gt;% ACF(.resid,lag_max = 24) %&gt;% autoplot()\nfit %&gt;% augment() %&gt;% PACF(.resid,lag_max = 24) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nThe are no significant spikes. We can conclude that the errors resemble white noise.\n\n\nRecall problem 1 in chapter 6 (ExercisesETS), where you are tasked to forecast Tesla’s energy deployment. Use the entire data found here: http://jagelves.github.io/Data/ElectricityBill.csv to estimate an ensemble of the TSLM model with linear trend and seasonality, and a TSLM model with quadratic trend and seasonality. Let the weight of the linear model be 30% and that of the quadratic model 70%. Graph the forecast for the next four periods of the three models.\n\n\n\nSuggested Answer\n\nWe can start by loading the packages, data, and creating a tsibble:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nstorage&lt;-read_csv(\"http://jagelves.github.io/Data/teslaE.csv\")\n\nstorage %&gt;%\n  mutate(Date=yearquarter(Date)) %&gt;%\n  as_tsibble(index=Date) -&gt; storage_ts\n\nLet’s now create the Linear and Quadratic models using TSLM. Then we can create the ensemble model by using the mutate() function and the given weights:\n\nstorage_ts %&gt;%\n  model(LM=TSLM(EnergyStorage~trend()+season()),\n        LM2=TSLM(EnergyStorage~trend()+I(trend()^2)+season())) -&gt;fit\n\nfit %&gt;% mutate(Ensemble=0.3*LM+0.7*LM2) -&gt; fit2\n\nWe can now graph the forecast for each model:\n\nfit2 %&gt;% forecast(h=4) %&gt;% \n  autoplot(level=NULL) +\n  autolayer(storage_ts,EnergyStorage) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nUsing the monthly inflation data from this source: https://jagelves.github.io/Data/Inflation.csv, forecast the inflation rate for the next four months. First, use the ARIMA() function to let the model automatically select the best-fitting ARIMA model for the inflation data while incorporating the interest rate into the model with the command ARIMA(Inflation ~ InterestRate). Develop two scenarios: the Federal Reserve holds the interest rate steady at 5.33% over the next four months, and the Fed follows a decreasing schedule of 5.25%, 5.00%, 4.75%, and 4.50%. Identify the month the Fed reaches its target of 2% inflation in each scenario and determine which schedule achieves this target faster. Create a graph that shows each scenario and the target inflation rate.\n\n\n\nSuggested Answer\n\nStart by loading the packages, data, and creating a tsibble:\n\nrm(list=ls())\n\nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/Inflation.csv\")\n\nRows: 52 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date\ndbl (3): Inflation, InterestRate, S&Preturns\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata %&gt;% mutate(Date=yearmonth(mdy(Date))) %&gt;% \n  as_tsibble(index=Date) -&gt; data_ts\n\nNext, create the model and the scenarios:\n\ndata_ts %&gt;% \n  model(ARIMA=ARIMA(Inflation~InterestRate)) -&gt; fit2\n\nInt &lt;- scenarios(\n  Keep = new_data(data_ts, 4) %&gt;% \n    mutate(InterestRate=rep(5.33,4)),\n  Lower = new_data(data_ts, 4) %&gt;% \n    mutate(InterestRate=c(5.25,5.00,4.75,4.5)),\n  names_to = \"Scenario\")\n\nNext we can report the forecasts:\n\nforecast(fit2,new_data=Int)\n\n# A fable: 8 x 6 [1M]\n# Key:     Scenario, .model [2]\n  Scenario .model     Date    Inflation .mean InterestRate\n  &lt;chr&gt;    &lt;chr&gt;     &lt;mth&gt;       &lt;dist&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1 Keep     ARIMA  2024 Sep N(2.2, 0.15)  2.17         5.33\n2 Keep     ARIMA  2024 Oct  N(2.1, 0.6)  2.13         5.33\n3 Keep     ARIMA  2024 Nov  N(1.9, 1.2)  1.94         5.33\n4 Keep     ARIMA  2024 Dec  N(1.6, 1.9)  1.60         5.33\n5 Lower    ARIMA  2024 Sep N(2.2, 0.15)  2.20         5.25\n6 Lower    ARIMA  2024 Oct  N(2.2, 0.6)  2.24         5   \n7 Lower    ARIMA  2024 Nov  N(2.1, 1.2)  2.14         4.75\n8 Lower    ARIMA  2024 Dec  N(1.9, 1.9)  1.88         4.5 \n\n\nThe forecasts show that if the Fed keeps the rate at 5.33%, the inflation will reach its goal sometime in November. However, if the Fed decides to lower rates, we are looking at some time in December. Lastly, we can plot our scenarios:\n\nforecast(fit2,new_data=Int) %&gt;% autoplot(level=95) + \n  autolayer(data_ts,Inflation) + \n  theme_clean() + \n  geom_hline(yintercept=2, lty=2) + \n  labs(title=\"US Monthly Inflation Rate\",\n       subtitle=\"Keep interest rate at 5.33% vs. 25 basis point reductions\",\n       x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\nRising U.S. household debt is concerning because it increases financial strain on families, reduces disposable income, and heightens vulnerability to interest rate hikes. It raises the risk of defaults and bankruptcies, potentially destabilizing financial institutions and the broader economy. Additionally, higher debt levels can slow consumer spending and economic growth, while also creating risks in the housing market, making the economy more fragile during downturns. Use the data found here: https://jagelves.github.io/Data/USDebt.csv to forecast the US total debt of households using ARIMA. Start by graphing the ACF and PACF, then suggest two ARIMA models and let the algorithm choose the third. Calculate the BIC for the three models and select the model with the lowest BIC. Forecast the series into the future. By what date does the model predict the US Household debt reaches 20 trillion dollars?\n\n\n\nSuggested Answer\n\nStart by loading the data and packages:\n\nrm(list=ls())\n\nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/USDebt.csv\")\n\ndata %&gt;% mutate(Quarter=yearquarter(Quarter)) %&gt;% \n  as_tsibble(index=Quarter) -&gt; data_ts\n\nNow we can plot the ACF and the PACF. Since the series seems non-stationary, we’ll go ahead and plot the differenced series:\ndata_ts %&gt;% ACF(diff(Total)) %&gt;% autoplot()\ndata_ts %&gt;% PACF(diff(Total)) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nLooking at the graphs, we suggest using an AR1 model or AR3 model. Next, estimate the models:\n\ndata_ts %&gt;% \n  model(AR3=ARIMA(Total~pdq(3,1,0)),\n        AR1=ARIMA(Total~pdq(1,1,0)),\n        ARIMA=ARIMA(Total)) -&gt; fit\n\nNow we can observe the residuals:\n\nfit %&gt;% augment() %&gt;% ACF(.resid) %&gt;% autoplot()\n\n\n\n\n\n\n\n\nBoth the algorithmic model and the AR3 residuals look like white noise. Let’s look at the BIC:\n\nfit %&gt;% glance() %&gt;% select(.model,BIC)\n\n# A tibble: 3 × 2\n  .model   BIC\n  &lt;chr&gt;  &lt;dbl&gt;\n1 AR3    -131.\n2 AR1    -125.\n3 ARIMA  -128.\n\n\nThe AR3 model has the lowest BIC, so we choose this model for our forecast. Below is a graph of the forecast:\n\nfit %&gt;% forecast(h=20) %&gt;% \n  as_tsibble() %&gt;% filter(.model==\"AR3\") %&gt;% \n  autoplot(.mean, col=\"blue\", lty=2) +\n  autolayer(data_ts,Total) + theme_clean() +\n  labs(title=\"US Consumer Debt\",\n       subtitle=\"Trillion of Dollars\",\n       x=\"\",y=\"\") +\n  geom_hline(yintercept=20,lty=2)\n\n\n\n\n\n\n\n\nIf all things remain constant we should expect the Household debt to reach 20 trillion dollar by Q2 2028.\n\n\n\n\n\nGonzalez, Gloria. 2013. Forecasting for Economics and Business. Pearson.\n\n\nHank, Christoph. 2023. Introduction to Econometrics with r. University of Duisburg-Essen. https://www.econometrics-with-r.org/index.html.\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nSvetunkov, Ivan. 2023. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM). Chapman; Hall/CRC. https://doi.org/10.1201/9781003452652.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ETS.html#exercises6",
    "href": "ETS.html#exercises6",
    "title": "6  ETS",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\n\nTesla is rapidly expanding its footprint in renewable energy and energy storage solutions, with innovations like the Powerwall, Megapack, and Solar Roof driving the company forward. Tesla’s energy storage systems are gaining traction in both residential and large-scale utility markets, while its vision of Virtual Power Plants (VPPs) is revolutionizing how power is managed and distributed.\nTesla is forecasting strong growth in energy deployment as they continue to scale their products and enter new markets. You have been asked to help forecast Tesla Energy’s deployment for Q4 2024, using historical quarterly data on energy storage and solar deployments from previous years.\nYou can find data on Tesla’s quarterly energy deployment (in megawatt hours) here: https://jagelves.github.io/Data/teslaE.csv\nForecast Tesla Energy’s deployment for the next four quarters using an ETS model with all additive terms, a TSLM model with linear trend and seasonality, and a TSLM model with quadratic trend and seasonality. Evaluate model performance using the information criterion. Finally, provide your forecast for the upcoming quarters with a graph. What is your expectation for next quarter? Does energy deployment go up or down relative to last quarter? What about when you compare to the same quarter last year?\n\n\n\nSuggested Answer\n\nWe can use the code below to develop the models:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nstorage&lt;-read_csv(\"http://jagelves.github.io/Data/teslaE.csv\")\n\nstorage %&gt;%\n  mutate(Date=yearquarter(Date)) %&gt;%\n  as_tsibble(index=Date) -&gt; storage_ts\n\nstorage_ts %&gt;%\n  model(LM=TSLM(EnergyStorage~trend()+season()),\n        LM2=TSLM(EnergyStorage~trend()+I(trend()^2)+season()),\n        ETS=ETS(EnergyStorage~error(\"A\") + trend(\"A\") + season(\"A\"))) -&gt;fit\n\nTo obtain the information criterion we can use the glance() function:\n\nfit %&gt;% glance() %&gt;% select(.model,AIC:BIC)\n\n# A tibble: 3 × 4\n  .model   AIC  AICc   BIC\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 LM      14.4  26.4  18.2\n2 LM2     11.2  29.9  15.7\n3 ETS     59.1 104.   64.9\n\n\nForecasts are obtained with the forecast() function setting h=4.\n\nfit %&gt;% forecast(h=4) %&gt;%\n  hilo(95)\n\n# A tsibble: 12 x 5 [1Q]\n# Key:       .model [3]\n   .model    Date EnergyStorage .mean                   `95%`\n   &lt;chr&gt;    &lt;qtr&gt;        &lt;dist&gt; &lt;dbl&gt;                  &lt;hilo&gt;\n 1 LM     2024 Q3     N(6.1, 3)  6.10 [2.716774,  9.486083]95\n 2 LM     2024 Q4     N(5.9, 3)  5.86 [2.480108,  9.249416]95\n 3 LM     2025 Q1   N(6.9, 3.1)  6.87 [3.407192, 10.336379]95\n 4 LM     2025 Q2   N(8.4, 3.1)  8.42 [4.952192, 11.881379]95\n 5 LM2    2024 Q3   N(8.1, 3.4)  8.11 [4.465782, 11.745374]95\n 6 LM2    2024 Q4   N(8.6, 4.4)  8.58 [4.472006, 12.680511]95\n 7 LM2    2025 Q1    N(10, 5.1)  9.97 [5.531357, 14.401500]95\n 8 LM2    2025 Q2    N(12, 6.9) 12.4  [7.256176, 17.535049]95\n 9 ETS    2024 Q3   N(6.6, 3.1)  6.59 [3.114998, 10.060041]95\n10 ETS    2024 Q4     N(6, 3.1)  5.98 [2.509588,  9.454631]95\n11 ETS    2025 Q1   N(6.7, 3.1)  6.65 [3.180104, 10.125147]95\n12 ETS    2025 Q2   N(8.1, 3.1)  8.06 [4.584554, 11.529598]95\n\n\nFinally, the graph is obtained using the autoplot() and autolayer() functions:\n\nfit %&gt;% forecast(h=4) %&gt;% autoplot(level=NULL)+\n  autolayer(storage_ts,EnergyStorage) + theme_clean()\n\n\n\n\n\n\n\n\nForecast suggest that energy deployment will go down relative to last quarter. However, if we compare the same quarter a year ago, energy deployment is expected to increase.\n\n\nIce Cream Heaven is a small ice cream shop that has recently purchased a new ice cream-making machine to expand its production and meet increased demand during the summer months. The machine was installed in July 2023, and while it has helped boost production, it has also increased the shop’s energy consumption.\nThe owners of Ice Cream Heaven want to estimate how much more they are paying in energy bills due to this new machine. They have historical monthly energy consumption data before and after the machine was installed and want to forecast what their energy consumption would have been without the machine to compare it with the actual values. Use the data found here: http://jagelves.github.io/Data/ElectricityBill.csv and an ETS model with all additive components to provide your estimate.\n\n\n\nSuggested Answer\n\nThe code below provides an estimate by forecasting the months of August 2023 onward with an ETS model with additive terms.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nrm(list=ls())\nElec&lt;-read_csv(\"http://jagelves.github.io/Data/ElectricityBill.csv\")\n\nElec %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  select(Date,`Bill Amount`) %&gt;% \n  as_tsibble(index=Date)-&gt; Elec_ts\n\npre&lt;-Elec_ts %&gt;% filter_index(.~\"2023 Jul\")\n\npre %&gt;% model(ETS=ETS(`Bill Amount`~error(\"A\") + trend(\"A\") + season(\"A\"))) -&gt; fitp\n\nfitp %&gt;% forecast(h=12) %&gt;% \n  as_tibble() %&gt;% select(Date,.mean) -&gt; NoMach\n\nElec_ts %&gt;% filter_index(\"2023 Aug\"~.) %&gt;% \n  as_tibble()  -&gt; Mach\n  \nsum(Mach$`Bill Amount`- NoMach$.mean)\n\n[1] 648.7878\n\n\nFollowing this analysis, the ice cream shop has paid an additional 649 dollars. Below is a graph that visualizes the difference:\n\nlibrary(ggthemes)\nElec_ts %&gt;% autoplot(`Bill Amount`) + theme_clean() +\n  geom_vline(xintercept=as.Date(\"2023-06-01\"), lty=2, col=\"blue\") +\n  labs(x=\"\", title=\"Electric Bill in Dollars\",\n       subtitle=\"Forecast in blue\") +\n  autolayer(forecast(fitp,h=12), level=NULL)\n\n\n\n\n\n\n\n\n\n\nRefer back to question 2, where your task is to estimate the extra cost in energy of a machine. Use the entire data set and forecast as scenario where the machine is kept and one where the machine is sold. Change the model to a TSLM with linear trend and seasonality. How much extra would the ice cream shop pay if they kept the machine?\n\n\n\nSuggested Answer\n\nCreate a dummy variable that tags the dates when the new machine was in operation and fit the TSLM model:\n\nElec_ts&lt;-mutate(Elec_ts,\n                Down=case_when(\n                  Date&gt;=yearmonth(\"2023 7\") ~ 1,\n                  TRUE ~ 0))\n\nElec_ts %&gt;%\n  model(\n    LM2 = TSLM(`Bill Amount`~trend()+season()+Down)) -&gt; fit2\n\nNow we can create a new tsibble with data that assumes the new machine and no new machine with the scenarios() function.\n\nMach &lt;- scenarios(\n  New_Mach = new_data(Elec_ts, 12) %&gt;% \n    mutate(Down=rep(1,12)),\n  Old_Mach = new_data(Elec_ts, 12) %&gt;% \n    mutate(Down=rep(0,12)),\n  names_to = \"Scenario\")\n\nLastly, we can use this new data to forecast with our model:\n\nforecast(fit2,new_data=Mach) -&gt;est\nas_tibble(est) %&gt;% group_by(Down) %&gt;% \n  summarise(Sum=sum(.mean)) %&gt;% pull(Sum) %&gt;% diff()\n\n[1] 535.3909\n\n\nIt will cost the ice cream shop an additional 535 dollars in energy consumption to keep the machine. Below is a graph of the two scenarios:\n\nElec_ts %&gt;%\n  autoplot(`Bill Amount`) + \n  autolayer(forecast(fit2,new_data=Mach),\n            level=NULL)+ theme_classic() +\n  labs(x=\"Months\", y=\"\",\n       title = \"Energy Consumption\",\n       subtitle = \"Scenario Forecast\") + theme_clean()\n\n\n\n\n\n\n\n\n\n\nDelta Airlines relies heavily on maintaining high load factors (the percentage of seats filled on flights) to maximize profitability and optimize operational efficiency. Accurately forecasting load factors helps Delta plan routes, set prices, and allocate resources effectively.\nDelta has ask for you to forecast the load factor the next four periods using historical data and two different time series models: the ETS model (algorithmic) and the TSLM model (with seasonality and trend). By doing so, you will help Delta better anticipate future demand and optimize its flight operations.\nYou can find the load factor data here: https://jagelves.github.io/Data/AirlinesLoad.csv Evaluate both models based on their RMSE on the test set (January 2022 to December 2023). Choose the model with the lowest RMSE as the best method for forecasting. Using the chosen model, forecast Delta’s load factor for the next four months (January 2024 to April 2024).\n\n\n\nSuggested Answer\n\nHere is the R code to complete the task. Start by loading the data and creating the tsibble.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nLoad&lt;-read_csv(\"http://jagelves.github.io/Data/AirlinesLoad.csv\")\n\nLoad %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  filter(Airline==\"Delta\") %&gt;% \n  select(Date,LF_total) %&gt;% as_tsibble(index=Date) -&gt; Load_ts\n\nNow we can create the train and test sets with the dates provided:\n\nLoad_ts %&gt;% filter_index(\"2022 Jan\"~\"2023 Dec\") -&gt; train\nLoad_ts %&gt;% filter_index(\"Jan 2024\"~.) -&gt; test\n\nNext, estimate both models and compare the accuracy of their predictions on the test set:\n\ntrain %&gt;% \n  model(TSLM=TSLM(LF_total~trend()+season()),\n        ETS=ETS(LF_total)) -&gt; fit\n\nfit %&gt;% forecast(test) %&gt;% \n  accuracy(test) %&gt;% select(.model,RMSE)\n\n# A tibble: 2 × 2\n  .model  RMSE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 ETS     2.45\n2 TSLM    3.38\n\n\nIt seems like the algorithmic ETS performs better. Finally, let’s forecast the series using the algorithmic ETS model:\n\nLoad_ts %&gt;% \n  model(ETS=ETS(LF_total)) %&gt;% \n  forecast(h=4)\n\n# A fable: 4 x 4 [1M]\n# Key:     .model [1]\n  .model     Date  LF_total .mean\n  &lt;chr&gt;     &lt;mth&gt;    &lt;dist&gt; &lt;dbl&gt;\n1 ETS    2024 May N(88, 16)  88.0\n2 ETS    2024 Jun N(92, 31)  92.5\n3 ETS    2024 Jul N(93, 47)  92.9\n4 ETS    2024 Aug N(91, 63)  91.0\n\n\nHere is the graph:\n\nLoad_ts %&gt;% \n  model(ETS=ETS(LF_total)) %&gt;% \n  forecast(h=4) %&gt;% autoplot(level=95) +\n  autolayer(train,LF_total) + autolayer(test,LF_total,lty=2) +\n   theme_clean() + \n  labs(title=\"Delta Airlines Load Factors\",\n       x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  }
]