[
  {
    "objectID": "ETS.html",
    "href": "ETS.html",
    "title": "6  ETS",
    "section": "",
    "text": "6.1 ETS Components\nIn this section we will be introducing and applying the Error, Trend, Seasonality (ETS) model. This model provides a flexible approach for modeling and forecasting time series data by incorporating components for error, trend, and seasonality. The ETS model allows different combinations of these components to be included in the model based on the characteristics observed in the data.\nETS models build on simple exponential smoothing (SES). The basic idea behind SES is to assign more weight to recent observations and gradually decrease the weights as the observations become older. The model emphasizes the most recent data points and gives less importance to older observations.\nMathematically, the simple exponential smoothing model can be defined as:\nwhere \\(\\hat{y}_{t+h}\\) is the forecast of period \\(t+h\\), \\(l_{t}\\) is smoothed value of the series at time \\(t\\), \\(y_t\\) is the value observed at the current time period \\(t\\) and \\(\\alpha\\) is the smoothing parameter. Note that when \\(\\alpha\\) is equal to one, the forecast equation is equivalent to the Naive model, and when \\(\\alpha\\) is equal to zero, the smoothing equation is always \\(l_{t-1}\\).\nThe SES model is useful when forecasting series that have no trend or seasonality. The SES model can easily be modified to account for trend and seasonality by adding additional components. For example, the Holt’s linear trend method adds a component to account for a linear trend, the damped trend methods flatten the trend some time into the future, and the Holt-Winters model accounts for seasonality. The collection of models generated by adding different components are summarized as Error, Trend, and Seasonality (ETS) models. We apply the ETS model to the deliveries of the electric car company Tesla in the sections below.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#ets-components",
    "href": "ETS.html#ets-components",
    "title": "6  ETS",
    "section": "",
    "text": "Forecast Equation: \\(\\hat{y}_{t+h}=l_t\\)\n\n\nSmoothing Equation: \\(l_{t}=\\alpha y_t + (1-\\alpha)l_{t-1}\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#teslas-deliveries",
    "href": "ETS.html#teslas-deliveries",
    "title": "6  ETS",
    "section": "6.2 Tesla’s Deliveries",
    "text": "6.2 Tesla’s Deliveries\nDeliveries are a carefully watched number by Tesla shareholders and are the closest approximation of sales disclosed by the company. Additionally, Tesla’s deliveries are closely followed due to their impact on financial markets, the EV industry, innovation and disruption, production efficiency, and the growth of the EV market. The numbers serve as a key performance indicator for Tesla’s success and provide insights into the broader trends in the electric vehicle industry. Can we use the ETS model to forecast Tesla’s deliveries?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#sec-data",
    "href": "ETS.html#sec-data",
    "title": "6  ETS",
    "section": "6.3 The Data",
    "text": "6.3 The Data\nThe data can be found here Tesla. Below is code that inputs the data as a tsibble in R.\n\nlibrary(fpp3)\n\n# Create tsibble\ntesla&lt;-tsibble(\n  period=yearquarter(c(\"2016:Q1\",\"2016:Q2\",\"2016:Q3\",\"2016:Q4\",\n                       \"2017:Q1\",\"2017:Q2\",\"2017:Q3\",\"2017:Q4\",\n                       \"2018:Q1\",\"2018:Q2\",\"2018:Q3\",\"2018:Q4\",\n                       \"2019:Q1\",\"2019:Q2\",\"2019:Q3\",\"2019:Q4\",\n                       \"2020:Q1\",\"2020:Q2\",\"2020:Q3\",\"2020:Q4\",\n                       \"2021:Q1\",\"2021:Q2\",\"2021:Q3\",\"2021:Q4\",\n                       \"2022:Q1\",\"2022:Q2\",\"2022:Q3\",\"2022:Q4\",\n                       \"2023:Q1\",\"2023:Q2\")),\n  deliveries=c(14.8,14.4,24.5,22.2,\n               25,22,26.2,29.9,\n               30,40.7,83.5,90.7,\n               63,95.2,97,112,\n               88.4,90.7,139.3,180.6,\n               184.82,201.25,241.3,308.6,\n               310.5,254.7,343.8,405.3,\n               422.9,466.1),\n  index=period     # This is the time variable\n)\n\nAs you can see the tsibble is created with the tsibble() function included in the fpp3 package. The yearquarter() function from the lubridate package is used to coerce the period data to a date. The time variable is then specified via the index parameter. The code below creates the plot of Tesla’s deliveries using the autoplot() function.\n\ntesla %&gt;% autoplot(.vars=deliveries) + theme_classic() +\n  labs(title= \"Tesla Car Deliveries\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")\n\n\n\n\n\n\n\n\nThe most striking aspect of Tesla’s deliveries is the exponential trend. There also seems to be a seasonal component, with relatively higher production Q4 versus the other quarters. These characteristics will be adopted by the ETS model to forecast the series. Below we can see the STL decomposition that confirm these characteristics.\n\ntesla %&gt;%\n  model(STL(deliveries~trend() + season())) %&gt;%\n  components() %&gt;% autoplot() + theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#models",
    "href": "ETS.html#models",
    "title": "6  ETS",
    "section": "6.4 Models",
    "text": "6.4 Models\nTo model the data and create the appropriate forecasts, we start by generating test and training sets from the available data.\n\ntrain_tesla&lt;-filter_index(.data=tesla,\"2016 Q1\"~\"2021 Q4\")\ntest_tesla&lt;-filter_index(.data=tesla,\"2022 Q1\"~\"2023 Q2\")\n\nThere is no fixed rule for determining the length of the train and test sets. In this example, it is important to allocate a sufficiently large portion of the data to the training set to capture the underlying seasonality and trend of Tesla’s deliveries. The sets are easily created using the filter_index() function.\nFive models will be estimated based on ETS. The first one is the Simple Exponential Smoothing model with additive errors (SES), the Holt model that includes an additive trend (HOLT), a dampened trend model (DAMPED), a damped model with seasonality (DAMPEDS), and finally an algorithmic function that attempts to select the best ETS model (see Hyndman (2021), Chapter 8). Along with these five models two more models are set forth. The first one is a simple least squares model (LS) and the second one is a quadratic model with seasonality dummies (LSS).\nModel selection will be done via cross validation. Recall, that the the stretch_tsibble() function reshapes the tsibble to accommodate for cross validation. The .init parameter sets the first eight observations to estimate our initial model and the .step argument increases the training set by four. Cross validation is done four periods ahead (a year) and accuracy measures are created by comparing forecasts to the test set.\nEach component of the ETS model can be included as either multiplicative (\\(M\\)) or additive (\\(A\\)). The trend component can be assigned to be damped (\\(Ad\\) or \\(Md\\)). If the component is to be omitted from the model, None (\\(N\\)) is specified. Below is the code to estimate the models and the results of the cross validation.\n\nlibrary(gt)\ntrain_tesla %&gt;% stretch_tsibble(.init = 8, .step=4) %&gt;%\n  model(\n    SES=ETS(deliveries ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    HOLT=ETS(deliveries ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    DAMPED=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"N\")),\n    DAMPEDS=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\nALGO=ETS(deliveries),\nLS = TSLM(deliveries ~ trend()+I(trend()^2)),\nLSS = TSLM(deliveries ~ trend()+I(trend()^2)+season()))%&gt;%\n  forecast(h = 4) %&gt;%\n  accuracy(tesla) %&gt;% select(-\"ACF1\") \n\n\n\n\n\n\n\n\n\nCross Validation Models\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\n\n\n\n\nALGO\nTest\n40.231\n57.608\n45.297\n23.474\n28.654\n1.042\n0.990\n\n\nDAMPED\nTest\n18.701\n55.497\n44.900\n8.033\n32.803\n1.033\n0.954\n\n\nDAMPEDS\nTest\n21.011\n47.198\n38.590\n9.399\n23.120\n0.888\n0.811\n\n\nHOLT\nTest\n15.122\n57.200\n46.728\n4.127\n34.811\n1.075\n0.983\n\n\nLS\nTest\n11.657\n43.208\n37.200\n2.527\n28.375\n0.856\n0.742\n\n\nLSS\nTest\n9.739\n41.890\n36.739\n1.641\n28.748\n0.845\n0.720\n\n\nSES\nTest\n40.991\n58.160\n45.750\n23.988\n28.824\n1.053\n0.999\n\n\n\n\n\n\n\nThe accuracy measures reveal that the DAMPEDS and LSS models perform consistently well. Below, we will continue with the DAMPEDS and LSS models as the trend seems to be exponential and there seems to be evidence of seasonality. These model are estimated and saved into an object called fit below.\n\nfit &lt;- tesla %&gt;%\n  model(\n    DAMPEDS = ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\n    LSS = TSLM(deliveries ~ trend()+I(trend()^2)+season())\n  )\n\nIf one is interested in retrieving the model coefficients, one can use the tidy() (or coef()) function. Below the function is used along with the fit object to retrieve the coefficients of the Least Squares model with seasonality:\n\ntidy(fit) %&gt;% filter(.model==\"LSS\") %&gt;%\n  select(-\".model\")\n\n\n\n\n\n\n\n\n\nLSS Model Coefficients\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.43\n13.97\n2.61\n0.02\n\n\ntrend()\n−7.33\n1.92\n−3.82\n0.00\n\n\nI(trend()^2)\n0.70\n0.06\n11.62\n0.00\n\n\nseason()year2\n−8.63\n10.91\n−0.79\n0.44\n\n\nseason()year3\n8.07\n11.35\n0.71\n0.48\n\n\nseason()year4\n21.40\n11.35\n1.88\n0.07\n\n\n\n\n\n\n\nThe output above, reveals that the seasonal dummy for Q4 is statistically significant at the \\(10\\)% confirming the seasonal pattern found in the decomposition (Section 6.3). The plot below shows the fit of the models with the blue line representing the LSS model and the red line the DAMPEDS model.\n\ntesla %&gt;% autoplot(deliveries, lwd=1.2, alpha=0.5) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"blue\",\n            data = augment(fit)  %&gt;% filter(`.model`==\"LSS\")) +\n              geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %&gt;% filter(`.model`==\"DAMPEDS\")) + \n  labs(title= \"Tesla Car Deliveries Fitted Values\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#information-criterion",
    "href": "ETS.html#information-criterion",
    "title": "6  ETS",
    "section": "6.5 Information Criterion",
    "text": "6.5 Information Criterion\nWe can also attempt to select our models via the AIC, AICc, or BIC. The code below summarizes these measure for the models considered.\n\ntrain_tesla %&gt;%\n  model(\n    SES=ETS(deliveries ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    HOLT=ETS(deliveries ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    DAMPED=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"N\")),\n    DAMPEDS=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\nALGO=ETS(deliveries),\nLS = TSLM(deliveries ~ trend()+I(trend()^2)),\nLSS = TSLM(deliveries ~ trend()+I(trend()^2)+season())) %&gt;% \n  report()  %&gt;%\n  select('.model',\"AIC\",\"AICc\",\"BIC\")\n\n\n\n\n\n\n\n\n\nModel Fit Information Criterion\n\n\n.model\nAIC\nAICc\nBIC\n\n\n\n\nSES\n237.16\n238.36\n240.69\n\n\nHOLT\n233.99\n237.32\n239.88\n\n\nDAMPED\n236.37\n241.31\n243.44\n\n\nDAMPEDS\n233.92\n250.84\n245.70\n\n\nALGO\n223.19\n224.39\n226.73\n\n\nLS\n150.12\n152.22\n154.83\n\n\nLSS\n147.17\n154.17\n155.41\n\n\n\n\n\n\n\nHere, once again the LSS model seems to perform the best as it provides the lowest values. Among the ETS models, the ALGO model now stands out. This should be of no surprise, as the ALGO model is designed to choose ETS components that minimize the AIC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#forecasts",
    "href": "ETS.html#forecasts",
    "title": "6  ETS",
    "section": "6.6 Forecasts",
    "text": "6.6 Forecasts\nForecasts are created by using the fit object. We will forecast four quarters ahead using the forecast() function. The code below generates a table with the forecasts.\n\nlibrary(gt)\nfit %&gt;%\n  forecast(h = 4) %&gt;% select(-\".model\") -&gt; deliveries_fc\ndeliveries_fc\n\n\n\n\n\n\n\n\n\nForecasts\n\n\nperiod\ndeliveries\n.mean\n.model\n\n\n\n\n2023 Q3\nN(516, 854)\n515.90\nDAMPEDS\n\n\n2023 Q4\nN(563, 1681)\n562.94\nDAMPEDS\n\n\n2024 Q1\nN(579, 2895)\n578.73\nDAMPEDS\n\n\n2024 Q2\nN(602, 4563)\n601.50\nDAMPEDS\n\n\n2023 Q3\nN(489, 709)\n488.95\nLSS\n\n\n2023 Q4\nN(539, 754)\n538.99\nLSS\n\n\n2024 Q1\nN(556, 782)\n555.69\nLSS\n\n\n2024 Q2\nN(587, 844)\n586.56\nLSS\n\n\n\n\n\n\n\nForecasts for the four quarters are shown above, with the corresponding mean. In general, both models predict Tesla will continue its trend and increase its deliveries every quarter. According to the DAMPEDS model, Tesla is expected to deliver about \\(516,000\\) cars on average. The plot below illustrates the forecasts for both models along with the \\(95\\)% prediction intervals. The increasing standard deviation for future periods reminds us that longer-period forecasts have even more uncertainty.\n\nfit %&gt;%\n  forecast(h = 4) %&gt;%\n  autoplot(tesla, level=95) +\n  labs(x=\"Quarter\", y=\"\",\n       title = \"Tesla Car Deliveries Forecasts\",\n       subtitle = \"Q1 2017 to Q2 2023\") + theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#scenarios",
    "href": "ETS.html#scenarios",
    "title": "6  ETS",
    "section": "6.7 Scenarios",
    "text": "6.7 Scenarios\nOn October 9, 2023, Tesla announced 435,059 deliveries for Q3, signaling strong performance. However, during the previous earnings call, Tesla had already cautioned investors about potential delivery impacts due to planned factory shutdowns and upgrades. These shutdowns are necessary to retool production lines for their upcoming vehicles, the Highland Model 3 and Cybertruck. Interestingly, a similar factory shutdown occurred in Q2 2022, giving us valuable historical data to make more precise predictions.\nTo factor in the effect of these shutdowns, we can introduce a dummy variable, where we assign a value of 1 during shutdown quarters and 0 otherwise. Here’s the code:\n\ntesla&lt;-mutate(tesla,\n                Down=case_when(\n                  period==yearquarter(\"2022 Q2\") ~ 1,\n                                       TRUE ~ 0))\n\nThe Down variable, captures the impact of factory shutdowns on deliveries. Now, we can incorporate this information into our Time Series Linear Model (TSLM) to estimate Tesla’s deliveries more accurately:\n\nfit &lt;- tesla %&gt;%\n  model(\n    LSS = TSLM(deliveries~trend()+\n                 I(trend()^2)+season()+Down)\n  )\n\nNext, let’s explore two possible scenarios: one where Tesla undergoes a factory shutdown and another where production continues uninterrupted. Using the scenarios() function, we can create a dataset for forecasting over the next four periods, simulating both situations:\n\nDown_Scenario &lt;- scenarios(\n  Factory_Down = new_data(tesla, 4) |&gt;\n    mutate(Down=rep(1,4)),\n  Factory_Up = new_data(tesla, 4) |&gt;\n    mutate(Down=rep(0,4)),\n  names_to = \"Scenario\")\n\nFinally, we can visualize how each scenario affects Tesla’s future delivery forecasts. Whether Tesla experiences another factory shutdown or keeps production running smoothly, this analysis will help us better understand the potential outcomes.\n\ntesla %&gt;%\n  autoplot(deliveries) + \n  autolayer(forecast(fit,new_data=Down_Scenario),\n            level=NULL)+ theme_classic() +\n  labs(x=\"Quarter\", y=\"\",\n       title = \"Tesla Car Deliveries Forecasts\",\n       subtitle = \"Scenario Forecast\") + theme_classic()\n\n\n\n\n\n\n\n\nThe new forecast of 429,404 is now closer to the reported delivery number of 435,059 for Q3 2023 when compared to the analyst consensus estimate of 454,809. For the 4th quarter of 2023 the analyst consensus was 480,500, which is inline with the forecast of the LSS model (about 480,000). The 95% confidence interval of [422,553, 537,182] is retrieved using the hilo() function.\n\nfit %&gt;% forecast(new_data=Down_Scenario) %&gt;%\n  hilo(95) %&gt;% unpack_hilo(\"95%\") %&gt;%\n  as_tibble() %&gt;%\n  select(Down,period,.mean,`95%_lower`,`95%_upper`) %&gt;%\n  gt() %&gt;% fmt_number(columns=c(.mean,\n                       `95%_lower`,`95%_upper`),\n             decimals = 2)\n\n\n\n\n\n\n\nDown\nperiod\n.mean\n95%_lower\n95%_upper\n\n\n\n\n1\n2023 Q3\n429.40\n373.00\n485.81\n\n\n1\n2023 Q4\n479.87\n422.55\n537.18\n\n\n1\n2024 Q1\n497.35\n439.66\n555.04\n\n\n1\n2024 Q2\n537.02\n480.78\n593.27\n\n\n0\n2023 Q3\n496.03\n451.89\n540.16\n\n\n0\n2023 Q4\n546.49\n500.98\n592.00\n\n\n0\n2024 Q1\n563.97\n517.60\n610.35\n\n\n0\n2024 Q2\n603.65\n554.66\n652.64",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#readings",
    "href": "ETS.html#readings",
    "title": "6  ETS",
    "section": "6.8 Readings",
    "text": "6.8 Readings\nThe main reading for ETS models comes from Chapter 8 of Hyndman (2021). These readings provide a bit more detail on the mathematical background behind each model and a few more applications. For an overview of the moving averages, the Holt, and Winters’ models review Winston and Albright (2019).\n\nHyndman (2021) Chapter 8 (Exponential Smoothing).\nWinston and Albright (2019) Chapter 13.5 (Overview of Time Series Models), 13.6 (Moving Average Models), 13.7 (Exponential Smoothing Models).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#leasons-learned",
    "href": "ETS.html#leasons-learned",
    "title": "6  ETS",
    "section": "6.9 Leasons Learned",
    "text": "6.9 Leasons Learned\nIn this module you have been introduced to ETS model. Particularly you have learned to:\n\nUse the model() and ETS() functions to estimate the model.\nIdentify when ETS model is superior to other model by using the cross validation and information criterion.\nForecast time series with the ETS model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ETS.html#exercises6",
    "href": "ETS.html#exercises6",
    "title": "6  ETS",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\n\nTesla is rapidly expanding its footprint in renewable energy and energy storage solutions, with innovations like the Powerwall, Megapack, and Solar Roof driving the company forward. Tesla’s energy storage systems are gaining traction in both residential and large-scale utility markets, while its vision of Virtual Power Plants (VPPs) is revolutionizing how power is managed and distributed.\nTesla is forecasting strong growth in energy deployment as they continue to scale their products and enter new markets. You have been asked to help forecast Tesla Energy’s deployment for Q4 2024, using historical quarterly data on energy storage and solar deployments from previous years.\nYou can find data on Tesla’s quarterly energy deployment (in megawatt hours) here: https://jagelves.github.io/Data/teslaE.csv\nForecast Tesla Energy’s deployment for the next four quarters using an ETS model with all additive terms, a TSLM model with linear trend and seasonality, and a TSLM model with quadratic trend and seasonality. Evaluate model performance using the information criterion. Finally, provide your forecast for the upcoming quarters with a graph. What is your expectation for next quarter? Does energy deployment go up or down relative to last quarter? What about when you compare to the same quarter last year?\n\n\n\nSuggested Answer\n\nWe can use the code below to develop the models:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nstorage&lt;-read_csv(\"http://jagelves.github.io/Data/teslaE.csv\")\n\nstorage %&gt;%\n  mutate(Date=yearquarter(Date)) %&gt;%\n  as_tsibble(index=Date) -&gt; storage_ts\n\nstorage_ts %&gt;%\n  model(LM=TSLM(EnergyStorage~trend()+season()),\n        LM2=TSLM(EnergyStorage~trend()+I(trend()^2)+season()),\n        ETS=ETS(EnergyStorage~error(\"A\") + trend(\"A\") + season(\"A\"))) -&gt;fit\n\nTo obtain the information criterion we can use the glance() function:\n\nfit %&gt;% glance() %&gt;% select(.model,AIC:BIC)\n\n# A tibble: 3 × 4\n  .model   AIC  AICc   BIC\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 LM      23.5  32.9  28.2\n2 LM2     14.2  28.2  19.7\n3 ETS     74.9 105.   81.8\n\n\nForecasts are obtained with the forecast() function setting h=4.\n\nfit %&gt;% forecast(h=4) %&gt;%\n  hilo(95)\n\n# A tsibble: 12 x 5 [1Q]\n# Key:       .model [3]\n   .model    Date\n   &lt;chr&gt;    &lt;qtr&gt;\n 1 LM     2025 Q1\n 2 LM     2025 Q2\n 3 LM     2025 Q3\n 4 LM     2025 Q4\n 5 LM2    2025 Q1\n 6 LM2    2025 Q2\n 7 LM2    2025 Q3\n 8 LM2    2025 Q4\n 9 ETS    2025 Q1\n10 ETS    2025 Q2\n11 ETS    2025 Q3\n12 ETS    2025 Q4\n# ℹ 3 more variables: EnergyStorage &lt;dist&gt;, .mean &lt;dbl&gt;, `95%` &lt;hilo&gt;\n\n\nFinally, the graph is obtained using the autoplot() and autolayer() functions:\n\nfit %&gt;% forecast(h=4) %&gt;% autoplot(level=NULL)+\n  autolayer(storage_ts,EnergyStorage) + theme_clean()\n\n\n\n\n\n\n\n\nForecast suggest that energy deployment will go down relative to last quarter. However, if we compare the same quarter a year ago, energy deployment is expected to increase.\n\n\nIce Cream Heaven is a small ice cream shop that has recently purchased a new ice cream-making machine to expand its production and meet increased demand during the summer months. The machine was installed in July 2023, and while it has helped boost production, it has also increased the shop’s energy consumption.\nThe owners of Ice Cream Heaven want to estimate how much more they are paying in energy bills due to this new machine. They have historical monthly energy consumption data before and after the machine was installed and want to forecast what their energy consumption would have been without the machine to compare it with the actual values. Use the data found here: http://jagelves.github.io/Data/ElectricityBill.csv and an ETS model with all additive components to provide your estimate.\n\n\n\nSuggested Answer\n\nThe code below provides an estimate by forecasting the months of August 2023 onward with an ETS model with additive terms.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nrm(list=ls())\nElec&lt;-read_csv(\"http://jagelves.github.io/Data/ElectricityBill.csv\")\n\nElec %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  select(Date,`Bill Amount`) %&gt;% \n  as_tsibble(index=Date)-&gt; Elec_ts\n\npre&lt;-Elec_ts %&gt;% filter_index(.~\"2023 Jul\")\n\npre %&gt;% model(ETS=ETS(`Bill Amount`~error(\"A\") + trend(\"A\") + season(\"A\"))) -&gt; fitp\n\nfitp %&gt;% forecast(h=12) %&gt;% \n  as_tibble() %&gt;% select(Date,.mean) -&gt; NoMach\n\nElec_ts %&gt;% filter_index(\"2023 Aug\"~.) %&gt;% \n  as_tibble()  -&gt; Mach\n  \nsum(Mach$`Bill Amount`- NoMach$.mean)\n\n[1] 648.7878\n\n\nFollowing this analysis, the ice cream shop has paid an additional 649 dollars. Below is a graph that visualizes the difference:\n\nlibrary(ggthemes)\nElec_ts %&gt;% autoplot(`Bill Amount`) + theme_clean() +\n  geom_vline(xintercept=as.Date(\"2023-06-01\"), lty=2, col=\"blue\") +\n  labs(x=\"\", title=\"Electric Bill in Dollars\",\n       subtitle=\"Forecast in blue\") +\n  autolayer(forecast(fitp,h=12), level=NULL)\n\n\n\n\n\n\n\n\n\n\nRefer back to question 2, where your task is to estimate the extra cost in energy of a machine. Use the entire data set and forecast as scenario where the machine is kept and one where the machine is sold. Change the model to a TSLM with linear trend and seasonality. How much extra would the ice cream shop pay if they kept the machine?\n\n\n\nSuggested Answer\n\nCreate a dummy variable that tags the dates when the new machine was in operation and fit the TSLM model:\n\nElec_ts&lt;-mutate(Elec_ts,\n                Down=case_when(\n                  Date&gt;=yearmonth(\"2023 7\") ~ 1,\n                  TRUE ~ 0))\n\nElec_ts %&gt;%\n  model(\n    LM2 = TSLM(`Bill Amount`~trend()+season()+Down)) -&gt; fit2\n\nNow we can create a new tsibble with data that assumes the new machine and no new machine with the scenarios() function.\n\nMach &lt;- scenarios(\n  New_Mach = new_data(Elec_ts, 12) %&gt;% \n    mutate(Down=rep(1,12)),\n  Old_Mach = new_data(Elec_ts, 12) %&gt;% \n    mutate(Down=rep(0,12)),\n  names_to = \"Scenario\")\n\nLastly, we can use this new data to forecast with our model:\n\nforecast(fit2,new_data=Mach) -&gt;est\nas_tibble(est) %&gt;% group_by(Down) %&gt;% \n  summarise(Sum=sum(.mean)) %&gt;% pull(Sum) %&gt;% diff()\n\n[1] 535.3909\n\n\nIt will cost the ice cream shop an additional 535 dollars in energy consumption to keep the machine. Below is a graph of the two scenarios:\n\nElec_ts %&gt;%\n  autoplot(`Bill Amount`) + \n  autolayer(forecast(fit2,new_data=Mach),\n            level=NULL)+ theme_classic() +\n  labs(x=\"Months\", y=\"\",\n       title = \"Energy Consumption\",\n       subtitle = \"Scenario Forecast\") + theme_clean()\n\n\n\n\n\n\n\n\n\n\nDelta Airlines relies heavily on maintaining high load factors (the percentage of seats filled on flights) to maximize profitability and optimize operational efficiency. Accurately forecasting load factors helps Delta plan routes, set prices, and allocate resources effectively.\nDelta has ask for you to forecast the load factor the next four periods using historical data and two different time series models: the ETS model (algorithmic) and the TSLM model (with seasonality and trend). By doing so, you will help Delta better anticipate future demand and optimize its flight operations.\nYou can find the load factor data here: https://jagelves.github.io/Data/AirlinesLoad.csv Evaluate both models based on their RMSE on the test set (January 2022 to December 2023). Choose the model with the lowest RMSE as the best method for forecasting. Using the chosen model, forecast Delta’s load factor for the next four months (January 2024 to April 2024).\n\n\n\nSuggested Answer\n\nHere is the R code to complete the task. Start by loading the data and creating the tsibble.\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nLoad&lt;-read_csv(\"http://jagelves.github.io/Data/AirlinesLoad.csv\")\n\nLoad %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  filter(Airline==\"Delta\") %&gt;% \n  select(Date,LF_total) %&gt;% as_tsibble(index=Date) -&gt; Load_ts\n\nNow we can create the train and test sets with the dates provided:\n\nLoad_ts %&gt;% filter_index(\"2022 Jan\"~\"2023 Dec\") -&gt; train\nLoad_ts %&gt;% filter_index(\"Jan 2024\"~.) -&gt; test\n\nNext, estimate both models and compare the accuracy of their predictions on the test set:\n\ntrain %&gt;% \n  model(TSLM=TSLM(LF_total~trend()+season()),\n        ETS=ETS(LF_total)) -&gt; fit\n\nfit %&gt;% forecast(test) %&gt;% \n  accuracy(test) %&gt;% select(.model,RMSE)\n\n# A tibble: 2 × 2\n  .model  RMSE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 ETS     2.45\n2 TSLM    3.38\n\n\nIt seems like the algorithmic ETS performs better. Finally, let’s forecast the series using the algorithmic ETS model:\n\nLoad_ts %&gt;% \n  model(ETS=ETS(LF_total)) %&gt;% \n  forecast(h=4)\n\n# A fable: 4 x 4 [1M]\n# Key:     .model [1]\n  .model     Date\n  &lt;chr&gt;     &lt;mth&gt;\n1 ETS    2024 May\n2 ETS    2024 Jun\n3 ETS    2024 Jul\n4 ETS    2024 Aug\n# ℹ 2 more variables: LF_total &lt;dist&gt;, .mean &lt;dbl&gt;\n\n\nHere is the graph:\n\nLoad_ts %&gt;% \n  model(ETS=ETS(LF_total)) %&gt;% \n  forecast(h=4) %&gt;% autoplot(level=95) +\n  autolayer(train,LF_total) + autolayer(test,LF_total,lty=2) +\n   theme_clean() + \n  labs(title=\"Delta Airlines Load Factors\",\n       x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management Science. Cengage.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ETS</span>"
    ]
  },
  {
    "objectID": "ARIMA.html",
    "href": "ARIMA.html",
    "title": "7  ARIMA",
    "section": "",
    "text": "7.1 Preliminaries\nThis section will introduce essential ideas and metrics to analyze time series further. Concepts such as white noise, stationarity, autocorrelation, and partial autocorrelation form the building blocks of one of time series most recognized models–the ARIMA model. The section then applies the AR(1), MA(1), and generalized ARIMA models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#preliminaries",
    "href": "ARIMA.html#preliminaries",
    "title": "7  ARIMA",
    "section": "",
    "text": "White Noise\nIn time series, white noise refers to a sequence of random data points that have no correlation to each other. This process is used as a benchmark for other types of time series data that exhibit patterns or trends. By comparing a series with the white noise process, we can verify if the series has systematic components that can be modeled (i.e., if there is any signal).\nWe can generate a white noise process by using the normal distribution with a mean of zero and a constant variance. Below we create a tsibble with the simulated data.\n\nlibrary(fpp3)\nset.seed(10)\nwn&lt;-tsibble(x=rnorm(100),period=seq(1:100),index=period)\n\nWe can now use the autoplot() function to observe the white noise process.\n\nwn %&gt;% autoplot(x) + theme_classic() + \n  labs(title=\"White Noise Process\",\n       subtitle=\"Mean=0 and Standard Deviation=1\",\nx=\"\",y=\"\") + \n  geom_hline(yintercept = 0, col=\"blue\", lwd=1, linetype=\"dashed\",\n             alpha=0.4)\n\n\n\n\n\n\n\n\nA very unpredictable and ragged pattern is shown in the graph above. The series behaves erratically but fluctuates around a mean of zero and keeps a standard deviation of one. Given the unpredictability of the white noise process, the best one can do is to describe it by its mean and standard deviation.\n\n\nStationarity\nA time series is said to be stationary if its statistical properties do not change over time. In other words, a stationary time series has a constant mean, variance, and auto-covariance, regardless of the time at which the series is observed. An example of a stationary process is the white noise process introduced above.\nMany time series models, including the ARIMA model, require a stationary time series. These models make predictions about future values based on past values, and the statistical properties of the past values are used to inform these predictions. If the time series’ statistical properties change over time, then the models may not work well, as the assumptions underlying them would not be met.\nIn general, before modeling and forecasting, we will check whether the series is stationary (i.e., has no trend and is homoskedastic). To eliminate the trend in the series we will use the first difference of the series. We can do this in R by using the difference() function. For example consider Tesla’s quarterly vehicle deliveries.\n\n\n\n\n\n\n\n\n\nDeliveries have been on an upward trend as the company is currently scaling its production, and demand is strong. This series is not stationary since it crosses the mean (blue line) once and never revisits it. That is, the mean is not constant and changes with time. It is possible to make the series stationary by finding differences (i.e. the change in deliveries from quarter to quarter). Below is the graph of the first difference.\n\ntesla %&gt;%\n  autoplot(difference(deliveries)) + theme_classic() +\n  labs(title=\"Quarterly Change In Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\") + \n  geom_hline(yintercept = mean(difference(tesla$deliveries), na.rm = TRUE), col=\"blue\", linetype=\"dashed\", lwd=1, alpha=0.4)\n\n\n\n\n\n\n\n\nThe series now fluctuates closer to the mean, but unlike the white noise process behaves less erratic. You will notice that in some periods the change in deliveries from quarter to quarter is high. For example, following the lows at the beginning of the year, deliveries seem to increase sharply. There seems to be correlations between the quarters (time dependencies).\nAnother pattern shown in the graph above is heteroskedasticity (increasing variance), The variance of the series seems to be low in the period of 2016-2018 while significantly higher for the period after. To normalize the variance of the series we can conduct a Box-Cox transformation.\n\nlambda &lt;- tesla %&gt;% \n  features(deliveries, features = guerrero) %&gt;% pull(lambda_guerrero) \n\ntesla %&gt;%\n  autoplot(box_cox(difference(deliveries), lambda)) +\n  labs(y = \"\")+ theme_classic() +\n  labs(title=\"Box-Cox Transformation of Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\")\n\n\n\n\n\n\n\n\nThe transformation has made the series a bit more homoskedastic (variance is more uniform) than the series without the transformation. It is important to note that both transformations (differencing and box-cox) can be undone by using an inverse function. Hence, we can always return the series to its original form. Luckily, fable does this automatically for us when we forecast a model.\nA couple of statistical features used to determine the stationarity of a series are the unitroot_kpss and unitroot_ndiffs. In general, a low p-value allows us to reject the null of hypothesis of stationarity. Below, we test the features with Tesla’s deliveries.\n\ntesla %&gt;%\n  features(deliveries, features = c(unitroot_kpss, unitroot_ndiffs)) \n\n\n\n\n\n\n\n\n\nStationarity Tests\n\n\nkpss_stat\nkpss_pvalue\nndiffs\n\n\n\n\n0.946\n0.010\n1\n\n\n\n\n\n\n\nUnfortunately, the test does not report the calculated p-value. Instead, it reports a p-value of 0.01 when the p-value is below 0.01 and 0.1 when it is above 0.1. Given that the p-value reported is 0.01, we verify that Tesla deliveries are non-stationary and that two differences are required to make the data stationary (ndiffs=2).\n\n\nThe autocorrelation function\nAutocorrelations are essential in time series analysis since they indicate the degree of similarity between a time series and a lagged version of itself (a previous period). They help identify patterns and trends in the data allowing us to predict future values of the series. For example, suppose a time series exhibits a strong positive autocorrelation at a lag of \\(k\\) periods. In such a case, the value at time \\(t+k\\) will likely be similar to that at time \\(t\\). Formally we can write the autocorrelation as:\n\n\\(\\rho_{y_t,y_{t-k}}=\\frac{cov(y_t,y_{y-k})}{sd(y_t)sd(y_{t-k})}\\)\n\n Let’s illustrate the use of autocorrelations with an example. This time let’s inspect personal income growth in the state of California. Below we load the data and create the train and test sets.\n\nlibrary(fpp3)\nlibrary(tidyverse)\nPI&lt;-read_csv(\"https://jagelves.github.io/Data/PersonalIncome.csv\")\nPI %&gt;% as_tsibble(index=Date) %&gt;% \n  filter_index(1970~2005) -&gt; PI_train\nPI %&gt;% as_tsibble(index=Date) %&gt;% \n  filter_index(2006~2021) -&gt; PI_test\n\nThe autocorrelation function can now be constructed by using the ACF() function and plotting it with autoplot() as shown below.\n\nPI_train %&gt;%\n  ACF(lag_max = 12,PI_Growth) %&gt;% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"\",\n                                 title=\"ACF Personal Income Growth in California\")\n\n\n\n\n\n\n\n\nThe plot shows that the correlation of the series with its first lag is strongest. That is, positive income growth in the previous period correlates with positive income growth in the current period. The plot also shows continuous decay in the strength of the correlation as the lags get larger. A positive income growth two or three periods ago still positively influences the current period’s income growth, but less than the immediate previous period. The blue lines determine which autocorrelations are statistically different from zero (significant) at the 5% level. As you can see, lags 1-4 are positively correlated with the series and are statistically significant.\nOn the other hand, a white noise process is expected to show no correlation with its lags since the series is constructed from independent draws from a normal distribution with constant variance. Previous periods do not affect the current or future periods. Below you can see the autocorrelation function of the white noise process.\n\nwn %&gt;% ACF(x) %&gt;% autoplot() + theme_bw() + labs(x=\"\", y=\"ACF\") +\nlabs(x=\"\", y=\"\", title=\"ACF White Noise Process\")\n\n\n\n\n\n\n\n\nInterestingly, lag 14 shows a positive correlation with the series. It is important to note that correlations can happen by chance even if we construct the series from a random process.\n\n\nThe partial autocorrelation function\nAs with the ACF, the partial autocorrelation function (PACF) summarizes the relationships between a series and its lags. However, the relationships of intervening lags are removed. The sample partial autocorrelation at lag \\(k\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nFormally speaking, when we calculate the autocorrelation between \\(y_t\\) and \\(y_{t+k}\\), information flows from \\(t\\) to \\(t+k\\), so that indirectly \\(\\rho_k\\) (the correlation) accounts for the contribution of lags between \\(t\\) and \\(t+k\\). A series of regressions would allow us to calculate the PACF. Luckily, R calculates these easily for us as shown below:\n\nPI_train %&gt;%\n  PACF(lag_max = 12,PI_Growth) %&gt;% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF Personal Income Growth In California\")\n\n\n\n\n\n\n\n\nThe graph shows that the series has a strong correlation only with its first lag. Specifically, lag 2, 3, and 4 seemed to have been correlated with the series (see ACF), but this was mainly because of the influence of lag 1.\nLet’s inspect the white noise process once more to confirm that there are no patterns.\n\nwn %&gt;% PACF(x) %&gt;% autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF White Noise Process\")\n\n\n\n\n\n\n\n\nIn sum, white noise processes are unpredictable and we can only describe them by their mean and standard deviation. Series that have patterns in their ACF or PACF can be modeled using ARIMA. Below we illustrate how to model Personal Income Growth in California with an AR(1) model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#the-ar1-model",
    "href": "ARIMA.html#the-ar1-model",
    "title": "7  ARIMA",
    "section": "7.2 The AR(1) model",
    "text": "7.2 The AR(1) model\nIn the previous section, we established that the growth of personal income in California has a decaying ACF and a single significant spike (at lag 1) in the PACF. These patterns can be generated with an AR(1) model. Specifically, the AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\n\nwhere \\(c\\) is a constant, \\(\\phi\\) is the lag coefficient, \\(y_{t-1}\\) is the first lag of \\(y\\), and \\(\\epsilon\\) is random error. Since this model uses only the first lag of the series as the independent variable, it is known as AR(1). The AR model can be extended to include more lags of \\(y\\), and in general, it would be called an \\(AR(p)\\) model, where \\(p\\) is the largest lag included. Below we simulate some data based on the AR(1) model.\n\ny&lt;-c(0)\nphi&lt;-0.7\nconst&lt;-1\nnrep&lt;-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data generated to see what the ACF looks like for a AR(1) process. This will allow us to visually identify the process.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %&gt;% ACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\n\n\n\n\nNote the resemblance of the ACF of the simulated data to that of the personal income growth. There is a decaying ACF described by a strong correlation of lag one and subsequent lower correlations as the lags get larger. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %&gt;% PACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+labs(x=\"\",y=\"PACF\",                         title=\"PACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\n\n\n\n\nOnce again, we can see the resemblance if we compare the PACF to the one in personal income growth. Specifically, there is one significant spike at lag one, and all other partial autocorrelations are not statistically different from zero. The patterns in the ACF and PACF confirm that we can model personal income growth with an AR(1) process.\n\nModeling and Residuals\nLet’s model personal income growth using the AR(1) model. We’ll also estimate a Least Squares model to compare. Recall, that we can estimate these models by using the model() function and retrieve the coefficients with the coef() function. We will use the AR() function along with the order() function set to one, to model the AR(1) process.\n\nPI_fit&lt;-PI_train %&gt;% \n  model(AR1 = AR(PI_Growth ~ order(1)),\n        LS = TSLM(PI_Growth ~ trend()))\ncoef(PI_fit)\n\n\n\n\n\n\n\n\n\nModel Coefficients For PI Growth\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nAR1\nconstant\n1.80\n0.82\n2.19\n0.04\n\n\nAR1\nar1\n0.70\n0.12\n5.80\n0.00\n\n\nLS\n(Intercept)\n9.19\n0.80\n11.50\n0.00\n\n\nLS\ntrend()\n−0.17\n0.04\n−4.43\n0.00\n\n\n\n\n\n\n\nThe estimated coefficient (\\(\\phi=0.7\\)) for the AR(1) process is equal the one used in our simulation. However, the estimated constant is 1.8. If we changed the constant to 1.8, the simulation would resemble the personal income growth data better. That is, the AR model selects the constant and lag coefficient (\\(\\phi\\)) such that it fits the data.\nIf the AR(1) process correctly describes the series, the errors should behave like white noise. If the model picks up all of the systemic variation, only random error should be left. To inspect the errors we can use the augment() function along with the object that contains the model (PI_fit). The ACF of the error is displayed below.\n\nerrors_PI&lt;-augment(PI_fit)\n\nerrors_PI %&gt;% select(.resid) %&gt;% ACF(.resid) %&gt;% \n  autoplot() + theme_bw() +\n  labs(title=\"ACF for Model Errors of AR(1) and LS\",\n       x=\"\")\n\n\n\n\n\n\n\n\nThe ACF of the errors from the AR(1) model resemble white noise. This suggests that we have correctly identified the systematic component of the series. In other words, there is nothing left to model since the errors are entirely random. This is not the case for the LS model since we still observe some significant spikes (lag 1 and lag 13) in the ACF function.\n\nerrors_PI %&gt;% select(.resid) %&gt;% PACF(.resid) %&gt;% \n  autoplot() + theme_bw()\n\n\n\n\n\n\n\n\nThe PACF once again shows no pattern for the residuals of the AR(1) model and some significant lags for the LS model. This further confirms that the AR(1) model correctly identifies the data generating process in the time series.\n\n\nModel Selection\nWe can choose between the LS and AR(1) models by looking at the AIC, AICc, or BIC.\n\nglance(PI_fit) %&gt;% arrange(AICc) %&gt;% select(.model:BIC)\n\n\n\n\n\n\n\n\n\nModel Fit Measures\n\n\n.model\nsigma2\nAIC\nAICc\nBIC\n\n\n\n\nAR1\n4.30\n−20.25\n−19.89\n−17.09\n\n\nLS\n5.51\n65.35\n66.10\n70.10\n\n\n\n\n\n\n\nHere we note that the AR(1) model performs better in all of the metrics as they are significantly lower than those for the LS. The accuracy on the test set shown below, once more confirms that the AR(1) model performs better than the LS model.\n\nPI_fc&lt;-PI_fit %&gt;% forecast(new_data = PI_test)\nPI_fc  %&gt;% accuracy(PI_test) \n\n\n\n\n\n\n\n\n\nAccuracy Measures\n\n\n.model\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\nACF1\n\n\n\n\nAR1\nTest\n−1.51\n3.47\n2.45\n−115.69\n156.61\nNaN\nNaN\n0.33\n\n\nLS\nTest\n2.60\n4.43\n3.87\n22.50\n106.69\nNaN\nNaN\n0.48\n\n\n\n\n\n\n\nThe graph below shows the test set along with the forecast of the AR(1) model. Prediction confidence intervals are shown to highlight the uncertainty of the prediction. The blue line indicates the mean of the predictions which are assumed to follow a normal distribution.\n\nPI_fc %&gt;% filter(.model==\"AR1\") %&gt;% autoplot(level=95) + theme_classic() +\n  autolayer(PI_train, PI_Growth) +\n  autolayer(PI_test, PI_Growth) + \n  labs(title=\"Personal Income Growth AR(1) Forecast Accuracy\",\nsubtitle=\"1970-2021\", y=\"\",x=\"\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#the-ma1-model",
    "href": "ARIMA.html#the-ma1-model",
    "title": "7  ARIMA",
    "section": "7.3 The MA(1) model",
    "text": "7.3 The MA(1) model\nThe moving average (MA) process models time series by using past errors (random shocks). Formally,\n\n\\(y_t=\\mu + \\theta \\epsilon_{t-1} +\\epsilon_{t}\\)\n\n\nwhere \\(\\mu\\) is the mean of the time series, and \\(\\epsilon\\) is the error term. As with the AR model, further error term lags could be included in the MA. An MA process with highest lag of \\(q\\) is called an MA(q) process. The MA process assumes that the current observation of a time series is a linear combination of the weighted sum of past error terms. The error terms capture the unpredictable and random fluctuations in the time series that are not accounted for by the AR model. Below we simulate an MA(1) process and generate the ACF and PACF plots.\nset.seed(13)\ne_t&lt;-rnorm(300,0,0.5)\ny_t=2+0.95*dplyr::lag(e_t)+e_t\n\ntsibble(y=y_t,period=seq(1,length(y_t)),index=period) %&gt;% ACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Simulated MA(1)\")\n\ntsibble(y=y_t,period=seq(1,length(y_t)),index=period) %&gt;% PACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"PACF For Simulated MA(1)\")\n\n\n\n\n\n\n\n\n\n\nThe pattern shown is one significant spike in the first lag of the ACF, and a decaying PACF as lags get larger. This is the opposite of the AR(1) process where we had a decaying ACF and one spike in the PACF.\n\nU.S. Treasury Example\nConsider the percent monthly change the yield of U.S. treasury securities (TCM5Y). Below is the plot of the series.\n\ntreasury&lt;-read_csv(\"https://jagelves.github.io/Data/treasury.csv\")\n\ntreasury %&gt;% mutate(period=1:length(OBS)) -&gt; treasury\nas_tsibble(treasury, index = period) %&gt;%\n  autoplot() + theme_classic() + \n  labs(title=\"Percent Monthly Change the Yield of U.S. Treasury Securities\",\n       subtitle=\"Monthly (1953-2008)\",\n       x=\"\", y=\"\")\n\n\n\n\n\n\n\n\nInterestingly, the graph above looks like white noise. A very ragged and unpredictable pattern that fluctuates around a mean of approximately zero. Let’s take a look at the ACF and PACF.\nas_tsibble(treasury, index = period) %&gt;% \n  ACF(lag_max = 12, changetreasury) %&gt;% \n  autoplot()+ theme_bw() +\nlabs(x=\"\", y=\"\", title=\"ACF for TCM5Y\")\n\nas_tsibble(treasury, index = period) %&gt;% \n  PACF(lag_max = 12, changetreasury) %&gt;% \n  autoplot()+ theme_bw() +\nlabs(x=\"\", y=\"\", title=\"PACF for TCM5Y\")\n\n\n\n\n\n\n\n\n\n\nWhen comparing with the MA(1) simulated values, the ACF of TCM5Y reflects the same pattern. One spike, and all other lags not significantly different from zero in the ACF. The PACF doesn’t exactly replicate the one from the simulation, but the pattern created is the same. Mainly, there is a decaying PACF, a large lag 1 partial autocorrelation, followed by smaller ones until they all are insignificant. These correlations additionally alternate in sign. As with the AR(1) process, we can estimate the model, check that the errors are white noise, and then forecast.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#arima",
    "href": "ARIMA.html#arima",
    "title": "7  ARIMA",
    "section": "7.4 ARIMA",
    "text": "7.4 ARIMA\nThe ARIMA model is based on stationary, AR, and MA processes. ARIMA stands for AutoRegressive Integrated Moving Average.\n\nAutoRegressive (AR): The “AR” part means that the future values of a time series depend on its own past values. If today’s value is high, tomorrow’s value is likely to be high too.\nIntegrated (I): The “I” part is about making the time series stationary. It means we take the differences between consecutive observations to remove trends or seasonality.\nMoving Average (MA): The “MA” part means that the future values also depend on the past prediction errors (residuals). If the model predicted a value that was too high or too low in the past, it will try to correct that in the future predictions.\n\nARIMA combines these three components to forecast future values in a time series by considering its own past, removing trends, and accounting for past errors in predictions. In terms of notation, an ARIMA(1,1,2), will perform the first difference of the series, and estimate the model with an AR(1) and MA(2) component. Below we estimate the ARIMA model.\n\nRevisiting Chilango’s Restaurant\nLet’s apply the ARIMA model to the avocado data. The code below loads the data and creates the training and test set.\n\ncali&lt;-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\ncali %&gt;%\n  as_tsibble(key=geography,index=date,regular=T) %&gt;%\n  filter_index(\"2015-01-04\"~\"2018-12-02\") -&gt; cali\n\ncali %&gt;%\n  as_tsibble(key=geography,index=date,regular=T) %&gt;%\n  filter_index(.~\"2018-06-02\") -&gt; calits_train\n\ncali %&gt;%\n  as_tsibble(key=geography,\n             index=date, regular=T) %&gt;%\n  filter_index(\"2018-06-02\"~\"2018-12-02\") -&gt; calits_test\n\nNext, we will estimate the LS, ETS, AR1, and ARIMA models. We will allow an algorithm to specify the parameters of both the ETS and ARIMA models by using the ETS() and ARIMA() functions and omitting arguments. Within the ARIMA() function we will set the approximation argument to FALSE so that the search for the best ARIMA model is exhaustive. Note also how an AR1 model is calculated. The ARIMA() function is called along with the pdq() function. The first argument of the pdq() function is the order of the AR process (p), the next argument is the number of differences needed to make the series stationary (d), and the last argument is the order of the MA process (q). Hence, for the AR1 model we specify pdq(1,0,0).\n\nfit &lt;- model(calits_train,ETS=ETS(average_price),\n              ARIMA=ARIMA(average_price,approximation = F),\n              AR1=ARIMA(average_price~pdq(1,0,0)),\n              LS=TSLM(average_price~trend()))\n\nThe coefficients for the models can be retrieved by using the tidy() or coef() functions.\n\nfit %&gt;% coef()\n\n\n\n\n\n\n\n\n\nModel Coefficients For The Avocado Data\n\n\ngeography\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nCalifornia\nETS\nalpha\n0.6461\nNA\nNA\nNA\n\n\nCalifornia\nETS\nl[0]\n1.2082\nNA\nNA\nNA\n\n\nCalifornia\nARIMA\nar1\n0.9220\n0.1097\n8.4033\n0.0000\n\n\nCalifornia\nARIMA\nar2\n−0.7364\n0.1135\n−6.4873\n0.0000\n\n\nCalifornia\nARIMA\nma1\n−1.1478\n0.0807\n−14.2160\n0.0000\n\n\nCalifornia\nARIMA\nma2\n0.8490\n0.0966\n8.7888\n0.0000\n\n\nCalifornia\nAR1\nar1\n0.8816\n0.0350\n25.2051\n0.0000\n\n\nCalifornia\nAR1\nconstant\n0.1971\n0.0097\n20.3814\n0.0000\n\n\nCalifornia\nLS\n(Intercept)\n1.4856\n0.0391\n38.0158\n0.0000\n\n\nCalifornia\nLS\ntrend()\n0.0022\n0.0004\n5.8702\n0.0000\n\n\n\n\n\n\n\nThe models suggested by the algorithms are fairly simple. The ETS model suggested is a SES with a smoothing parameter of about \\(0.65\\). As for the ARIMA model, an ARIMA(2,0,2) process is suggested. Below we plot the ACF and the PACF of the average price of avocados.\ncalits_train %&gt;% ACF(lag_max=12, average_price) %&gt;%\nautoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Avocado Average Price\")\n\ncalits_train %&gt;% PACF(lag_max=12, average_price) %&gt;%\nautoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"PACF For Avocado Average Price\")\n\n\n\n\n\n\n\n\n\n\nThe patterns shown in the plots above are textbook AR(1). Hence, the inclusion of the model in the set of candidates. The code below forecasts the series 27 periods ahead and plots.\n\nfit %&gt;% forecast(h=27) %&gt;% autoplot(level=NULL) + \n  theme_classic() + \n  autolayer(cali, average_price) + \n  labs(y=\"\", title= \"California's Forecasted Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - Dec 2, 2018\",\n       x=\"\")\n\n\n\n\n\n\n\n\nInterestingly, the AR(1) and the LS model perform the best. The LS model overestimates the test data, while the AR(1) underestimates it. One could imagine combining the forecasts of these models to obtain a better one.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#combination-of-models-ensembles",
    "href": "ARIMA.html#combination-of-models-ensembles",
    "title": "7  ARIMA",
    "section": "7.5 Combination of Models (Ensembles)",
    "text": "7.5 Combination of Models (Ensembles)\nTo create a combination of models we can use the fit object containing the mable. Specifically, any model in the fit object can be combined using a linear combination. The code below combines the AR(1) model and the LS model by finding a simple average between both models.\n\nfit %&gt;%  mutate(Ensemble=0.5*LS+0.5*AR1) -&gt; fit2\n\nNow we can plot the forecasts for the ensemble, the AR1 and the LS model.\n\nfit2 %&gt;% select(AR1,LS,Ensemble) %&gt;%\n  forecast(h=27) %&gt;% autoplot(level=NULL) +\n  autolayer(cali,average_price) + theme_classic() +\n  labs(y=\"\", title= \"California's Forecasted Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - Dec 2, 2018\",\n       x=\"\")\n\n\n\n\n\n\n\n\nThe plot illustrates how the ensemble fits the test data better than each model individually. Let’s confirm the accuracy of the ensemble to the test set by calculating some measures.\n\nfit2 %&gt;% forecast(h=27) %&gt;% accuracy(cali)\n\n\n\n\n\n\n\n\n\nModel Fit\n\n\n.model\ngeography\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nMASE\nRMSSE\nACF1\n\n\n\n\nAR1\nCalifornia\nTest\n0.16\n0.21\n0.16\n8.65\n8.65\n1.63\n1.49\n0.50\n\n\nARIMA\nCalifornia\nTest\n0.16\n0.20\n0.16\n8.20\n8.20\n1.55\n1.46\n0.51\n\n\nETS\nCalifornia\nTest\n0.20\n0.23\n0.20\n10.39\n10.39\n1.95\n1.69\n0.50\n\n\nEnsemble\nCalifornia\nTest\n0.03\n0.13\n0.10\n1.38\n5.26\n0.98\n0.92\n0.51\n\n\nLS\nCalifornia\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\n1.37\n1.14\n0.52",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#readings",
    "href": "ARIMA.html#readings",
    "title": "7  ARIMA",
    "section": "7.6 Readings",
    "text": "7.6 Readings\nChapter 9 of Hyndman (2021) deals with ARIMA models. The readings are not very technical and omit explaining the model selection process. For readers wanting even more mathematical details as well as a deep dive into the model selection process I recommend reading Gonzalez (2013). For those looking for an alternative to Hyndman (2021) you can try Svetunkov (2023) or Hank (2023).\n\nHyndman (2021) Chapter 9 (ARIMA Models).\nGonzalez (2013) Chapter 6 (MA models), Chapter 7 (AR models), Chapter 8 (Forecasting Practice I)\nSvetunkov (2023)\nHank (2023)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#leasons-learned",
    "href": "ARIMA.html#leasons-learned",
    "title": "7  ARIMA",
    "section": "7.7 Leasons Learned",
    "text": "7.7 Leasons Learned\nIn this module you have been introduced to ARIMA model. Particularly you have learned to:\n\nApply autocorrelations, partial autocorrelations, stationarity and white noise to model time series.\nIdentify the AR(1) and MA(1) processes using the ACF and PACF.\nModel the ARIMA process using the ARIMA() function.\nProvide forecasts of the ARIMA process in R.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "ARIMA.html#exercises",
    "href": "ARIMA.html#exercises",
    "title": "7  ARIMA",
    "section": "7.8 Exercises",
    "text": "7.8 Exercises\n\nRecall problem 2 in chapter 6 (ExercisesETS), where you are tasked to forecast the energy consumption of a new ice cream machine. Use the entire data found here: http://jagelves.github.io/Data/ElectricityBill.csv to estimate an AR1 model. Graph the ACF and PACF and confirm that an AR1 is appropriate. Check residuals from the model and confirm that they resemble white noise. Create a graph of the series and the forecast for four periods ahead.\n\n\n\nSuggested Answer\n\nLet’s start by loading the data and creating a tsibble:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nrm(list=ls())\nElec&lt;-read_csv(\"http://jagelves.github.io/Data/ElectricityBill.csv\")\n\nElec %&gt;% mutate(Date=yearmonth(paste(Year, Month, sep = \"-\"))) %&gt;% \n  select(Date,`Bill Amount`) %&gt;% \n  as_tsibble(index=Date)-&gt; Elec_ts\n\nWe can now graph the ACF and the PACF of the Bill Amount variable:\nElec_ts %&gt;% ACF(`Bill Amount`,lag_max = 24) %&gt;% autoplot()\nElec_ts %&gt;% PACF(`Bill Amount`,lag_max = 24) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nThere is a one spike at lag number one in the ACF and one spike in lag number one in the PACF. We could use both an AR1 or MA1 process to model the behavior. Let’s estimate the AR1.\n\nElec_ts %&gt;% model(AR1=ARIMA(`Bill Amount`~pdq(1,0,0))) -&gt; fit\n\nNow we can check the residuals and see if they resemble white noise:\nfit %&gt;% augment() %&gt;% ACF(.resid,lag_max = 24) %&gt;% autoplot()\nfit %&gt;% augment() %&gt;% PACF(.resid,lag_max = 24) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nThe are no significant spikes. We can conclude that the errors resemble white noise.\n\n\nRecall problem 1 in chapter 6 (ExercisesETS), where you are tasked to forecast Tesla’s energy deployment. Use the entire data found here: http://jagelves.github.io/Data/ElectricityBill.csv to estimate an ensemble of the TSLM model with linear trend and seasonality, and a TSLM model with quadratic trend and seasonality. Let the weight of the linear model be 30% and that of the quadratic model 70%. Graph the forecast for the next four periods of the three models.\n\n\n\nSuggested Answer\n\nWe can start by loading the packages, data, and creating a tsibble:\n\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(ggthemes)\nrm(list=ls())\nstorage&lt;-read_csv(\"http://jagelves.github.io/Data/teslaE.csv\")\n\nstorage %&gt;%\n  mutate(Date=yearquarter(Date)) %&gt;%\n  as_tsibble(index=Date) -&gt; storage_ts\n\nLet’s now create the Linear and Quadratic models using TSLM. Then we can create the ensemble model by using the mutate() function and the given weights:\n\nstorage_ts %&gt;%\n  model(LM=TSLM(EnergyStorage~trend()+season()),\n        LM2=TSLM(EnergyStorage~trend()+I(trend()^2)+season())) -&gt;fit\n\nfit %&gt;% mutate(Ensemble=0.3*LM+0.7*LM2) -&gt; fit2\n\nWe can now graph the forecast for each model:\n\nfit2 %&gt;% forecast(h=4) %&gt;% \n  autoplot(level=NULL) +\n  autolayer(storage_ts,EnergyStorage) +\n  theme_clean()\n\n\n\n\n\n\n\n\n\n\nUsing the monthly inflation data from this source: https://jagelves.github.io/Data/Inflation.csv, forecast the inflation rate for the next four months. First, use the ARIMA() function to let the model automatically select the best-fitting ARIMA model for the inflation data while incorporating the interest rate into the model with the command ARIMA(Inflation ~ InterestRate). Develop two scenarios: the Federal Reserve holds the interest rate steady at 5.33% over the next four months, and the Fed follows a decreasing schedule of 5.25%, 5.00%, 4.75%, and 4.50%. Identify the month the Fed reaches its target of 2% inflation in each scenario and determine which schedule achieves this target faster. Create a graph that shows each scenario and the target inflation rate.\n\n\n\nSuggested Answer\n\nStart by loading the packages, data, and creating a tsibble:\n\nrm(list=ls())\n\nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/Inflation.csv\")\n\nRows: 54 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date\ndbl (4): Inflation, InterestRate, S&Preturns, Unemployment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata %&gt;% mutate(Date=yearmonth(mdy(Date))) %&gt;% \n  as_tsibble(index=Date) -&gt; data_ts\n\nNext, create the model and the scenarios:\n\ndata_ts %&gt;% \n  model(ARIMA=ARIMA(Inflation~InterestRate)) -&gt; fit2\n\nInt &lt;- scenarios(\n  Keep = new_data(data_ts, 4) %&gt;% \n    mutate(InterestRate=rep(5.33,4)),\n  Lower = new_data(data_ts, 4) %&gt;% \n    mutate(InterestRate=c(5.25,5.00,4.75,4.5)),\n  names_to = \"Scenario\")\n\nNext we can report the forecasts:\n\nforecast(fit2,new_data=Int)\n\n# A fable: 8 x 6 [1M]\n# Key:     Scenario, .model [2]\n  Scenario .model     Date\n  &lt;chr&gt;    &lt;chr&gt;     &lt;mth&gt;\n1 Keep     ARIMA  2024 Nov\n2 Keep     ARIMA  2024 Dec\n3 Keep     ARIMA  2025 Jan\n4 Keep     ARIMA  2025 Feb\n5 Lower    ARIMA  2024 Nov\n6 Lower    ARIMA  2024 Dec\n7 Lower    ARIMA  2025 Jan\n8 Lower    ARIMA  2025 Feb\n# ℹ 3 more variables: Inflation &lt;dist&gt;, .mean &lt;dbl&gt;, InterestRate &lt;dbl&gt;\n\n\nThe forecasts show that if the Fed keeps the rate at 5.33%, the inflation will reach its goal sometime in November. However, if the Fed decides to lower rates, we are looking at some time in December. Lastly, we can plot our scenarios:\n\nforecast(fit2,new_data=Int) %&gt;% autoplot(level=95) + \n  autolayer(data_ts,Inflation) + \n  theme_clean() + \n  geom_hline(yintercept=2, lty=2) + \n  labs(title=\"US Monthly Inflation Rate\",\n       subtitle=\"Keep interest rate at 5.33% vs. 25 basis point reductions\",\n       x=\"\",y=\"\")\n\n\n\n\n\n\n\n\n\n\nRising U.S. household debt is concerning because it increases financial strain on families, reduces disposable income, and heightens vulnerability to interest rate hikes. It raises the risk of defaults and bankruptcies, potentially destabilizing financial institutions and the broader economy. Additionally, higher debt levels can slow consumer spending and economic growth, while also creating risks in the housing market, making the economy more fragile during downturns. Use the data found here: https://jagelves.github.io/Data/USDebt.csv to forecast the US total debt of households using ARIMA. Start by graphing the ACF and PACF, then suggest two ARIMA models and let the algorithm choose the third. Calculate the BIC for the three models and select the model with the lowest BIC. Forecast the series into the future. By what date does the model predict the US Household debt reaches 20 trillion dollars?\n\n\n\nSuggested Answer\n\nStart by loading the data and packages:\n\nrm(list=ls())\n\nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(ggthemes)\ndata&lt;-read_csv(\"https://jagelves.github.io/Data/USDebt.csv\")\n\ndata %&gt;% mutate(Quarter=yearquarter(Quarter)) %&gt;% \n  as_tsibble(index=Quarter) -&gt; data_ts\n\nNow we can plot the ACF and the PACF. Since the series seems non-stationary, we’ll go ahead and plot the differenced series:\ndata_ts %&gt;% ACF(diff(Total)) %&gt;% autoplot()\ndata_ts %&gt;% PACF(diff(Total)) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nLooking at the graphs, we suggest using an AR1 model or AR3 model. Next, estimate the models:\n\ndata_ts %&gt;% \n  model(AR3=ARIMA(Total~pdq(3,1,0)),\n        AR1=ARIMA(Total~pdq(1,1,0)),\n        ARIMA=ARIMA(Total)) -&gt; fit\n\nNow we can observe the residuals:\n\nfit %&gt;% augment() %&gt;% ACF(.resid) %&gt;% autoplot()\n\n\n\n\n\n\n\n\nBoth the algorithmic model and the AR3 residuals look like white noise. Let’s look at the BIC:\n\nfit %&gt;% glance() %&gt;% select(.model,BIC)\n\n# A tibble: 3 × 2\n  .model   BIC\n  &lt;chr&gt;  &lt;dbl&gt;\n1 AR3    -131.\n2 AR1    -125.\n3 ARIMA  -128.\n\n\nThe AR3 model has the lowest BIC, so we choose this model for our forecast. Below is a graph of the forecast:\n\nfit %&gt;% forecast(h=20) %&gt;% \n  as_tsibble() %&gt;% filter(.model==\"AR3\") %&gt;% \n  autoplot(.mean, col=\"blue\", lty=2) +\n  autolayer(data_ts,Total) + theme_clean() +\n  labs(title=\"US Consumer Debt\",\n       subtitle=\"Trillion of Dollars\",\n       x=\"\",y=\"\") +\n  geom_hline(yintercept=20,lty=2)\n\n\n\n\n\n\n\n\nIf all things remain constant we should expect the Household debt to reach 20 trillion dollar by Q2 2028.\n\n\n\n\n\nGonzalez, Gloria. 2013. Forecasting for Economics and Business. Pearson.\n\n\nHank, Christoph. 2023. Introduction to Econometrics with r. University of Duisburg-Essen. https://www.econometrics-with-r.org/index.html.\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice. Otexts. https://otexts.com/fpp3/.\n\n\nSvetunkov, Ivan. 2023. Forecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM). Chapman; Hall/CRC. https://doi.org/10.1201/9781003452652.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ARIMA</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gelves, J. Alejandro. 2022. Business Statistics. Bookdown. https://jagelves.github.io/BusinessStats/.\n\n\nGonzalez, Gloria. 2013. Forecasting for Economics and Business.\nPearson.\n\n\nGrolemund, Garret. 2014. Hands-on Programming with r. O’Reilly.\nhttps://jjallaire.github.io/hopr/#license.\n\n\nHank, Christoph. 2023. Introduction to Econometrics with r.\nUniversity of Duisburg-Essen. https://www.econometrics-with-r.org/index.html.\n\n\nHyndman, Rob. 2021. Forecasting Principles and Practice.\nOtexts. https://otexts.com/fpp3/.\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. Business Statistics.\nMcGraw Hill.\n\n\nKwak, Young, and Lisa Ingall. 2007. “Exploring Monte Carlo\nSimulation Applications for Project Management.”\n\n\nSvetunkov, Ivan. 2023. Forecasting and Analytics with the Augmented\nDynamic Adaptive Model (ADAM). Chapman; Hall/CRC. https://doi.org/10.1201/9781003452652.\n\n\nWickham, Hadley. 2017. R for Data Science. O’Reilly. https://r4ds.hadley.nz.\n\n\nWinston, Wayne, and Christian Albright. 2019. Practical Management\nScience. Cengage.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "Decisions.html",
    "href": "Decisions.html",
    "title": "1  Decisions Under Uncertainty",
    "section": "",
    "text": "1.1 Shopaholic Retail Company\nIn this module we will learn how to create decision models. These models rely on probability and expected values. We will map the decision process using a decision tree and value each decision with a monetary value or an expected monetary value (EMV). Our objective is to identify the best decision given the possible choices and uncertainty.\nDecision models are crucial in business because they provide a structured approach to making complex decisions under uncertainty. By quantifying the potential outcomes and associated risks, these models help companies navigate uncertainties and make informed choices that maximize value. Whether it’s deciding to launch a new product, enter a new market, or allocate resources, decision models enable businesses to evaluate different scenarios, anticipate potential challenges, and choose strategies that align with their long-term goals. In a rapidly changing business environment, the ability to make data-driven decisions is a critical competitive advantage.\nImagine you are the CEO of the Shopaholic Retail Company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome. Determining how to adequately assess the risks involved in this decision and the potential outcomes is of importance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#shopaholic-retail-company",
    "href": "Decisions.html#shopaholic-retail-company",
    "title": "1  Decisions Under Uncertainty",
    "section": "",
    "text": "Success Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#sec-EMV",
    "href": "Decisions.html#sec-EMV",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.2 Expected Monetary Value",
    "text": "1.2 Expected Monetary Value\nAccording to your estimates, you believe that there is a \\(60\\)% chance of success and a \\(40\\)% chance of failure. Using this information, you can calculate the expected monetary value (EMV) of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\nThe expected monetary value estimates the average monetary outcome you can expect from a decision involving uncertainty. In this example, the value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EMV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EMV\\) is the expected monetary value, \\(p_{i}\\) is the probability of outcome \\(i\\), and \\(x_{i}\\) is the monetary value resulting from outcome \\(i\\). Notice that if we had other decisions involving uncertainty, we could also evaluate them using the EMV. Hence, the EMV allows us to rank and choose best decisions when uncertainty is present.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#decision-trees",
    "href": "Decisions.html#decision-trees",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.3 Decision Trees",
    "text": "1.3 Decision Trees\nDecision trees visually map the entire business decision process. Below you can see the decision tree for the decision to introduce a new production line.\n\n\n\n\n\ngraph LR\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( 0 )\n    C --&gt;|Success p=0.6| D( 5 )\n    C --&gt;|Failure p=0.4| E( -3 )\n\n\n\n\n\n\n Decision trees are read from left to right. Notice that there are two main branches stemming from the first decision node (i.e., the first square to the left). You can decide to Introduce or Not Introduce the new production line. If you Introduce, you reach a probability node (i.e., the circle that leads to the success and failure branches) where chance determines success or failure at the probabilities given. In the end, your choice of introducing the new product line yields an expected monetary value of 1.8 million, whereas not introducing the line yields no payoff.\nTo solve decision trees and find the optimal decision use backward induction. Starting from the right of the decision tree and working back to the left at each probability node calculate the EMV. At each decision node, take the maximum of EMV’s to identify the optimal decision. Applying this procedure to our simple example, we would start with the probability node that leads to the success and failure branches and calculate the EMV of 1.8 million (refer to Section 1.2). Moving back to our initial decision, we now have the choice to Introduce or Not Introduce. In this case we should Introduce, since the resulting EMV (1.8) is higher than Not Introduce (0).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#consulting-team",
    "href": "Decisions.html#consulting-team",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Consulting Team",
    "text": "1.4 Consulting Team\nConsider now the option of hiring a consulting team that promises to give you a bit more certainty. The team provides a recommendation based on market research, historical data, and their expertise for $500,000. However, it is known that the team has a false positive rate (i.e., recommending to go with a project when the project would fail) of 15% and a false negative rate of 5% (i.e., recommending not going with a project and when the project would succeed). Below you can see the updated decision tree with the results of the recommendation (i.e., Positive, and Negative).\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=?| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=?| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )\n\n\n\n\n\n\nNote that even though the consultants might recommend not introducing the production line, the CEO can still decide to go against their recommendation. Also, some probabilities are now unknown and must be calculated (they are highlighted in the decision tree with \\(?\\)). For example, given that the recommendation is positive, the probability that the introduction would succeed is unknown \\(p(Success|+)\\). We will use probability theory to uncover the missing probabilities in the upcoming sections. Finally, the final payoffs have all been adjusted to reflect the cost of hiring the team of consultants.\nUltimately, we would like gain more certainty on our decision to introduce the production line. Additionally, knowing if we should hire consultants to advise is equally essential.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#updating-probabilities",
    "href": "Decisions.html#updating-probabilities",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.5 Updating Probabilities",
    "text": "1.5 Updating Probabilities\nThe Law of total probability is useful in determining the probabilities that we get a positive recommendation \\(p(+)\\) or a negative recommendation \\(p(-)\\). In sum the law states:\n\n\\(p(A)=p(A|B)p(B)+p(A|B^c)p(B^c)\\)\n\nSubstituting the values provided in our problem we obtain:\n\n\\(p(+)=p(+|Failure)p(Failure)+p(+|Success)p(Success)\\) \n\\(p(+)=0.15(0.4)+0.95(0.6)\\) \n\\(p(+)=0.63\\)\n\nHence, the probability of obtaining a recommendation of introducing the new product line is \\(63\\)% and of not recommending the introduction is \\(p(-)=37\\)%. The updated decision tree is now:\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#bayes-theorem",
    "href": "Decisions.html#bayes-theorem",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.6 Bayes’ Theorem",
    "text": "1.6 Bayes’ Theorem\nNow we can use Bayes’ Theorem to update our probabilities given the recommendation from the consulting team. Recall that Bayes’ Theorem is a concept in probability that helps us update our beliefs when given new information. Mathematically, Bayes’ Theorem states:\n\n\\(p(A|B)=\\frac{p(B|A)p(A)}{p(B)}\\)\n\nSubstituting values we can find the missing probabilities at the edge of the tree. In particular:\n\n\\(p(Success|+)=\\frac{p(+|Success)p(Success)}{p(+)}\\) \n\\(p(Success|+)=\\frac{0.95(0.6)}{0.63}\\) \n\\(p(Success|+)=0.90\\)\n\nThis implies that \\(p(Failure|+)=0.10\\). Similarly, the probabilities \\(p(Success|-)\\) and \\(p(Failure|-)\\) can be found using Bayes’ theorem (the reader should try to obtain these probabilities). The updated decision tree is given below:\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=0.90| D( 4.5 )\n    C --&gt;|Failure p=0.10| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=0.08| J( 4.5 )\n    H --&gt;|Failure p=0.92| K( -3.5 )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#sec-Optimal",
    "href": "Decisions.html#sec-Optimal",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Optimal Decision",
    "text": "1.7 Optimal Decision\nNow that we have calculated the probabilities we can finally decide whether we should hire the consulting team, and most importantly whether we should introduce the new product line. Summarizing the decision tree by calculating the EMV’s for each probability node yields:\n\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( ))\n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt; D(EMV=3.70)\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt; J(EMV=-2.86)\n\n\n\n\n\n\nNote that even though the consultants might recommend not introducing the production line, the CEO can still decide to go against their recommendation. If the recommendation is positive, then the EMV resulting from introduction of the product line (\\(3.70\\)) is greater than the loss of not introducing the product line (\\(-0.5\\)). Similarly, if the recommendation is negative, the EMV from the introduction (\\(-2.86\\)) is less than the EMV of not introducing the product line (\\(-0.5\\)). As a consequence, the recommendation is aligned with the CEO’s best decision.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#the-value-of-information",
    "href": "Decisions.html#the-value-of-information",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.8 The Value of Information",
    "text": "1.8 The Value of Information\nWe are now faced with an important question. What is the value of the information provided by the consulting team? In other words, should the CEO hire the consultants? To answer this question, let’s start by calculating expected monetary value of the tree in Section 1.7:\n\n\\(EMV=0.63(3.70)+0.37(-0.5)\\) \n\\(EMV=2.146\\) \n\nThis EMV is higher than the EMV of making the decision without the consultants (1.8 million). This highlights that the information provided by the consulting team is valuable to the CEO. Moreover, the CEO should be willing to pay up to $846,000 (EMV with free information - EMV without information) for the consulting service.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#readings",
    "href": "Decisions.html#readings",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.9 Readings",
    "text": "1.9 Readings\nThe readings for this chapter are mainly from Winston and Albright (2019). Chapter 9 provides an excellent introduction to decision models with a couple of solved problems using Excel. I recommend using R (as a calculator) instead of Excel, as this book mainly uses R. To construct decision trees in R, you can use mermaid along with Quarto. Some probability concepts are essential to review before you start reading the chapter. In particular, discrete random variables, expected value, conditional probability, probability rules, and Bayes’ theorem.\n\nWinston and Albright (2019) Chapters 9.1 (Introduction), 9.2 (Elements of Decision Analysis), 9.3 (Single-Stage Decision Problems) and 9.5 (Multistage Decision Problems). It is recommended you follow along in R as opposed to Excel (or Precision Tree Add-In).\nJaggia and Kelly (2022) Chapter 4.1 (Fundamental Probability Concepts), 4.2 (Rules of Probability), 4.3 (Contingency Tables and Probabilities), and 4.4 (The Total Probability Rule and Bayes’ Theorem).\nQuarto: https://Quarto.org\nMermaid in Quarto Document: https://quarto.org/docs/authoring/diagrams.html\nMermaid in R Script: https://www.rdocumentation.org/packages/DiagrammeR/versions/1.0.10/topics/mermaid",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#lessons-learned-in-this-chapter",
    "href": "Decisions.html#lessons-learned-in-this-chapter",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.10 Lessons Learned In This Chapter",
    "text": "1.10 Lessons Learned In This Chapter\n\nUse the concept of Expected Monetary Value to rank decisions involving uncertainty.\nUse Decision Trees to map the business decision process.\nApply backward induction to solve decision problems.\nApply the Law of Total Probability and Bayes’ Theorem to update probabilities.\nDetermine the price of information.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  },
  {
    "objectID": "Decisions.html#exercises",
    "href": "Decisions.html#exercises",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.11 Exercises",
    "text": "1.11 Exercises\n\nTesla, a U.S. EV car manufacturer, is considering expanding production to meet potential increases in the demand for one of its cars. Tesla’s alternatives are to construct a new plant, expand the existing plant, or do nothing in the short run. Analysts within Tesla expect the market for their EV’s may expand, remain stable, or contract, with probabilities of 0.35, 0.35, and 0.30, respectively. The table below contains the profit generated under each scenario. What is your recommendation to Tesla?\n\n\n\n\n\nExpand\nStable\nContract\n\n\n\n\nNew Plant\n400,000\n-100,000\n-200,000\n\n\nExpand Plant\n250,000\n-50,000\n-75,000\n\n\nDo Nothing\n50,000\n0\n-30,000\n\n\n\n\n\nCalculation Video\n\n\n\n\nYour browser does not support the video tag. \n:::\n\n\n\nSuggested Answer\n\nWe can use expected values to guide our recommendation to Tesla. The expected value of each option is as follows: constructing a new plant is 45,000, expanding plant is 47,500, and doing nothing is 8,500. Following these results we could recommend Tesla to expand its existing plant, as this option would give the company the highest expected profit.\n\n\nFrontier Airlines is considering purchasing a new airplane from an aircraft manufacturer. The manufacturer offers the airplane for 50 million. The airline estimates that, if the airplane is purchased, it could generate an additional revenue of 120 million over its operational lifetime. However, this is contingent on the condition that the airline industry remains stable and demand for flights remains high, which the airline estimates has a 70% probability of occurring.\nIf the industry faces a downturn (a 30% probability), the airline estimates it will only be able to generate $30 million in revenue from the airplane over its lifetime.\nAlternatively, the airline has the option to lease an older model airplane for 10 million. The older airplane would generate $40 million in revenue over its lifetime, regardless of industry conditions.\nShould the airline purchase the new airplane or lease the older model?\n\n\n\nCalculation Video\n\n\n\n\nYour browser does not support the video tag. \n:::\n\n\n\nSuggested Answer\n\nOnce again we can use the expected value in this problem. The option of buying the airplane yields an expected profit of 43 million dollars, whereas the option of leasing the airplane yields a profit of 30 million. Given this result, we should recommend Frontier airlines to buy the airplane.\n\n\nPharmaCo is a pharmaceutical company considering whether to launch a new drug called “CureFast”. The success of CureFast in the market depends on the drug’s effectiveness in treating a particular condition. PharmaCo estimates that there is a 40% chance that the drug will be highly effective and thus successful in the market. If CureFast is successful, the company will earn a profit of 50 million. However, if the drug is not effective, PharmaCo will incur a loss of 20 million due to production, marketing, and other sunk costs. The company can always decide to not launch the drug. This option yields a profit of 0.\nBefore deciding to launch the drug, PharmaCo can conduct a clinical trial at a cost of $5 million. The clinical trial is not perfect and has the following characteristics:\n\nIf the drug is truly effective, there is a 90% chance that the clinical trial will show positive results. If the drug is not effective, there is a 15% chance that the clinical trial will still show positive results.\n\nShould PharmaCo launch the drug? Should they conduct the clinical trial? What is the maximum amount they should pay for the trial?\n\n\n\nCalculation Video\n\n\n\n\nYour browser does not support the video tag. \n:::\n\n\n\nSuggested Answer\n\nThe expected value when not launching the clinical trial is 8 million. Since the expected value after conducting the trial is higher (11.2 million), PharmaCo should conduct the clinical trial and use the results to inform their decision on whether to launch CureFast.PharmaCo should pay at most 8.2 million for the clinical trial.\n\n\nJoão “Golden Toes” Silva is the next Brazilian league wonder kid, and Arsenal’s scouts are buzzing with excitement. The club’s analysts estimate that João has a 30% chance of being a success at Arsenal, performing at a top level and helping the club win titles.\nIf João is successful, Arsenal stands to make £18 million in profits through merchandise sales, increased ticket revenue, and potential transfer fees. However, if João fails to live up to expectations (which has a 70% chance), it could cost the club £8 million (wasted wages, and negative press).\nGiven the risk of signing the player, the club can conduct a thorough exam on João making sure that he will be a success. The test is perfect and has no type 1 or 2 errors. The cost of this physical test is £3 million.\nShould Arsenal sign “Golden Toes”? Should they conduct the physical test? What is the maximum Arsenal should pay for the test?\n\n\n\nSuggested Answer\n\nWithout the test, the expected value is -200,000 pounds. With the perfect test, the expected value is 2,400,000 pounds. We recommend Arsenal go with the test and sign the player. The club can pay up to 5,400,000 pounds for the test.\n\n\nThe Crypto Gains investment firm is planning to invest in Bitcoin to diversify its portfolio. The firm knows that it will need 50 Bitcoins six months from now to meet its investment strategy. The company can buy the Bitcoins at the market price six months from now or purchase a futures contract now. The contract guarantees delivery of the Bitcoins in six months, and the cost of purchasing it will be based on today’s market price. Assume that the possible Bitcoin futures contracts available for purchase are for 25 Bitcoins or 50 Bitcoins only. No futures contracts can be purchased or sold in the intervening months; thus, Crypto Gains’ possible decisions are to:\n\n\nPurchase a futures contract for 50 Bitcoins now.\nPurchase a futures contract for 25 Bitcoins now and purchase 25 Bitcoins in six months at market price.\nPurchase all 50 Bitcoins needed in six months at the market price.\nThe price of Bitcoin bought now for delivery in six months is 30,000 dollars per Bitcoin. The transaction cost for 25-Bitcoin and 50-Bitcoin future contracts are 1,500 and 2,800 dollars, respectively. Finally, the company has assessed the probability distribution for the possible Bitcoin prices six months from now in dollars per Bitcoin. The table below contains the possible prices and the corresponding probabilities. Which investment strategy should Crypto Gains pursue to minimize risk and potential costs while securing the necessary Bitcoins?\n\n\n\n\nPrice\nProbability\n\n\n\n\n$28,000\n0.10\n\n\n$29,500\n0.30\n\n\n$31,000\n0.35\n\n\n$32,500\n0.15\n\n\n$34,000\n0.10\n\n\n\n\n\nSuggested Answer\n\nThe expected values are a follows: buy 50 Bitcoins now via futures contract: 1,502,800, buy 25 Bitcoins now via futures contract, and 25 Bitcoins in six months at market price: 1,520,875, buy all 50 Bitcoins in six months at market price: 1,538,750. Crypto Gains should purchase a futures contract for 50 Bitcoins now to secure the necessary investment and minimize costs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Decisions Under Uncertainty</span>"
    ]
  }
]